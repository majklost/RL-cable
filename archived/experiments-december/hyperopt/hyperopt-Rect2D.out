/mnt/personal/mrkosmic/synced/RL-cable/experiments/hyperopt
Starting hyperopt
Starting optimization
Starting optimization
Starting optimization
Starting optimization
Starting optimization
Starting optimization
Starting optimization
Starting optimization
Starting optimization
Starting optimization
Eval num_timesteps=40000, episode_reward=-1517.93 +/- 1253.72
Episode length: 158.30 +/- 195.99
New best mean reward!
Eval num_timesteps=40000, episode_reward=-597.37 +/- 725.44
Episode length: 202.85 +/- 138.09
New best mean reward!
Eval num_timesteps=160000, episode_reward=-1101.58 +/- 1233.57
Episode length: 410.70 +/- 388.56
New best mean reward!
Eval num_timesteps=320000, episode_reward=-989.12 +/- 211.50
Episode length: 78.85 +/- 26.41
New best mean reward!
Eval num_timesteps=80000, episode_reward=-729.08 +/- 450.10
Episode length: 60.25 +/- 41.64
New best mean reward!
Eval num_timesteps=160000, episode_reward=-406.15 +/- 604.68
Episode length: 202.85 +/- 336.31
New best mean reward!
[W 2024-12-10 21:48:24,514] Trial 9 failed with parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.95, 'learning_rate': 0.7132753782667536, 'clip_range': 0.3, 'n_epochs': 4, 'gae_lambda': 0.98, 'use_sde': True, 'net_arch': 'tiny'} because of the following error: ValueError('Expected parameter loc (Tensor of shape (512, 2)) of distribution Normal(loc: torch.Size([512, 2]), scale: torch.Size([512, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        ...,\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]], grad_fn=<AddmmBackward0>)').
Traceback (most recent call last):
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/optuna/study/_optimize.py", line 197, in _run_trial
    value_or_values = func(trial)
  File "/mnt/personal/mrkosmic/synced/RL-cable/experiments/hyperopt/../../scripts/hyperopt.py", line 85, in objectiveRect2D
    model.learn(total_timesteps=TIMESTEPS, callback=[save_norm_clb, eval_clb])
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 336, in learn
    self.train()
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 213, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 737, in evaluate_actions
    distribution = self._get_action_dist_from_latent(latent_pi)
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 705, in _get_action_dist_from_latent
    return self.action_dist.proba_distribution(mean_actions, self.log_std, latent_pi)
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/common/distributions.py", line 555, in proba_distribution
    self.distribution = Normal(mean_actions, th.sqrt(variance + self.epsilon))
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/torch/distributions/normal.py", line 59, in __init__
    super().__init__(batch_shape, validate_args=validate_args)
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/torch/distributions/distribution.py", line 71, in __init__
    raise ValueError(
ValueError: Expected parameter loc (Tensor of shape (512, 2)) of distribution Normal(loc: torch.Size([512, 2]), scale: torch.Size([512, 2])) to satisfy the constraint Real(), but found invalid values:
tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        ...,
        [nan, nan],
        [nan, nan],
        [nan, nan]], grad_fn=<AddmmBackward0>)
[W 2024-12-10 21:48:24,547] Trial 9 failed with value None.
Eval num_timesteps=320000, episode_reward=-4578.88 +/- 1808.96
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=80000, episode_reward=-1013.65 +/- 957.14
Episode length: 131.40 +/- 167.00
Eval num_timesteps=160000, episode_reward=87.49 +/- 342.32
Episode length: 129.20 +/- 78.73
New best mean reward!
Eval num_timesteps=120000, episode_reward=-781.66 +/- 313.35
Episode length: 63.20 +/- 34.36
Eval num_timesteps=320000, episode_reward=-675.94 +/- 1174.87
Episode length: 337.85 +/- 363.92
New best mean reward!
Eval num_timesteps=160000, episode_reward=-1044.85 +/- 326.37
Episode length: 103.15 +/- 49.69
New best mean reward!
Eval num_timesteps=160000, episode_reward=-911.52 +/- 340.90
Episode length: 68.85 +/- 37.40
Eval num_timesteps=120000, episode_reward=-874.32 +/- 402.03
Episode length: 78.15 +/- 39.50
Eval num_timesteps=160000, episode_reward=-727.82 +/- 292.70
Episode length: 56.00 +/- 31.03
New best mean reward!
[I 2024-12-10 21:49:34,496] Trial 7 finished with value: -1669.3991094500002 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.9, 'learning_rate': 2.2694437981911393e-05, 'clip_range': 0.1, 'n_epochs': 4, 'gae_lambda': 0.9, 'use_sde': False, 'net_arch': 'tiny'}. Best is trial 7 with value: -1669.3991094500002.
Eval num_timesteps=480000, episode_reward=-340.58 +/- 712.37
Episode length: 262.35 +/- 290.39
New best mean reward!
[I 2024-12-10 21:49:56,152] Trial 5 finished with value: -732.4331840000001 and parameters: {'n_envs': 16, 'batch_size': 512, 'gamma': 0.95, 'learning_rate': 0.007940350610447176, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': True, 'net_arch': 'tiny'}. Best is trial 5 with value: -732.4331840000001.
Eval num_timesteps=320000, episode_reward=319.51 +/- 35.86
Episode length: 60.35 +/- 14.19
New best mean reward!
Eval num_timesteps=200000, episode_reward=-822.03 +/- 344.29
Episode length: 58.15 +/- 26.05
Eval num_timesteps=160000, episode_reward=-1300.35 +/- 978.26
Episode length: 149.30 +/- 102.22
Eval num_timesteps=320000, episode_reward=202.29 +/- 270.55
Episode length: 98.90 +/- 53.54
New best mean reward!
Eval num_timesteps=320000, episode_reward=316.19 +/- 43.33
Episode length: 65.95 +/- 19.89
New best mean reward!
Eval num_timesteps=240000, episode_reward=-682.71 +/- 439.68
Episode length: 62.30 +/- 19.60
New best mean reward!
Eval num_timesteps=40000, episode_reward=-790.53 +/- 432.76
Episode length: 71.20 +/- 40.59
New best mean reward!
Eval num_timesteps=200000, episode_reward=-1712.40 +/- 2867.70
Episode length: 121.35 +/- 131.88
Eval num_timesteps=280000, episode_reward=-837.94 +/- 115.80
Episode length: 68.45 +/- 25.16
Eval num_timesteps=320000, episode_reward=-1101.82 +/- 668.55
Episode length: 143.65 +/- 206.81
Eval num_timesteps=480000, episode_reward=301.08 +/- 37.75
Episode length: 75.60 +/- 22.27
Eval num_timesteps=80000, episode_reward=-830.39 +/- 507.84
Episode length: 92.45 +/- 61.52
[I 2024-12-10 21:51:41,738] Trial 3 finished with value: 319.13469210000005 and parameters: {'n_envs': 16, 'batch_size': 32, 'gamma': 0.9999, 'learning_rate': 0.005488925660092585, 'clip_range': 0.1, 'n_epochs': 4, 'gae_lambda': 0.8, 'use_sde': True, 'net_arch': 'tiny'}. Best is trial 3 with value: 319.13469210000005.
Eval num_timesteps=320000, episode_reward=-865.04 +/- 329.83
Episode length: 70.75 +/- 25.43
Eval num_timesteps=240000, episode_reward=-2637.17 +/- 2443.93
Episode length: 414.20 +/- 241.64
Eval num_timesteps=40000, episode_reward=-769.61 +/- 403.37
Episode length: 66.70 +/- 23.08
New best mean reward!
Eval num_timesteps=480000, episode_reward=258.39 +/- 93.33
Episode length: 89.80 +/- 36.46
New best mean reward!
Eval num_timesteps=320000, episode_reward=-871.01 +/- 177.44
Episode length: 65.35 +/- 25.10
Eval num_timesteps=360000, episode_reward=-810.37 +/- 181.09
Episode length: 60.10 +/- 32.57
[I 2024-12-10 21:52:27,536] Trial 8 finished with value: 264.0050026 and parameters: {'n_envs': 16, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.017110607312645777, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': True, 'net_arch': 'tiny'}. Best is trial 3 with value: 319.13469210000005.
Eval num_timesteps=160000, episode_reward=-740.62 +/- 151.71
Episode length: 39.90 +/- 18.25
New best mean reward!
Eval num_timesteps=120000, episode_reward=-892.34 +/- 395.89
Episode length: 71.60 +/- 28.66
[I 2024-12-10 21:52:53,683] Trial 0 finished with value: 326.75642625 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.00016705813304494397, 'clip_range': 0.1, 'n_epochs': 4, 'gae_lambda': 0.9, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 0 with value: 326.75642625.
Eval num_timesteps=280000, episode_reward=-2678.19 +/- 2085.56
Episode length: 468.45 +/- 340.90
Eval num_timesteps=400000, episode_reward=-839.45 +/- 156.51
Episode length: 64.85 +/- 34.21
Eval num_timesteps=320000, episode_reward=-869.65 +/- 177.78
Episode length: 64.60 +/- 24.96
[I 2024-12-10 21:53:20,761] Trial 12 pruned. 
Eval num_timesteps=480000, episode_reward=-1460.53 +/- 1184.86
Episode length: 237.05 +/- 267.63
[I 2024-12-10 21:53:34,451] Trial 6 pruned. 
Eval num_timesteps=160000, episode_reward=-926.84 +/- 535.66
Episode length: 67.45 +/- 40.11
[I 2024-12-10 21:53:35,182] Trial 11 pruned. 
Eval num_timesteps=440000, episode_reward=-922.43 +/- 407.46
Episode length: 82.70 +/- 28.85
Eval num_timesteps=40000, episode_reward=158.70 +/- 85.69
Episode length: 160.90 +/- 64.48
New best mean reward!
Eval num_timesteps=320000, episode_reward=-1888.70 +/- 1140.89
Episode length: 245.10 +/- 126.59
[I 2024-12-10 21:53:49,689] Trial 2 pruned. 
Eval num_timesteps=40000, episode_reward=-647.48 +/- 367.28
Episode length: 45.10 +/- 20.84
New best mean reward!
Eval num_timesteps=480000, episode_reward=-873.73 +/- 151.97
Episode length: 75.85 +/- 31.52
[I 2024-12-10 21:54:20,754] Trial 1 pruned. 
Eval num_timesteps=320000, episode_reward=-688.28 +/- 859.12
Episode length: 168.00 +/- 130.32
New best mean reward!
[I 2024-12-10 21:54:28,254] Trial 13 pruned. 
Eval num_timesteps=80000, episode_reward=240.86 +/- 90.87
Episode length: 125.35 +/- 51.06
New best mean reward!
Eval num_timesteps=80000, episode_reward=-603.17 +/- 532.53
Episode length: 70.75 +/- 37.37
New best mean reward!
Eval num_timesteps=80000, episode_reward=-804.21 +/- 595.18
Episode length: 57.90 +/- 39.04
Eval num_timesteps=480000, episode_reward=-780.04 +/- 128.15
Episode length: 53.30 +/- 25.32
[I 2024-12-10 21:54:52,053] Trial 4 pruned. 
Eval num_timesteps=160000, episode_reward=-3534.23 +/- 1221.96
Episode length: 952.15 +/- 208.57
New best mean reward!
[I 2024-12-10 21:54:54,302] Trial 16 pruned. 
Eval num_timesteps=120000, episode_reward=-789.84 +/- 211.39
Episode length: 42.35 +/- 18.34
Eval num_timesteps=120000, episode_reward=-847.49 +/- 379.26
Episode length: 953.05 +/- 204.65
Eval num_timesteps=160000, episode_reward=-884.66 +/- 474.42
Episode length: 43.95 +/- 23.43
[I 2024-12-10 21:55:56,707] Trial 17 pruned. 
Eval num_timesteps=160000, episode_reward=-3484.83 +/- 2774.36
Episode length: 957.15 +/- 186.78
Eval num_timesteps=200000, episode_reward=-1595.53 +/- 1317.81
Episode length: 149.10 +/- 133.51
Eval num_timesteps=120000, episode_reward=-818.31 +/- 224.67
Episode length: 52.55 +/- 29.62
Eval num_timesteps=240000, episode_reward=-847.11 +/- 195.45
Episode length: 48.65 +/- 18.30
Eval num_timesteps=320000, episode_reward=314.87 +/- 48.38
Episode length: 69.05 +/- 27.76
New best mean reward!
Eval num_timesteps=320000, episode_reward=-6344.69 +/- 4294.94
Episode length: 869.50 +/- 311.57
New best mean reward!
[I 2024-12-10 21:58:27,230] Trial 20 pruned. 
Eval num_timesteps=280000, episode_reward=-756.01 +/- 194.73
Episode length: 40.35 +/- 21.06
Eval num_timesteps=320000, episode_reward=-827.44 +/- 212.69
Episode length: 43.05 +/- 17.39
Eval num_timesteps=320000, episode_reward=353.75 +/- 38.02
Episode length: 45.45 +/- 12.97
New best mean reward!
Eval num_timesteps=360000, episode_reward=-716.33 +/- 153.05
Episode length: 33.55 +/- 14.29
Eval num_timesteps=160000, episode_reward=-932.35 +/- 418.85
Episode length: 64.80 +/- 30.94
[I 2024-12-10 22:00:27,990] Trial 10 pruned. 
Eval num_timesteps=400000, episode_reward=-716.54 +/- 76.06
Episode length: 36.45 +/- 9.57
[I 2024-12-10 22:00:56,312] Trial 15 finished with value: 305.80349814999994 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.9999, 'learning_rate': 0.0012545679524690718, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': True, 'net_arch': 'tiny'}. Best is trial 0 with value: 326.75642625.
Eval num_timesteps=440000, episode_reward=-828.00 +/- 342.80
Episode length: 40.75 +/- 15.19
Eval num_timesteps=480000, episode_reward=-746.08 +/- 141.62
Episode length: 37.45 +/- 14.01
[I 2024-12-10 22:01:56,509] Trial 14 pruned. 
Eval num_timesteps=320000, episode_reward=-2121.26 +/- 1763.74
Episode length: 507.45 +/- 446.63
New best mean reward!
[I 2024-12-10 22:03:17,699] Trial 22 pruned. 
[I 2024-12-10 22:03:17,872] Trial 18 finished with value: 340.48188074999996 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.00039629770270041754, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
Eval num_timesteps=320000, episode_reward=-934.13 +/- 773.67
Episode length: 508.40 +/- 446.39
New best mean reward!
[I 2024-12-10 22:03:56,090] Trial 23 pruned. 
Eval num_timesteps=320000, episode_reward=-203.57 +/- 267.41
Episode length: 358.55 +/- 138.46
New best mean reward!
[I 2024-12-10 22:04:03,883] Trial 27 pruned. 
Eval num_timesteps=320000, episode_reward=351.07 +/- 39.37
Episode length: 47.20 +/- 13.66
New best mean reward!
Eval num_timesteps=320000, episode_reward=349.74 +/- 39.02
Episode length: 48.20 +/- 13.60
New best mean reward!
Eval num_timesteps=320000, episode_reward=309.91 +/- 55.67
Episode length: 73.35 +/- 32.14
New best mean reward!
[I 2024-12-10 22:05:28,324] Trial 31 pruned. 
[I 2024-12-10 22:05:47,248] Trial 29 finished with value: 339.54886605 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.99, 'learning_rate': 0.00013114874650620794, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.9, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
[I 2024-12-10 22:05:47,764] Trial 28 finished with value: 336.3730987 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.99, 'learning_rate': 0.00010700267236212983, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.9, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
Eval num_timesteps=320000, episode_reward=-1875.73 +/- 2066.97
Episode length: 593.20 +/- 450.75
New best mean reward!
[I 2024-12-10 22:06:26,199] Trial 24 pruned. 
Eval num_timesteps=320000, episode_reward=306.60 +/- 53.31
Episode length: 82.55 +/- 33.92
New best mean reward!
[I 2024-12-10 22:07:18,296] Trial 33 pruned. 
Eval num_timesteps=320000, episode_reward=337.10 +/- 36.98
Episode length: 59.20 +/- 18.11
New best mean reward!
Eval num_timesteps=320000, episode_reward=55.63 +/- 106.17
Episode length: 233.90 +/- 74.98
New best mean reward!
[I 2024-12-10 22:07:55,899] Trial 35 pruned. 
Eval num_timesteps=320000, episode_reward=316.28 +/- 55.03
Episode length: 61.60 +/- 20.95
New best mean reward!
[I 2024-12-10 22:08:15,985] Trial 34 finished with value: 330.3567385 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.99, 'learning_rate': 6.493346662110781e-05, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.9, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
Eval num_timesteps=320000, episode_reward=-5734.31 +/- 4015.93
Episode length: 814.35 +/- 371.46
New best mean reward!
[I 2024-12-10 22:08:30,255] Trial 25 pruned. 
Eval num_timesteps=320000, episode_reward=255.00 +/- 106.32
Episode length: 119.15 +/- 73.50
New best mean reward!
[I 2024-12-10 22:08:43,647] Trial 36 pruned. 
Eval num_timesteps=320000, episode_reward=-1288.24 +/- 879.40
Episode length: 381.75 +/- 409.40
New best mean reward!
[I 2024-12-10 22:09:02,248] Trial 26 pruned. 
Eval num_timesteps=320000, episode_reward=317.28 +/- 57.40
Episode length: 65.50 +/- 25.84
New best mean reward!
[I 2024-12-10 22:09:26,641] Trial 37 pruned. 
Eval num_timesteps=320000, episode_reward=334.48 +/- 50.40
Episode length: 55.00 +/- 18.16
New best mean reward!
[I 2024-12-10 22:09:49,902] Trial 32 finished with value: 284.5225092 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.0009005658820938755, 'clip_range': 0.4, 'n_epochs': 4, 'gae_lambda': 0.9, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
Eval num_timesteps=320000, episode_reward=348.09 +/- 41.95
Episode length: 50.30 +/- 17.99
New best mean reward!
Eval num_timesteps=320000, episode_reward=-2109.72 +/- 747.54
Episode length: 939.25 +/- 167.09
New best mean reward!
[I 2024-12-10 22:10:13,624] Trial 39 pruned. 
Eval num_timesteps=320000, episode_reward=316.91 +/- 42.37
Episode length: 71.95 +/- 22.83
New best mean reward!
Eval num_timesteps=320000, episode_reward=347.04 +/- 41.73
Episode length: 51.25 +/- 18.26
New best mean reward!
[I 2024-12-10 22:10:50,506] Trial 38 finished with value: 320.79351965 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.99, 'learning_rate': 0.0012583511314366263, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.9, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
Eval num_timesteps=320000, episode_reward=348.42 +/- 41.82
Episode length: 48.95 +/- 17.21
New best mean reward!
Eval num_timesteps=320000, episode_reward=-663.63 +/- 659.86
Episode length: 792.95 +/- 378.29
New best mean reward!
[I 2024-12-10 22:11:07,697] Trial 21 pruned. 
[I 2024-12-10 22:11:16,882] Trial 40 finished with value: 331.34103624999995 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.99, 'learning_rate': 0.00014939853747667913, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.9, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
Eval num_timesteps=320000, episode_reward=351.41 +/- 38.99
Episode length: 47.10 +/- 13.51
New best mean reward!
[I 2024-12-10 22:11:36,119] Trial 41 finished with value: 338.19216730000005 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.99, 'learning_rate': 0.00010127004168362392, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.9, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
Eval num_timesteps=320000, episode_reward=350.55 +/- 41.08
Episode length: 48.20 +/- 16.92
New best mean reward!
[I 2024-12-10 22:11:58,588] Trial 42 finished with value: 336.62020155 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.99, 'learning_rate': 0.00012843378677914263, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.9, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
[I 2024-12-10 22:12:16,758] Trial 43 finished with value: 339.0843661 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.99, 'learning_rate': 0.00012134146270135403, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.9, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
Eval num_timesteps=320000, episode_reward=315.27 +/- 58.39
Episode length: 68.80 +/- 25.59
New best mean reward!
[I 2024-12-10 22:12:27,703] Trial 19 pruned. 
Eval num_timesteps=320000, episode_reward=340.99 +/- 49.02
Episode length: 57.20 +/- 26.07
New best mean reward!
Eval num_timesteps=320000, episode_reward=300.89 +/- 62.85
Episode length: 83.10 +/- 34.49
New best mean reward!
[I 2024-12-10 22:12:45,399] Trial 46 pruned. 
[I 2024-12-10 22:12:55,119] Trial 44 finished with value: 339.45436385000005 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.99, 'learning_rate': 0.00014877188914628727, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
Eval num_timesteps=320000, episode_reward=347.22 +/- 45.27
Episode length: 52.40 +/- 23.96
New best mean reward!
Eval num_timesteps=40000, episode_reward=-1731.90 +/- 742.18
Episode length: 927.85 +/- 216.46
New best mean reward!
Eval num_timesteps=320000, episode_reward=345.00 +/- 44.61
Episode length: 53.75 +/- 23.72
New best mean reward!
[I 2024-12-10 22:13:16,188] Trial 30 finished with value: 335.66521040000003 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 8.005459295306434e-05, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.9, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
[I 2024-12-10 22:13:30,876] Trial 45 finished with value: 337.6978717 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.95, 'learning_rate': 0.0001222930852502434, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
Eval num_timesteps=40000, episode_reward=-3882.36 +/- 940.59
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=40000, episode_reward=274.06 +/- 62.97
Episode length: 85.65 +/- 24.23
New best mean reward!
[I 2024-12-10 22:13:57,396] Trial 47 finished with value: 337.86078555 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.95, 'learning_rate': 0.00015626073815218307, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
Eval num_timesteps=320000, episode_reward=348.04 +/- 43.30
Episode length: 51.90 +/- 21.97
New best mean reward!
Eval num_timesteps=80000, episode_reward=-365.08 +/- 620.33
Episode length: 266.60 +/- 242.25
New best mean reward!
Eval num_timesteps=40000, episode_reward=-776.68 +/- 400.37
Episode length: 697.85 +/- 266.54
New best mean reward!
[I 2024-12-10 22:14:16,325] Trial 48 finished with value: 337.23738084999997 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.95, 'learning_rate': 0.0001106177403853264, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
Eval num_timesteps=80000, episode_reward=-1718.31 +/- 971.66
Episode length: 861.95 +/- 308.76
New best mean reward!
Eval num_timesteps=320000, episode_reward=234.16 +/- 126.44
Episode length: 117.15 +/- 74.66
New best mean reward!
[I 2024-12-10 22:14:26,026] Trial 51 pruned. 
Eval num_timesteps=80000, episode_reward=243.77 +/- 300.85
Episode length: 50.85 +/- 27.10
Eval num_timesteps=40000, episode_reward=-646.74 +/- 412.08
Episode length: 620.15 +/- 296.41
New best mean reward!
Eval num_timesteps=120000, episode_reward=-681.77 +/- 574.27
Episode length: 163.05 +/- 68.81
Eval num_timesteps=80000, episode_reward=285.87 +/- 91.40
Episode length: 90.60 +/- 42.50
New best mean reward!
[I 2024-12-10 22:14:52,936] Trial 50 finished with value: 338.5620877 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.95, 'learning_rate': 0.00014229054756496597, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
Eval num_timesteps=40000, episode_reward=-715.32 +/- 776.56
Episode length: 722.80 +/- 423.49
New best mean reward!
Eval num_timesteps=120000, episode_reward=-155.40 +/- 113.95
Episode length: 347.10 +/- 89.03
New best mean reward!
Eval num_timesteps=120000, episode_reward=314.21 +/- 48.95
Episode length: 61.35 +/- 20.44
New best mean reward!
Eval num_timesteps=80000, episode_reward=295.79 +/- 84.66
Episode length: 105.30 +/- 59.40
New best mean reward!
Eval num_timesteps=40000, episode_reward=-2414.42 +/- 2435.97
Episode length: 250.50 +/- 156.83
New best mean reward!
Eval num_timesteps=160000, episode_reward=332.10 +/- 48.36
Episode length: 55.50 +/- 19.47
New best mean reward!
Eval num_timesteps=160000, episode_reward=211.78 +/- 54.40
Episode length: 134.90 +/- 45.14
New best mean reward!
Eval num_timesteps=120000, episode_reward=311.93 +/- 53.82
Episode length: 76.65 +/- 35.06
New best mean reward!
Eval num_timesteps=160000, episode_reward=89.88 +/- 205.02
Episode length: 142.65 +/- 75.77
New best mean reward!
Eval num_timesteps=40000, episode_reward=-2557.35 +/- 2332.96
Episode length: 326.50 +/- 356.59
New best mean reward!
Eval num_timesteps=80000, episode_reward=-4410.35 +/- 3448.84
Episode length: 900.10 +/- 299.70
Eval num_timesteps=120000, episode_reward=294.47 +/- 69.20
Episode length: 87.65 +/- 45.42
Eval num_timesteps=200000, episode_reward=349.24 +/- 54.07
Episode length: 51.50 +/- 24.12
New best mean reward!
Eval num_timesteps=200000, episode_reward=313.43 +/- 55.90
Episode length: 79.60 +/- 44.14
New best mean reward!
Eval num_timesteps=160000, episode_reward=-998.57 +/- 437.64
Episode length: 954.60 +/- 197.89
New best mean reward!
[I 2024-12-10 22:16:27,835] Trial 57 pruned. 
Eval num_timesteps=80000, episode_reward=-668.34 +/- 1085.24
Episode length: 583.80 +/- 461.35
New best mean reward!
Eval num_timesteps=200000, episode_reward=210.86 +/- 133.71
Episode length: 99.95 +/- 55.75
New best mean reward!
Eval num_timesteps=160000, episode_reward=328.99 +/- 53.24
Episode length: 62.60 +/- 27.69
New best mean reward!
Eval num_timesteps=80000, episode_reward=-821.60 +/- 514.54
Episode length: 72.80 +/- 46.00
New best mean reward!
Eval num_timesteps=240000, episode_reward=334.87 +/- 41.05
Episode length: 55.85 +/- 19.20
Eval num_timesteps=240000, episode_reward=330.90 +/- 46.57
Episode length: 64.45 +/- 33.70
New best mean reward!
Eval num_timesteps=160000, episode_reward=331.72 +/- 51.14
Episode length: 59.30 +/- 25.96
New best mean reward!
Eval num_timesteps=120000, episode_reward=-341.27 +/- 537.16
Episode length: 644.80 +/- 435.86
New best mean reward!
Eval num_timesteps=40000, episode_reward=60.70 +/- 360.58
Episode length: 291.30 +/- 356.72
New best mean reward!
Eval num_timesteps=280000, episode_reward=318.15 +/- 36.41
Episode length: 63.40 +/- 19.69
Eval num_timesteps=240000, episode_reward=276.39 +/- 110.18
Episode length: 95.65 +/- 73.10
New best mean reward!
Eval num_timesteps=200000, episode_reward=356.39 +/- 51.63
Episode length: 46.90 +/- 19.49
New best mean reward!
Eval num_timesteps=280000, episode_reward=298.28 +/- 49.13
Episode length: 84.10 +/- 34.08
Eval num_timesteps=120000, episode_reward=296.90 +/- 46.72
Episode length: 79.55 +/- 21.72
New best mean reward!
Eval num_timesteps=120000, episode_reward=-929.63 +/- 411.34
Episode length: 88.80 +/- 45.91
Eval num_timesteps=200000, episode_reward=356.14 +/- 52.04
Episode length: 47.80 +/- 21.70
New best mean reward!
Eval num_timesteps=320000, episode_reward=341.89 +/- 40.87
Episode length: 50.95 +/- 16.20
Eval num_timesteps=320000, episode_reward=317.98 +/- 57.16
Episode length: 73.60 +/- 35.08
[I 2024-12-10 22:18:01,530] Trial 52 pruned. 
Eval num_timesteps=160000, episode_reward=-583.70 +/- 566.21
Episode length: 817.65 +/- 365.49
Eval num_timesteps=80000, episode_reward=350.78 +/- 60.82
Episode length: 48.85 +/- 20.25
New best mean reward!
Eval num_timesteps=280000, episode_reward=253.49 +/- 74.19
Episode length: 115.75 +/- 53.49
Eval num_timesteps=240000, episode_reward=341.32 +/- 39.54
Episode length: 55.20 +/- 21.54
Eval num_timesteps=160000, episode_reward=-1018.68 +/- 575.84
Episode length: 97.40 +/- 65.67
[I 2024-12-10 22:18:26,675] Trial 59 pruned. 
Eval num_timesteps=160000, episode_reward=-366.36 +/- 1410.69
Episode length: 390.25 +/- 447.69
Eval num_timesteps=360000, episode_reward=310.42 +/- 38.09
Episode length: 68.30 +/- 22.28
Eval num_timesteps=240000, episode_reward=340.28 +/- 38.10
Episode length: 54.15 +/- 17.64
Eval num_timesteps=40000, episode_reward=-1782.00 +/- 1236.48
Episode length: 764.80 +/- 407.42
New best mean reward!
Eval num_timesteps=120000, episode_reward=326.39 +/- 44.61
Episode length: 58.60 +/- 17.15
Eval num_timesteps=320000, episode_reward=302.44 +/- 63.46
Episode length: 73.25 +/- 31.98
New best mean reward!
[I 2024-12-10 22:19:12,538] Trial 49 pruned. 
Eval num_timesteps=200000, episode_reward=-1100.40 +/- 962.36
Episode length: 810.45 +/- 379.28
Eval num_timesteps=280000, episode_reward=321.01 +/- 33.95
Episode length: 60.20 +/- 16.05
Eval num_timesteps=400000, episode_reward=322.42 +/- 22.89
Episode length: 58.40 +/- 13.20
Eval num_timesteps=160000, episode_reward=322.04 +/- 31.87
Episode length: 57.95 +/- 14.08
New best mean reward!
Eval num_timesteps=280000, episode_reward=321.50 +/- 34.19
Episode length: 60.50 +/- 17.45
Eval num_timesteps=200000, episode_reward=-1044.09 +/- 1695.54
Episode length: 631.30 +/- 451.80
Eval num_timesteps=440000, episode_reward=328.76 +/- 41.14
Episode length: 55.05 +/- 14.54
Eval num_timesteps=80000, episode_reward=-723.73 +/- 852.73
Episode length: 152.65 +/- 108.65
New best mean reward!
Eval num_timesteps=160000, episode_reward=336.92 +/- 50.62
Episode length: 54.50 +/- 21.97
Eval num_timesteps=160000, episode_reward=324.74 +/- 30.92
Episode length: 55.80 +/- 10.63
New best mean reward!
Eval num_timesteps=320000, episode_reward=341.28 +/- 40.20
Episode length: 50.80 +/- 14.11
Eval num_timesteps=320000, episode_reward=340.56 +/- 36.24
Episode length: 49.85 +/- 12.33
New best mean reward!
[I 2024-12-10 22:20:04,094] Trial 62 pruned. 
Eval num_timesteps=240000, episode_reward=-278.19 +/- 503.49
Episode length: 628.45 +/- 383.84
New best mean reward!
Eval num_timesteps=480000, episode_reward=324.49 +/- 37.24
Episode length: 58.95 +/- 18.95
Eval num_timesteps=320000, episode_reward=339.87 +/- 42.63
Episode length: 53.15 +/- 18.31
[I 2024-12-10 22:20:45,240] Trial 54 finished with value: 315.6830098 and parameters: {'n_envs': 4, 'batch_size': 256, 'gamma': 0.95, 'learning_rate': 0.002127746165894433, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'small'}. Best is trial 18 with value: 340.48188074999996.
Eval num_timesteps=240000, episode_reward=-77.64 +/- 513.57
Episode length: 445.75 +/- 452.72
[I 2024-12-10 22:20:50,568] Trial 58 pruned. 
Eval num_timesteps=120000, episode_reward=-2336.86 +/- 2642.16
Episode length: 397.75 +/- 369.28
[I 2024-12-10 22:20:51,156] Trial 61 pruned. 
Eval num_timesteps=320000, episode_reward=341.26 +/- 36.14
Episode length: 49.60 +/- 12.31
New best mean reward!
[I 2024-12-10 22:20:52,664] Trial 63 pruned. 
Eval num_timesteps=200000, episode_reward=357.66 +/- 52.86
Episode length: 45.65 +/- 19.80
New best mean reward!
Eval num_timesteps=360000, episode_reward=319.59 +/- 36.63
Episode length: 59.50 +/- 15.44
Eval num_timesteps=280000, episode_reward=-378.25 +/- 638.31
Episode length: 563.90 +/- 436.41
[I 2024-12-10 22:21:12,034] Trial 56 pruned. 
Eval num_timesteps=360000, episode_reward=319.27 +/- 36.68
Episode length: 60.05 +/- 15.69
Eval num_timesteps=240000, episode_reward=342.37 +/- 39.22
Episode length: 52.45 +/- 20.03
Eval num_timesteps=400000, episode_reward=325.92 +/- 18.13
Episode length: 55.75 +/- 6.50
Eval num_timesteps=320000, episode_reward=352.93 +/- 38.82
Episode length: 45.90 +/- 13.25
New best mean reward!
Eval num_timesteps=400000, episode_reward=324.83 +/- 19.37
Episode length: 56.30 +/- 6.73
Eval num_timesteps=280000, episode_reward=325.81 +/- 31.52
Episode length: 55.00 +/- 10.84
Eval num_timesteps=440000, episode_reward=335.64 +/- 42.94
Episode length: 54.75 +/- 18.01
Eval num_timesteps=320000, episode_reward=353.06 +/- 38.73
Episode length: 45.95 +/- 13.21
New best mean reward!
Eval num_timesteps=440000, episode_reward=336.89 +/- 41.61
Episode length: 52.30 +/- 14.75
Eval num_timesteps=320000, episode_reward=250.52 +/- 115.67
Episode length: 111.20 +/- 71.58
New best mean reward!
[I 2024-12-10 22:22:58,035] Trial 65 pruned. 
Eval num_timesteps=320000, episode_reward=345.96 +/- 38.55
Episode length: 48.20 +/- 13.28
Eval num_timesteps=480000, episode_reward=327.14 +/- 31.74
Episode length: 55.05 +/- 10.94
[I 2024-12-10 22:23:27,547] Trial 64 finished with value: 340.71502375 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.00026414196326859273, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'small'}. Best is trial 64 with value: 340.71502375.
Eval num_timesteps=480000, episode_reward=327.75 +/- 30.62
Episode length: 55.15 +/- 10.75
[I 2024-12-10 22:23:58,526] Trial 53 finished with value: 319.56661615 and parameters: {'n_envs': 4, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 3.059850287501685e-05, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'small'}. Best is trial 64 with value: 340.71502375.
Eval num_timesteps=320000, episode_reward=351.72 +/- 40.49
Episode length: 46.40 +/- 14.08
New best mean reward!
[I 2024-12-10 22:24:05,975] Trial 67 finished with value: 340.84431915 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.00027838440620927364, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 67 with value: 340.84431915.
Eval num_timesteps=360000, episode_reward=325.49 +/- 34.75
Episode length: 55.05 +/- 11.89
[I 2024-12-10 22:24:16,293] Trial 55 finished with value: 320.10737830000005 and parameters: {'n_envs': 4, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 2.7958370308920788e-05, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'small'}. Best is trial 67 with value: 340.84431915.
Eval num_timesteps=320000, episode_reward=353.38 +/- 38.80
Episode length: 45.75 +/- 13.42
New best mean reward!
Eval num_timesteps=320000, episode_reward=235.29 +/- 136.68
Episode length: 102.30 +/- 56.00
New best mean reward!
[I 2024-12-10 22:24:26,702] Trial 66 pruned. 
Eval num_timesteps=400000, episode_reward=331.34 +/- 16.97
Episode length: 53.10 +/- 5.76
Eval num_timesteps=320000, episode_reward=331.15 +/- 63.93
Episode length: 56.85 +/- 23.57
New best mean reward!
[I 2024-12-10 22:25:25,514] Trial 70 pruned. 
Eval num_timesteps=440000, episode_reward=339.76 +/- 41.29
Episode length: 50.30 +/- 14.18
[I 2024-12-10 22:26:09,735] Trial 68 finished with value: 340.5865897 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.00030299383169332867, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 67 with value: 340.84431915.
[I 2024-12-10 22:26:24,347] Trial 69 finished with value: 340.90472289999997 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.00025797784114747703, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 69 with value: 340.90472289999997.
Eval num_timesteps=320000, episode_reward=352.92 +/- 39.25
Episode length: 45.80 +/- 13.43
New best mean reward!
Eval num_timesteps=480000, episode_reward=329.02 +/- 36.85
Episode length: 55.50 +/- 18.03
Eval num_timesteps=320000, episode_reward=352.35 +/- 39.50
Episode length: 45.95 +/- 13.49
New best mean reward!
Eval num_timesteps=320000, episode_reward=346.64 +/- 43.70
Episode length: 53.30 +/- 22.73
New best mean reward!
[I 2024-12-10 22:27:01,788] Trial 76 pruned. 
Eval num_timesteps=320000, episode_reward=353.57 +/- 38.81
Episode length: 45.70 +/- 13.42
New best mean reward!
[I 2024-12-10 22:27:15,529] Trial 60 finished with value: 322.98547859999996 and parameters: {'n_envs': 4, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.00022368722766958973, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'small'}. Best is trial 69 with value: 340.90472289999997.
Eval num_timesteps=320000, episode_reward=353.64 +/- 38.42
Episode length: 45.70 +/- 13.22
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.50 +/- 39.42
Episode length: 46.00 +/- 13.51
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.09 +/- 39.94
Episode length: 47.20 +/- 15.51
New best mean reward!
[I 2024-12-10 22:28:12,019] Trial 75 finished with value: 340.99193355 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.00032252350428245153, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 75 with value: 340.99193355.
[I 2024-12-10 22:28:22,347] Trial 71 finished with value: 340.74117149999995 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.00027500164579357555, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 75 with value: 340.99193355.
Eval num_timesteps=320000, episode_reward=-1605.40 +/- 617.90
Episode length: 189.25 +/- 69.07
New best mean reward!
[I 2024-12-10 22:28:41,700] Trial 78 pruned. 
[I 2024-12-10 22:29:22,823] Trial 72 finished with value: 340.3606689 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.0002634369933275503, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 75 with value: 340.99193355.
[I 2024-12-10 22:29:28,254] Trial 77 finished with value: 340.2811354 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.0002292975774505178, 'clip_range': 0.4, 'n_epochs': 4, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 75 with value: 340.99193355.
[I 2024-12-10 22:29:30,524] Trial 73 finished with value: 341.16808435 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.00022564862913168051, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 73 with value: 341.16808435.
[I 2024-12-10 22:29:43,662] Trial 74 finished with value: 341.12561075 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.00024132272600465093, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 73 with value: 341.16808435.
Eval num_timesteps=320000, episode_reward=352.23 +/- 39.68
Episode length: 46.25 +/- 13.75
New best mean reward!
Eval num_timesteps=320000, episode_reward=-820.18 +/- 374.50
Episode length: 53.30 +/- 22.61
New best mean reward!
[I 2024-12-10 22:31:11,089] Trial 80 pruned. 
Eval num_timesteps=320000, episode_reward=352.50 +/- 39.94
Episode length: 46.10 +/- 13.77
New best mean reward!
Eval num_timesteps=320000, episode_reward=350.61 +/- 39.73
Episode length: 48.60 +/- 14.94
New best mean reward!
[I 2024-12-10 22:32:19,747] Trial 79 finished with value: 341.00142945000005 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.00020253478862479414, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 73 with value: 341.16808435.
Eval num_timesteps=320000, episode_reward=-915.35 +/- 240.24
Episode length: 61.55 +/- 22.60
New best mean reward!
[I 2024-12-10 22:32:30,909] Trial 83 pruned. 
Eval num_timesteps=320000, episode_reward=353.08 +/- 39.41
Episode length: 46.10 +/- 13.66
New best mean reward!
[I 2024-12-10 22:33:02,508] Trial 82 finished with value: 341.07319175000003 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.0003829959268587831, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 73 with value: 341.16808435.
[I 2024-12-10 22:34:12,755] Trial 81 finished with value: 339.30734359999997 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.00046306144943643557, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 73 with value: 341.16808435.
[I 2024-12-10 22:34:52,439] Trial 84 finished with value: 52.77062259999999 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.00047782412796577894, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 73 with value: 341.16808435.
Eval num_timesteps=320000, episode_reward=350.80 +/- 40.57
Episode length: 47.25 +/- 14.22
New best mean reward!
Eval num_timesteps=320000, episode_reward=345.77 +/- 46.02
Episode length: 48.60 +/- 16.33
New best mean reward!
[I 2024-12-10 22:35:05,610] Trial 86 pruned. 
Eval num_timesteps=320000, episode_reward=-27.58 +/- 528.00
Episode length: 44.50 +/- 14.51
New best mean reward!
[I 2024-12-10 22:35:30,167] Trial 87 pruned. 
Eval num_timesteps=320000, episode_reward=343.46 +/- 51.10
Episode length: 50.60 +/- 18.01
New best mean reward!
[I 2024-12-10 22:36:55,412] Trial 88 pruned. 
Eval num_timesteps=320000, episode_reward=331.08 +/- 78.40
Episode length: 51.45 +/- 22.14
New best mean reward!
[I 2024-12-10 22:37:14,282] Trial 94 pruned. 
Eval num_timesteps=320000, episode_reward=354.29 +/- 37.71
Episode length: 45.45 +/- 12.92
New best mean reward!
Eval num_timesteps=320000, episode_reward=351.35 +/- 41.26
Episode length: 46.65 +/- 14.21
New best mean reward!
Eval num_timesteps=320000, episode_reward=345.77 +/- 55.71
Episode length: 47.45 +/- 16.20
New best mean reward!
[I 2024-12-10 22:37:56,056] Trial 89 pruned. 
Eval num_timesteps=320000, episode_reward=353.31 +/- 39.07
Episode length: 45.80 +/- 13.27
New best mean reward!
Eval num_timesteps=320000, episode_reward=307.04 +/- 72.65
Episode length: 69.20 +/- 29.67
New best mean reward!
[I 2024-12-10 22:38:05,829] Trial 93 pruned. 
[I 2024-12-10 22:38:47,620] Trial 85 finished with value: 338.44209044999997 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.00037113106068048155, 'clip_range': 0.4, 'n_epochs': 20, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 73 with value: 341.16808435.
[I 2024-12-10 22:39:07,253] Trial 95 finished with value: 340.98170485 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.000851194132585138, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 73 with value: 341.16808435.
Eval num_timesteps=320000, episode_reward=353.39 +/- 38.39
Episode length: 45.75 +/- 13.09
New best mean reward!
Eval num_timesteps=320000, episode_reward=-984.65 +/- 743.08
Episode length: 150.85 +/- 73.58
New best mean reward!
[I 2024-12-10 22:39:38,104] Trial 97 pruned. 
Eval num_timesteps=160000, episode_reward=312.44 +/- 29.60
Episode length: 64.15 +/- 11.24
New best mean reward!
[I 2024-12-10 22:40:00,165] Trial 99 pruned. 
Eval num_timesteps=320000, episode_reward=343.55 +/- 39.61
Episode length: 51.85 +/- 16.02
New best mean reward!
[I 2024-12-10 22:40:10,509] Trial 98 pruned. 
[I 2024-12-10 22:40:30,893] Trial 96 finished with value: 340.90634644999994 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.00019189672945460845, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 73 with value: 341.16808435.
Eval num_timesteps=320000, episode_reward=-499.86 +/- 1707.09
Episode length: 228.80 +/- 328.62
New best mean reward!
[I 2024-12-10 22:40:45,922] Trial 92 pruned. 
[I 2024-12-10 22:41:01,765] Trial 91 finished with value: 221.29585834999997 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.0004002228182448402, 'clip_range': 0.4, 'n_epochs': 20, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 73 with value: 341.16808435.
Eval num_timesteps=320000, episode_reward=353.74 +/- 38.49
Episode length: 45.45 +/- 13.15
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.44 +/- 38.62
Episode length: 45.75 +/- 13.28
New best mean reward!
[I 2024-12-10 22:41:45,391] Trial 90 finished with value: 182.1337746 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.0003373936892721724, 'clip_range': 0.4, 'n_epochs': 20, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 73 with value: 341.16808435.
Eval num_timesteps=320000, episode_reward=345.37 +/- 43.24
Episode length: 48.95 +/- 14.77
New best mean reward!
[I 2024-12-10 22:42:01,634] Trial 100 pruned. 
Eval num_timesteps=320000, episode_reward=344.40 +/- 44.55
Episode length: 50.20 +/- 16.10
New best mean reward!
[I 2024-12-10 22:42:09,351] Trial 103 pruned. 
Eval num_timesteps=320000, episode_reward=352.53 +/- 39.57
Episode length: 46.05 +/- 13.41
New best mean reward!
[I 2024-12-10 22:42:40,701] Trial 101 finished with value: 341.2645613 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0007831444932703005, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=352.76 +/- 38.82
Episode length: 46.05 +/- 13.33
New best mean reward!
Eval num_timesteps=320000, episode_reward=212.00 +/- 85.88
Episode length: 127.50 +/- 48.93
New best mean reward!
[I 2024-12-10 22:42:58,180] Trial 107 pruned. 
[I 2024-12-10 22:43:08,178] Trial 102 finished with value: 341.16381585 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.00020126472835684466, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=228.82 +/- 103.75
Episode length: 123.00 +/- 56.17
New best mean reward!
[I 2024-12-10 22:43:08,475] Trial 106 pruned. 
[I 2024-12-10 22:43:46,717] Trial 104 finished with value: 340.54343975 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0007875806847314669, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=229.66 +/- 155.05
Episode length: 94.35 +/- 46.60
New best mean reward!
[I 2024-12-10 22:43:55,787] Trial 108 pruned. 
[I 2024-12-10 22:44:05,678] Trial 105 finished with value: 340.8823099 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0001915056666358707, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=238.85 +/- 199.87
Episode length: 75.80 +/- 43.39
New best mean reward!
[I 2024-12-10 22:44:14,729] Trial 109 pruned. 
Eval num_timesteps=320000, episode_reward=318.80 +/- 60.05
Episode length: 69.30 +/- 31.96
New best mean reward!
[I 2024-12-10 22:44:20,722] Trial 110 pruned. 
Eval num_timesteps=320000, episode_reward=353.42 +/- 38.56
Episode length: 45.75 +/- 13.19
New best mean reward!
Eval num_timesteps=320000, episode_reward=299.29 +/- 52.65
Episode length: 83.25 +/- 31.39
New best mean reward!
[I 2024-12-10 22:44:55,545] Trial 111 pruned. 
Eval num_timesteps=160000, episode_reward=-2249.09 +/- 1248.95
Episode length: 413.50 +/- 311.16
New best mean reward!
[I 2024-12-10 22:45:14,463] Trial 118 pruned. 
Eval num_timesteps=320000, episode_reward=353.41 +/- 38.41
Episode length: 45.85 +/- 13.11
New best mean reward!
Eval num_timesteps=160000, episode_reward=-1046.62 +/- 276.31
Episode length: 951.20 +/- 212.71
New best mean reward!
[I 2024-12-10 22:45:31,560] Trial 119 pruned. 
Eval num_timesteps=160000, episode_reward=305.53 +/- 42.22
Episode length: 80.50 +/- 29.38
New best mean reward!
[I 2024-12-10 22:45:32,701] Trial 117 pruned. 
Eval num_timesteps=320000, episode_reward=353.39 +/- 38.96
Episode length: 45.70 +/- 13.26
New best mean reward!
Eval num_timesteps=320000, episode_reward=336.21 +/- 45.77
Episode length: 54.60 +/- 16.21
New best mean reward!
[I 2024-12-10 22:46:01,049] Trial 115 pruned. 
Eval num_timesteps=320000, episode_reward=347.52 +/- 40.78
Episode length: 49.90 +/- 15.27
New best mean reward!
[I 2024-12-10 22:46:07,521] Trial 116 pruned. 
[I 2024-12-10 22:46:12,900] Trial 112 finished with value: 341.09753625 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0008928702746684536, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 22:46:41,326] Trial 113 finished with value: 336.46608725000004 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0008992407420213455, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 22:46:57,693] Trial 114 finished with value: 340.89946955 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0008226021219106019, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=347.55 +/- 40.40
Episode length: 50.15 +/- 15.01
New best mean reward!
[I 2024-12-10 22:47:14,682] Trial 120 pruned. 
Eval num_timesteps=320000, episode_reward=352.51 +/- 39.96
Episode length: 46.10 +/- 13.81
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.04 +/- 38.83
Episode length: 45.85 +/- 13.38
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.65 +/- 38.51
Episode length: 45.70 +/- 13.26
New best mean reward!
Eval num_timesteps=320000, episode_reward=351.00 +/- 40.52
Episode length: 48.10 +/- 16.85
New best mean reward!
[I 2024-12-10 22:48:10,773] Trial 126 pruned. 
Eval num_timesteps=320000, episode_reward=350.95 +/- 40.63
Episode length: 46.85 +/- 14.14
New best mean reward!
[I 2024-12-10 22:48:15,345] Trial 124 pruned. 
Eval num_timesteps=320000, episode_reward=353.88 +/- 38.29
Episode length: 45.45 +/- 13.17
New best mean reward!
[I 2024-12-10 22:48:50,137] Trial 121 finished with value: 340.98333355 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0006497051557473028, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=351.06 +/- 40.51
Episode length: 46.40 +/- 13.76
New best mean reward!
[I 2024-12-10 22:48:59,216] Trial 127 pruned. 
[I 2024-12-10 22:49:11,941] Trial 122 finished with value: 341.16076825 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.00031635233711546363, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 22:49:12,112] Trial 123 finished with value: 341.11443435 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0006078632135711059, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=343.71 +/- 35.05
Episode length: 51.05 +/- 13.40
New best mean reward!
[I 2024-12-10 22:49:21,676] Trial 128 pruned. 
Eval num_timesteps=320000, episode_reward=352.28 +/- 39.22
Episode length: 46.30 +/- 13.58
New best mean reward!
[I 2024-12-10 22:49:44,424] Trial 125 finished with value: 341.07247974999996 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0006725855140368634, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=352.32 +/- 39.47
Episode length: 46.00 +/- 13.46
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.14 +/- 39.87
Episode length: 46.20 +/- 13.68
New best mean reward!
[I 2024-12-10 22:50:20,204] Trial 131 pruned. 
[I 2024-12-10 22:50:46,890] Trial 129 finished with value: 341.13530390000005 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0005898066829372508, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=351.34 +/- 40.61
Episode length: 46.65 +/- 13.93
New best mean reward!
[I 2024-12-10 22:50:59,289] Trial 132 pruned. 
Eval num_timesteps=320000, episode_reward=341.94 +/- 38.91
Episode length: 54.55 +/- 16.39
New best mean reward!
[I 2024-12-10 22:51:00,402] Trial 136 pruned. 
Eval num_timesteps=320000, episode_reward=353.72 +/- 38.36
Episode length: 45.65 +/- 13.02
New best mean reward!
[I 2024-12-10 22:51:16,091] Trial 130 finished with value: 341.03168559999995 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0007665726951308398, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=353.61 +/- 38.25
Episode length: 45.80 +/- 13.12
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.53 +/- 39.10
Episode length: 46.05 +/- 13.35
New best mean reward!
Eval num_timesteps=320000, episode_reward=-4415.00 +/- 2636.65
Episode length: 953.20 +/- 204.00
New best mean reward!
[I 2024-12-10 22:51:30,976] Trial 137 pruned. 
Eval num_timesteps=320000, episode_reward=-3270.44 +/- 2312.19
Episode length: 903.35 +/- 289.95
New best mean reward!
[I 2024-12-10 22:52:02,806] Trial 138 pruned. 
Eval num_timesteps=320000, episode_reward=347.84 +/- 41.66
Episode length: 51.50 +/- 19.07
New best mean reward!
[I 2024-12-10 22:52:18,098] Trial 139 pruned. 
[I 2024-12-10 22:52:25,558] Trial 133 finished with value: 341.124066 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0005552944930889354, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 22:52:40,516] Trial 135 finished with value: 341.0444266999999 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0003158026487530071, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 22:52:40,531] Trial 134 finished with value: 341.1177174 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0003424559274469675, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=352.65 +/- 39.23
Episode length: 46.00 +/- 13.39
New best mean reward!
Eval num_timesteps=320000, episode_reward=162.36 +/- 257.89
Episode length: 120.80 +/- 78.24
New best mean reward!
[I 2024-12-10 22:53:12,342] Trial 142 pruned. 
Eval num_timesteps=320000, episode_reward=-662.74 +/- 1124.93
Episode length: 446.15 +/- 453.40
New best mean reward!
[I 2024-12-10 22:53:19,476] Trial 141 pruned. 
Eval num_timesteps=320000, episode_reward=351.29 +/- 40.55
Episode length: 46.20 +/- 13.79
New best mean reward!
[I 2024-12-10 22:53:41,427] Trial 143 pruned. 
Eval num_timesteps=320000, episode_reward=352.73 +/- 39.28
Episode length: 46.10 +/- 13.60
New best mean reward!
[I 2024-12-10 22:54:29,117] Trial 140 finished with value: 341.05080109999994 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0003651689442007765, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=352.68 +/- 39.29
Episode length: 46.20 +/- 13.54
New best mean reward!
Eval num_timesteps=320000, episode_reward=351.37 +/- 40.14
Episode length: 46.25 +/- 13.68
New best mean reward!
[I 2024-12-10 22:54:34,380] Trial 146 pruned. 
Eval num_timesteps=320000, episode_reward=351.10 +/- 41.63
Episode length: 49.60 +/- 20.16
New best mean reward!
[I 2024-12-10 22:54:53,748] Trial 147 pruned. 
Eval num_timesteps=320000, episode_reward=351.16 +/- 40.34
Episode length: 46.60 +/- 13.99
New best mean reward!
[I 2024-12-10 22:54:53,836] Trial 148 pruned. 
Eval num_timesteps=320000, episode_reward=353.74 +/- 38.19
Episode length: 45.50 +/- 13.04
New best mean reward!
[I 2024-12-10 22:55:28,536] Trial 144 finished with value: 341.21760175000003 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0005156893064176218, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=351.18 +/- 40.24
Episode length: 46.40 +/- 13.68
New best mean reward!
[I 2024-12-10 22:55:36,282] Trial 150 pruned. 
[I 2024-12-10 22:55:56,254] Trial 145 finished with value: 340.45329730000003 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.00240999552579214, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 22:56:16,063] Trial 149 finished with value: 341.0956028 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0010745450487216553, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=351.39 +/- 40.02
Episode length: 46.60 +/- 13.77
New best mean reward!
[I 2024-12-10 22:56:41,407] Trial 152 pruned. 
Eval num_timesteps=320000, episode_reward=351.58 +/- 39.94
Episode length: 46.45 +/- 13.75
New best mean reward!
[I 2024-12-10 22:56:48,947] Trial 153 pruned. 
Eval num_timesteps=320000, episode_reward=343.17 +/- 34.39
Episode length: 51.20 +/- 12.86
New best mean reward!
[I 2024-12-10 22:57:20,923] Trial 158 pruned. 
Eval num_timesteps=320000, episode_reward=-864.89 +/- 373.06
Episode length: 63.20 +/- 26.24
New best mean reward!
[I 2024-12-10 22:57:28,103] Trial 159 pruned. 
Eval num_timesteps=320000, episode_reward=353.83 +/- 38.28
Episode length: 45.80 +/- 13.24
New best mean reward!
Eval num_timesteps=320000, episode_reward=349.85 +/- 40.09
Episode length: 50.50 +/- 18.40
New best mean reward!
[I 2024-12-10 23:04:15,692] Trial 155 pruned. 
Eval num_timesteps=320000, episode_reward=345.20 +/- 40.28
Episode length: 54.65 +/- 19.03
New best mean reward!
[I 2024-12-10 23:04:21,985] Trial 154 pruned. 
Eval num_timesteps=320000, episode_reward=353.81 +/- 37.68
Episode length: 45.80 +/- 12.94
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.91 +/- 39.22
Episode length: 45.80 +/- 13.36
New best mean reward!
Eval num_timesteps=320000, episode_reward=260.10 +/- 89.51
Episode length: 84.60 +/- 35.61
New best mean reward!
[I 2024-12-10 23:05:45,607] Trial 161 pruned. 
Eval num_timesteps=320000, episode_reward=352.84 +/- 38.98
Episode length: 45.95 +/- 13.42
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.07 +/- 38.79
Episode length: 46.00 +/- 13.34
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.11 +/- 38.78
Episode length: 46.50 +/- 13.43
New best mean reward!
[I 2024-12-10 23:06:37,235] Trial 160 pruned. 
Eval num_timesteps=320000, episode_reward=304.48 +/- 80.70
Episode length: 72.70 +/- 45.49
New best mean reward!
[I 2024-12-10 23:06:38,676] Trial 157 pruned. 
Eval num_timesteps=320000, episode_reward=352.40 +/- 38.68
Episode length: 46.45 +/- 13.36
New best mean reward!
[I 2024-12-10 23:06:47,576] Trial 162 pruned. 
[I 2024-12-10 23:07:37,256] Trial 164 finished with value: 341.0449248 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.00039423489298460616, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 23:07:51,943] Trial 165 finished with value: 341.1124229 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.000776716439446456, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=353.57 +/- 38.67
Episode length: 45.65 +/- 13.27
New best mean reward!
Eval num_timesteps=320000, episode_reward=348.96 +/- 40.71
Episode length: 47.90 +/- 13.95
New best mean reward!
[I 2024-12-10 23:08:41,823] Trial 167 pruned. 
Eval num_timesteps=320000, episode_reward=348.18 +/- 43.05
Episode length: 47.35 +/- 14.64
New best mean reward!
[I 2024-12-10 23:08:58,046] Trial 168 pruned. 
Eval num_timesteps=320000, episode_reward=342.71 +/- 50.98
Episode length: 50.85 +/- 18.90
New best mean reward!
[I 2024-12-10 23:08:59,284] Trial 169 pruned. 
[I 2024-12-10 23:09:14,485] Trial 166 finished with value: 341.20281825 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0007599615322930167, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 23:09:33,579] Trial 151 finished with value: 341.04258115000005 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.0004234570265555528, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=349.96 +/- 40.60
Episode length: 46.95 +/- 13.79
New best mean reward!
[I 2024-12-10 23:09:45,214] Trial 170 pruned. 
Eval num_timesteps=320000, episode_reward=353.39 +/- 38.40
Episode length: 45.70 +/- 13.13
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.52 +/- 38.48
Episode length: 45.85 +/- 13.26
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.46 +/- 38.19
Episode length: 45.75 +/- 13.04
New best mean reward!
Eval num_timesteps=320000, episode_reward=341.87 +/- 53.41
Episode length: 48.95 +/- 17.14
New best mean reward!
[I 2024-12-10 23:11:13,758] Trial 173 pruned. 
[I 2024-12-10 23:11:24,361] Trial 171 finished with value: 341.1299309 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0007812374337316164, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=353.53 +/- 38.46
Episode length: 45.70 +/- 13.21
New best mean reward!
[I 2024-12-10 23:11:34,294] Trial 163 finished with value: 339.018291 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.0004186607919884295, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=353.37 +/- 38.52
Episode length: 45.65 +/- 13.02
New best mean reward!
[I 2024-12-10 23:11:40,716] Trial 156 finished with value: 341.19857680000007 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.0004983289442664634, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=351.69 +/- 39.44
Episode length: 46.90 +/- 13.72
New best mean reward!
[I 2024-12-10 23:11:50,112] Trial 177 pruned. 
[I 2024-12-10 23:12:09,704] Trial 172 finished with value: 340.8437276 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0008089501870157817, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=40000, episode_reward=-1115.17 +/- 407.91
Episode length: 1000.00 +/- 0.00
New best mean reward!
[I 2024-12-10 23:12:11,126] Trial 178 pruned. 
Eval num_timesteps=40000, episode_reward=-1831.42 +/- 935.55
Episode length: 862.70 +/- 326.86
New best mean reward!
[I 2024-12-10 23:12:19,399] Trial 179 pruned. 
[I 2024-12-10 23:12:35,512] Trial 174 finished with value: 340.79430179999997 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0008324530679169395, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 23:12:42,209] Trial 175 finished with value: 340.88478765 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0008633136810886551, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 23:12:57,353] Trial 176 finished with value: 340.44358099999994 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0008464726797200075, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=40000, episode_reward=-2333.07 +/- 1633.66
Episode length: 808.60 +/- 382.83
New best mean reward!
[I 2024-12-10 23:12:58,512] Trial 183 pruned. 
Eval num_timesteps=320000, episode_reward=352.27 +/- 39.15
Episode length: 46.25 +/- 13.42
New best mean reward!
[I 2024-12-10 23:13:29,604] Trial 180 pruned. 
Eval num_timesteps=320000, episode_reward=353.69 +/- 38.56
Episode length: 45.60 +/- 13.35
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.95 +/- 39.23
Episode length: 45.90 +/- 13.45
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.39 +/- 38.55
Episode length: 45.85 +/- 13.21
New best mean reward!
[I 2024-12-10 23:15:17,752] Trial 181 finished with value: 341.16850569999997 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0007996172727904822, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 23:15:24,140] Trial 182 finished with value: 341.151294 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0008620609874482743, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=352.96 +/- 39.04
Episode length: 46.10 +/- 13.41
New best mean reward!
[I 2024-12-10 23:15:59,222] Trial 184 finished with value: 341.1791222 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0005940593455129194, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=352.73 +/- 38.95
Episode length: 46.20 +/- 13.41
New best mean reward!
[I 2024-12-10 23:16:06,894] Trial 187 pruned. 
Eval num_timesteps=320000, episode_reward=351.73 +/- 39.98
Episode length: 46.05 +/- 13.66
New best mean reward!
[I 2024-12-10 23:16:07,163] Trial 186 pruned. 
Eval num_timesteps=320000, episode_reward=353.03 +/- 38.60
Episode length: 45.95 +/- 13.32
New best mean reward!
Eval num_timesteps=320000, episode_reward=351.93 +/- 39.56
Episode length: 46.20 +/- 13.49
New best mean reward!
[I 2024-12-10 23:16:27,507] Trial 189 pruned. 
Eval num_timesteps=320000, episode_reward=353.63 +/- 38.41
Episode length: 45.65 +/- 13.15
New best mean reward!
[I 2024-12-10 23:18:02,035] Trial 185 finished with value: 340.82633004999997 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.0006251028391911975, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=352.37 +/- 39.64
Episode length: 46.15 +/- 13.52
New best mean reward!
[I 2024-12-10 23:18:07,489] Trial 193 pruned. 
Eval num_timesteps=320000, episode_reward=353.53 +/- 38.54
Episode length: 45.65 +/- 13.17
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.52 +/- 39.45
Episode length: 46.20 +/- 13.71
New best mean reward!
[I 2024-12-10 23:18:13,322] Trial 195 pruned. 
[I 2024-12-10 23:18:22,482] Trial 190 finished with value: 340.80311205000004 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.0006287836107331801, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=352.84 +/- 39.84
Episode length: 47.70 +/- 18.42
New best mean reward!
[I 2024-12-10 23:18:32,126] Trial 196 pruned. 
Eval num_timesteps=320000, episode_reward=352.89 +/- 38.79
Episode length: 46.05 +/- 13.14
New best mean reward!
[I 2024-12-10 23:18:41,335] Trial 188 finished with value: 341.17728645 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.0006094091847601207, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=352.69 +/- 38.88
Episode length: 45.95 +/- 13.35
New best mean reward!
[I 2024-12-10 23:18:43,309] Trial 192 pruned. 
Eval num_timesteps=160000, episode_reward=321.07 +/- 33.37
Episode length: 57.45 +/- 11.96
New best mean reward!
[I 2024-12-10 23:19:18,360] Trial 197 pruned. 
Eval num_timesteps=160000, episode_reward=310.11 +/- 38.60
Episode length: 63.35 +/- 15.62
New best mean reward!
[I 2024-12-10 23:19:27,358] Trial 198 pruned. 
[I 2024-12-10 23:19:29,306] Trial 194 finished with value: 341.0986289 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.000582719421923167, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=160000, episode_reward=319.59 +/- 32.32
Episode length: 58.10 +/- 11.73
New best mean reward!
[I 2024-12-10 23:19:31,109] Trial 199 pruned. 
Eval num_timesteps=160000, episode_reward=314.44 +/- 36.66
Episode length: 61.95 +/- 14.87
New best mean reward!
[I 2024-12-10 23:19:45,368] Trial 201 pruned. 
Eval num_timesteps=160000, episode_reward=308.36 +/- 39.08
Episode length: 61.05 +/- 13.32
New best mean reward!
[I 2024-12-10 23:19:55,959] Trial 203 pruned. 
Eval num_timesteps=320000, episode_reward=353.65 +/- 38.61
Episode length: 45.40 +/- 13.15
New best mean reward!
[I 2024-12-10 23:20:43,773] Trial 191 finished with value: 340.72668595 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.0005479106839058102, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=353.63 +/- 38.08
Episode length: 45.90 +/- 13.07
New best mean reward!
[I 2024-12-10 23:21:18,438] Trial 200 finished with value: 340.93309070000004 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.001156924492740214, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=353.58 +/- 38.53
Episode length: 45.50 +/- 13.22
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.74 +/- 38.48
Episode length: 45.50 +/- 13.20
New best mean reward!
Eval num_timesteps=320000, episode_reward=338.61 +/- 46.24
Episode length: 54.15 +/- 18.57
New best mean reward!
[I 2024-12-10 23:21:38,108] Trial 207 pruned. 
[I 2024-12-10 23:22:10,169] Trial 202 finished with value: 340.93508945 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0010326829976272977, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 23:22:43,248] Trial 204 finished with value: 341.03157465 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0009962187903481557, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 23:22:55,336] Trial 206 finished with value: 341.1124618 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0003026672209844883, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=350.59 +/- 37.34
Episode length: 48.25 +/- 13.42
New best mean reward!
[I 2024-12-10 23:24:43,546] Trial 214 pruned. 
Eval num_timesteps=320000, episode_reward=342.12 +/- 42.31
Episode length: 53.25 +/- 18.15
New best mean reward!
[I 2024-12-10 23:24:47,700] Trial 208 pruned. 
Eval num_timesteps=320000, episode_reward=349.48 +/- 38.61
Episode length: 49.40 +/- 15.30
New best mean reward!
[I 2024-12-10 23:24:58,659] Trial 215 pruned. 
Eval num_timesteps=320000, episode_reward=345.59 +/- 43.30
Episode length: 51.95 +/- 17.92
New best mean reward!
[I 2024-12-10 23:25:02,164] Trial 209 pruned. 
Eval num_timesteps=320000, episode_reward=347.64 +/- 42.28
Episode length: 49.05 +/- 15.09
New best mean reward!
[I 2024-12-10 23:25:38,021] Trial 205 pruned. 
Eval num_timesteps=320000, episode_reward=282.27 +/- 34.80
Episode length: 96.70 +/- 22.71
New best mean reward!
[I 2024-12-10 23:25:59,242] Trial 216 pruned. 
Eval num_timesteps=320000, episode_reward=346.78 +/- 37.20
Episode length: 51.30 +/- 17.52
New best mean reward!
[I 2024-12-10 23:26:05,295] Trial 217 pruned. 
Eval num_timesteps=320000, episode_reward=340.83 +/- 44.86
Episode length: 51.00 +/- 15.74
New best mean reward!
[I 2024-12-10 23:26:05,501] Trial 211 pruned. 
Eval num_timesteps=320000, episode_reward=346.74 +/- 41.75
Episode length: 54.00 +/- 24.31
New best mean reward!
[I 2024-12-10 23:26:11,648] Trial 218 pruned. 
Eval num_timesteps=320000, episode_reward=351.83 +/- 39.58
Episode length: 46.45 +/- 13.71
New best mean reward!
[I 2024-12-10 23:26:14,120] Trial 210 pruned. 
Eval num_timesteps=320000, episode_reward=342.55 +/- 38.40
Episode length: 55.15 +/- 20.84
New best mean reward!
[I 2024-12-10 23:26:15,799] Trial 219 pruned. 
Eval num_timesteps=320000, episode_reward=336.89 +/- 59.81
Episode length: 54.30 +/- 26.50
New best mean reward!
[I 2024-12-10 23:27:07,091] Trial 212 pruned. 
Eval num_timesteps=320000, episode_reward=339.08 +/- 43.92
Episode length: 54.45 +/- 17.30
New best mean reward!
[I 2024-12-10 23:27:25,096] Trial 213 pruned. 
Eval num_timesteps=320000, episode_reward=352.80 +/- 38.98
Episode length: 45.90 +/- 13.35
New best mean reward!
[I 2024-12-10 23:27:54,208] Trial 220 pruned. 
Eval num_timesteps=320000, episode_reward=352.39 +/- 39.55
Episode length: 46.20 +/- 13.67
New best mean reward!
[I 2024-12-10 23:28:01,313] Trial 223 pruned. 
Eval num_timesteps=320000, episode_reward=353.10 +/- 39.08
Episode length: 45.75 +/- 13.30
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.73 +/- 39.18
Episode length: 46.00 +/- 13.47
New best mean reward!
[I 2024-12-10 23:28:13,164] Trial 222 pruned. 
Eval num_timesteps=320000, episode_reward=352.67 +/- 39.44
Episode length: 46.05 +/- 13.57
New best mean reward!
[I 2024-12-10 23:28:19,201] Trial 224 pruned. 
Eval num_timesteps=320000, episode_reward=351.07 +/- 40.95
Episode length: 47.10 +/- 14.79
New best mean reward!
[I 2024-12-10 23:28:24,722] Trial 225 pruned. 
Eval num_timesteps=320000, episode_reward=353.64 +/- 38.46
Episode length: 45.60 +/- 13.28
New best mean reward!
[I 2024-12-10 23:29:15,108] Trial 221 finished with value: 341.21176645 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0004515782186576635, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=351.30 +/- 40.78
Episode length: 46.50 +/- 14.02
New best mean reward!
[I 2024-12-10 23:29:18,190] Trial 227 pruned. 
Eval num_timesteps=320000, episode_reward=352.03 +/- 40.41
Episode length: 46.10 +/- 13.87
New best mean reward!
[I 2024-12-10 23:29:22,939] Trial 228 pruned. 
[I 2024-12-10 23:29:42,077] Trial 226 finished with value: 341.0285637 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0004808061400006265, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=350.82 +/- 39.86
Episode length: 47.00 +/- 13.78
New best mean reward!
[I 2024-12-10 23:30:01,953] Trial 229 pruned. 
Eval num_timesteps=320000, episode_reward=352.98 +/- 39.43
Episode length: 46.05 +/- 13.78
New best mean reward!
Eval num_timesteps=320000, episode_reward=350.94 +/- 40.13
Episode length: 46.70 +/- 13.76
New best mean reward!
[I 2024-12-10 23:31:20,496] Trial 234 pruned. 
Eval num_timesteps=320000, episode_reward=352.36 +/- 39.37
Episode length: 46.15 +/- 13.36
New best mean reward!
[I 2024-12-10 23:31:26,067] Trial 235 pruned. 
Eval num_timesteps=320000, episode_reward=351.46 +/- 39.83
Episode length: 46.35 +/- 13.54
New best mean reward!
[I 2024-12-10 23:31:26,219] Trial 236 pruned. 
Eval num_timesteps=320000, episode_reward=352.55 +/- 39.33
Episode length: 46.05 +/- 13.46
New best mean reward!
[I 2024-12-10 23:31:33,590] Trial 232 pruned. 
Eval num_timesteps=320000, episode_reward=353.12 +/- 38.65
Episode length: 45.75 +/- 13.09
New best mean reward!
Eval num_timesteps=320000, episode_reward=341.54 +/- 41.39
Episode length: 55.50 +/- 19.74
New best mean reward!
[I 2024-12-10 23:31:39,551] Trial 238 pruned. 
Eval num_timesteps=320000, episode_reward=353.65 +/- 38.35
Episode length: 45.65 +/- 13.20
New best mean reward!
Eval num_timesteps=320000, episode_reward=343.47 +/- 46.10
Episode length: 51.50 +/- 17.93
New best mean reward!
[I 2024-12-10 23:31:47,149] Trial 237 pruned. 
[I 2024-12-10 23:32:38,744] Trial 230 finished with value: 340.9901618 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.00023622931728608474, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=351.00 +/- 39.75
Episode length: 46.65 +/- 13.57
New best mean reward!
[I 2024-12-10 23:32:48,842] Trial 239 pruned. 
[I 2024-12-10 23:33:46,708] Trial 233 finished with value: 341.19030835 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.000216910435650073, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 23:33:58,715] Trial 231 finished with value: 340.88835465 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.0005871729163570694, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=352.46 +/- 38.91
Episode length: 45.95 +/- 13.17
New best mean reward!
[I 2024-12-10 23:34:35,878] Trial 241 pruned. 
Eval num_timesteps=320000, episode_reward=350.86 +/- 40.73
Episode length: 46.50 +/- 13.96
New best mean reward!
[I 2024-12-10 23:34:45,089] Trial 240 pruned. 
Eval num_timesteps=320000, episode_reward=352.93 +/- 39.19
Episode length: 46.20 +/- 13.71
New best mean reward!
[I 2024-12-10 23:34:57,718] Trial 242 pruned. 
Eval num_timesteps=320000, episode_reward=353.78 +/- 38.71
Episode length: 45.60 +/- 13.44
New best mean reward!
Eval num_timesteps=320000, episode_reward=350.87 +/- 41.10
Episode length: 46.65 +/- 14.10
New best mean reward!
[I 2024-12-10 23:35:32,723] Trial 243 pruned. 
Eval num_timesteps=320000, episode_reward=224.29 +/- 368.23
Episode length: 53.85 +/- 24.55
New best mean reward!
[I 2024-12-10 23:35:41,183] Trial 245 pruned. 
Eval num_timesteps=320000, episode_reward=348.84 +/- 40.38
Episode length: 49.80 +/- 16.40
New best mean reward!
[I 2024-12-10 23:36:07,348] Trial 248 pruned. 
Eval num_timesteps=320000, episode_reward=350.36 +/- 41.59
Episode length: 48.65 +/- 17.09
New best mean reward!
[I 2024-12-10 23:36:10,075] Trial 246 pruned. 
Eval num_timesteps=320000, episode_reward=348.44 +/- 41.41
Episode length: 51.15 +/- 18.78
New best mean reward!
[I 2024-12-10 23:36:38,923] Trial 249 pruned. 
Eval num_timesteps=320000, episode_reward=350.75 +/- 40.83
Episode length: 47.35 +/- 14.42
New best mean reward!
[I 2024-12-10 23:37:12,117] Trial 247 pruned. 
[I 2024-12-10 23:37:20,940] Trial 244 finished with value: 340.33929930000005 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.0003654784397120222, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=351.92 +/- 39.06
Episode length: 46.75 +/- 13.56
New best mean reward!
[I 2024-12-10 23:37:46,225] Trial 252 pruned. 
Eval num_timesteps=320000, episode_reward=353.34 +/- 38.76
Episode length: 45.80 +/- 13.30
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.58 +/- 39.59
Episode length: 47.25 +/- 15.96
New best mean reward!
[I 2024-12-10 23:38:14,414] Trial 255 pruned. 
Eval num_timesteps=320000, episode_reward=353.19 +/- 39.00
Episode length: 45.80 +/- 13.39
New best mean reward!
Eval num_timesteps=320000, episode_reward=349.21 +/- 40.78
Episode length: 48.60 +/- 14.75
New best mean reward!
[I 2024-12-10 23:38:19,415] Trial 251 pruned. 
[I 2024-12-10 23:39:30,476] Trial 254 finished with value: 341.11536479999995 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0008509118489830808, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 23:39:56,448] Trial 250 finished with value: 341.10479260000005 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.00015777709730176317, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=236.14 +/- 177.96
Episode length: 96.50 +/- 50.34
New best mean reward!
[I 2024-12-10 23:40:23,011] Trial 261 pruned. 
Eval num_timesteps=320000, episode_reward=267.09 +/- 122.77
Episode length: 90.90 +/- 55.26
New best mean reward!
[I 2024-12-10 23:41:30,542] Trial 262 pruned. 
Eval num_timesteps=320000, episode_reward=-5302.69 +/- 4330.09
Episode length: 812.35 +/- 375.34
New best mean reward!
[I 2024-12-10 23:42:58,770] Trial 263 pruned. 
Eval num_timesteps=320000, episode_reward=349.20 +/- 41.94
Episode length: 48.55 +/- 15.67
New best mean reward!
[I 2024-12-10 23:44:11,488] Trial 253 pruned. 
Eval num_timesteps=320000, episode_reward=330.42 +/- 62.20
Episode length: 59.15 +/- 31.86
New best mean reward!
[I 2024-12-10 23:45:59,687] Trial 267 pruned. 
Eval num_timesteps=320000, episode_reward=295.40 +/- 66.58
Episode length: 81.20 +/- 27.81
New best mean reward!
[I 2024-12-10 23:46:20,115] Trial 256 pruned. 
Eval num_timesteps=320000, episode_reward=12.08 +/- 355.59
Episode length: 140.50 +/- 64.79
New best mean reward!
[I 2024-12-10 23:47:39,806] Trial 268 pruned. 
Eval num_timesteps=320000, episode_reward=-1264.14 +/- 2408.86
Episode length: 240.50 +/- 268.38
New best mean reward!
[I 2024-12-10 23:47:54,072] Trial 260 pruned. 
Eval num_timesteps=320000, episode_reward=352.02 +/- 39.68
Episode length: 46.40 +/- 13.59
New best mean reward!
[I 2024-12-10 23:48:10,951] Trial 269 pruned. 
Eval num_timesteps=320000, episode_reward=286.56 +/- 139.61
Episode length: 70.10 +/- 30.13
New best mean reward!
[I 2024-12-10 23:48:32,083] Trial 259 pruned. 
Eval num_timesteps=40000, episode_reward=-2013.54 +/- 678.21
Episode length: 947.85 +/- 157.17
New best mean reward!
[I 2024-12-10 23:49:02,471] Trial 272 pruned. 
Eval num_timesteps=320000, episode_reward=345.07 +/- 44.48
Episode length: 51.40 +/- 19.26
New best mean reward!
[I 2024-12-10 23:49:18,762] Trial 270 pruned. 
Eval num_timesteps=320000, episode_reward=351.08 +/- 39.80
Episode length: 46.90 +/- 13.97
New best mean reward!
[I 2024-12-10 23:49:50,448] Trial 271 pruned. 
Eval num_timesteps=320000, episode_reward=353.72 +/- 38.36
Episode length: 45.65 +/- 13.26
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.01 +/- 39.68
Episode length: 45.85 +/- 13.64
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.74 +/- 38.33
Episode length: 45.65 +/- 13.24
New best mean reward!
Eval num_timesteps=320000, episode_reward=-1717.66 +/- 643.78
Episode length: 207.55 +/- 71.63
New best mean reward!
[I 2024-12-10 23:51:56,122] Trial 276 pruned. 
[I 2024-12-10 23:52:06,337] Trial 273 finished with value: 341.03749125 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0006618668142168213, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 23:52:15,936] Trial 275 finished with value: 341.10426605000004 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0006717967841691566, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 23:52:22,268] Trial 274 finished with value: 340.93859340000006 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0006623253582686704, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=-425.12 +/- 728.56
Episode length: 120.05 +/- 82.53
New best mean reward!
[I 2024-12-10 23:52:35,988] Trial 266 pruned. 
Eval num_timesteps=320000, episode_reward=353.54 +/- 38.46
Episode length: 45.75 +/- 13.18
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.17 +/- 38.48
Episode length: 46.05 +/- 13.22
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.96 +/- 38.33
Episode length: 45.30 +/- 13.06
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.88 +/- 39.23
Episode length: 45.85 +/- 13.43
New best mean reward!
[I 2024-12-10 23:54:42,552] Trial 281 pruned. 
[I 2024-12-10 23:55:12,425] Trial 279 finished with value: 341.03396834999995 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0005267321013917735, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
[I 2024-12-10 23:55:15,796] Trial 277 finished with value: 341.13433210000005 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0006794288947752232, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 101 with value: 341.2645613.
Eval num_timesteps=320000, episode_reward=352.20 +/- 39.31
Episode length: 46.70 +/- 13.82
New best mean reward!
[I 2024-12-10 23:55:21,967] Trial 280 pruned. 
[I 2024-12-10 23:55:41,637] Trial 278 finished with value: 341.28885329999997 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0005490898077245991, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=340.43 +/- 44.97
Episode length: 50.25 +/- 15.24
New best mean reward!
[I 2024-12-10 23:56:16,036] Trial 265 pruned. 
Eval num_timesteps=320000, episode_reward=32.53 +/- 416.10
Episode length: 87.50 +/- 68.02
New best mean reward!
[I 2024-12-10 23:56:38,191] Trial 257 pruned. 
Eval num_timesteps=320000, episode_reward=-1801.64 +/- 1388.78
Episode length: 858.95 +/- 335.83
New best mean reward!
[I 2024-12-10 23:57:11,206] Trial 258 pruned. 
Eval num_timesteps=320000, episode_reward=353.22 +/- 38.23
Episode length: 45.95 +/- 13.07
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.05 +/- 39.74
Episode length: 46.15 +/- 13.44
New best mean reward!
[I 2024-12-10 23:57:46,552] Trial 283 pruned. 
Eval num_timesteps=320000, episode_reward=347.54 +/- 43.10
Episode length: 49.10 +/- 16.27
New best mean reward!
[I 2024-12-10 23:58:37,339] Trial 285 pruned. 
Eval num_timesteps=320000, episode_reward=351.69 +/- 40.22
Episode length: 46.20 +/- 13.48
New best mean reward!
[I 2024-12-10 23:59:19,427] Trial 286 pruned. 
[I 2024-12-10 23:59:31,790] Trial 282 finished with value: 341.12757294999994 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.00041952887792266237, 'clip_range': 0.3, 'n_epochs': 20, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=349.42 +/- 41.55
Episode length: 47.40 +/- 14.37
New best mean reward!
[I 2024-12-10 23:59:33,022] Trial 287 pruned. 
Eval num_timesteps=320000, episode_reward=-1948.71 +/- 1282.50
Episode length: 213.15 +/- 106.64
New best mean reward!
[I 2024-12-10 23:59:33,162] Trial 264 pruned. 
Eval num_timesteps=320000, episode_reward=352.87 +/- 38.79
Episode length: 46.20 +/- 13.27
New best mean reward!
[I 2024-12-11 00:00:06,048] Trial 288 pruned. 
Eval num_timesteps=40000, episode_reward=208.66 +/- 90.57
Episode length: 131.75 +/- 48.01
New best mean reward!
Eval num_timesteps=40000, episode_reward=-2217.00 +/- 975.95
Episode length: 912.00 +/- 264.03
New best mean reward!
[I 2024-12-11 00:00:25,137] Trial 294 pruned. 
Eval num_timesteps=40000, episode_reward=-1754.72 +/- 496.98
Episode length: 918.45 +/- 170.36
New best mean reward!
[I 2024-12-11 00:00:25,466] Trial 295 pruned. 
Eval num_timesteps=320000, episode_reward=352.99 +/- 38.55
Episode length: 46.40 +/- 13.52
New best mean reward!
[I 2024-12-11 00:00:26,764] Trial 289 pruned. 
Eval num_timesteps=320000, episode_reward=352.24 +/- 39.62
Episode length: 46.00 +/- 13.38
New best mean reward!
Eval num_timesteps=320000, episode_reward=351.86 +/- 39.74
Episode length: 46.20 +/- 13.47
New best mean reward!
[I 2024-12-11 00:00:44,517] Trial 284 pruned. 
[I 2024-12-11 00:00:44,527] Trial 290 pruned. 
Eval num_timesteps=320000, episode_reward=353.70 +/- 38.25
Episode length: 45.60 +/- 13.12
New best mean reward!
Eval num_timesteps=80000, episode_reward=347.08 +/- 66.13
Episode length: 50.30 +/- 25.04
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.40 +/- 38.36
Episode length: 45.95 +/- 13.22
New best mean reward!
Eval num_timesteps=120000, episode_reward=328.07 +/- 45.74
Episode length: 54.45 +/- 15.81
Eval num_timesteps=320000, episode_reward=352.52 +/- 39.03
Episode length: 46.10 +/- 13.38
New best mean reward!
[I 2024-12-11 00:02:23,394] Trial 296 pruned. 
Eval num_timesteps=320000, episode_reward=352.89 +/- 38.93
Episode length: 46.05 +/- 13.43
New best mean reward!
[I 2024-12-11 00:02:39,195] Trial 297 pruned. 
[I 2024-12-11 00:02:47,304] Trial 291 finished with value: 341.0763627 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.99, 'learning_rate': 0.0004313323382180798, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=160000, episode_reward=335.19 +/- 51.04
Episode length: 54.65 +/- 20.92
Eval num_timesteps=320000, episode_reward=353.65 +/- 38.66
Episode length: 45.60 +/- 13.25
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.07 +/- 38.87
Episode length: 45.70 +/- 13.22
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.19 +/- 38.76
Episode length: 46.05 +/- 13.39
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.93 +/- 37.99
Episode length: 45.45 +/- 13.03
New best mean reward!
[I 2024-12-11 00:03:14,009] Trial 292 finished with value: 341.26859300000007 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.00028527449615227494, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=200000, episode_reward=359.63 +/- 52.12
Episode length: 43.80 +/- 17.81
New best mean reward!
[I 2024-12-11 00:04:26,571] Trial 300 finished with value: 331.9644828 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.000602273388752279, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=353.45 +/- 38.53
Episode length: 45.60 +/- 13.09
New best mean reward!
Eval num_timesteps=240000, episode_reward=346.42 +/- 35.09
Episode length: 47.85 +/- 12.03
[I 2024-12-11 00:04:41,413] Trial 301 finished with value: 340.90512715000006 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.99, 'learning_rate': 0.000637210070145578, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 00:04:45,925] Trial 298 finished with value: 341.1409442 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.99, 'learning_rate': 0.00048791155024360773, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=353.45 +/- 38.42
Episode length: 45.60 +/- 13.21
New best mean reward!
[I 2024-12-11 00:04:47,404] Trial 299 finished with value: 341.05231405 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.99, 'learning_rate': 0.0006078405764544077, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=353.66 +/- 38.04
Episode length: 45.65 +/- 13.02
New best mean reward!
Eval num_timesteps=280000, episode_reward=329.85 +/- 30.95
Episode length: 53.50 +/- 10.60
Eval num_timesteps=320000, episode_reward=350.56 +/- 40.97
Episode length: 48.45 +/- 17.71
New best mean reward!
[I 2024-12-11 00:05:48,257] Trial 305 pruned. 
[I 2024-12-11 00:05:58,304] Trial 302 finished with value: 341.00105090000005 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0005563554217623028, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 00:06:10,231] Trial 303 finished with value: 340.79505155 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0005946582604342481, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=347.32 +/- 38.27
Episode length: 47.50 +/- 13.06
[I 2024-12-11 00:06:23,096] Trial 304 finished with value: 340.95620184999996 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.000560059047288986, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=351.77 +/- 39.72
Episode length: 46.30 +/- 13.63
New best mean reward!
[I 2024-12-11 00:06:37,718] Trial 306 pruned. 
Eval num_timesteps=320000, episode_reward=354.16 +/- 37.92
Episode length: 45.45 +/- 12.95
New best mean reward!
Eval num_timesteps=360000, episode_reward=327.33 +/- 34.09
Episode length: 54.30 +/- 11.66
Eval num_timesteps=320000, episode_reward=352.97 +/- 38.43
Episode length: 46.00 +/- 13.11
New best mean reward!
[I 2024-12-11 00:07:14,901] Trial 308 pruned. 
Eval num_timesteps=320000, episode_reward=352.33 +/- 39.55
Episode length: 46.15 +/- 13.72
New best mean reward!
[I 2024-12-11 00:07:15,184] Trial 309 pruned. 
Eval num_timesteps=320000, episode_reward=346.64 +/- 43.25
Episode length: 53.80 +/- 24.63
New best mean reward!
[I 2024-12-11 00:07:15,623] Trial 311 pruned. 
Eval num_timesteps=320000, episode_reward=350.10 +/- 41.05
Episode length: 49.95 +/- 19.52
New best mean reward!
[I 2024-12-11 00:07:35,610] Trial 310 pruned. 
Eval num_timesteps=320000, episode_reward=351.18 +/- 40.22
Episode length: 48.30 +/- 17.47
New best mean reward!
[I 2024-12-11 00:07:42,678] Trial 312 pruned. 
Eval num_timesteps=400000, episode_reward=331.99 +/- 16.76
Episode length: 52.75 +/- 5.80
[I 2024-12-11 00:07:54,981] Trial 307 finished with value: 341.03760555 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0005264985284791263, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=352.53 +/- 38.13
Episode length: 46.50 +/- 13.08
New best mean reward!
[I 2024-12-11 00:07:59,510] Trial 313 pruned. 
Eval num_timesteps=320000, episode_reward=351.60 +/- 38.26
Episode length: 48.30 +/- 16.11
New best mean reward!
[I 2024-12-11 00:08:13,793] Trial 314 pruned. 
Eval num_timesteps=160000, episode_reward=320.92 +/- 32.76
Episode length: 57.55 +/- 11.52
New best mean reward!
Eval num_timesteps=160000, episode_reward=318.29 +/- 35.75
Episode length: 64.40 +/- 25.10
New best mean reward!
[I 2024-12-11 00:08:29,094] Trial 316 pruned. 
[I 2024-12-11 00:08:29,113] Trial 315 pruned. 
Eval num_timesteps=440000, episode_reward=340.57 +/- 41.90
Episode length: 49.90 +/- 14.39
Eval num_timesteps=160000, episode_reward=293.81 +/- 58.17
Episode length: 74.80 +/- 25.03
New best mean reward!
[I 2024-12-11 00:08:56,254] Trial 317 pruned. 
Eval num_timesteps=480000, episode_reward=333.77 +/- 29.22
Episode length: 52.25 +/- 10.23
Eval num_timesteps=160000, episode_reward=313.42 +/- 34.34
Episode length: 61.20 +/- 12.68
New best mean reward!
[I 2024-12-11 00:09:44,655] Trial 321 pruned. 
[I 2024-12-11 00:09:52,932] Trial 293 finished with value: 323.84262025000004 and parameters: {'n_envs': 4, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0003064901308297787, 'clip_range': 0.3, 'n_epochs': 20, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=344.04 +/- 55.62
Episode length: 48.55 +/- 18.16
New best mean reward!
[I 2024-12-11 00:10:20,712] Trial 322 pruned. 
Eval num_timesteps=320000, episode_reward=352.34 +/- 39.68
Episode length: 46.15 +/- 13.63
New best mean reward!
[I 2024-12-11 00:10:31,725] Trial 324 pruned. 
Eval num_timesteps=320000, episode_reward=342.41 +/- 52.66
Episode length: 49.60 +/- 18.03
New best mean reward!
[I 2024-12-11 00:10:32,519] Trial 323 pruned. 
Eval num_timesteps=320000, episode_reward=349.19 +/- 41.98
Episode length: 47.35 +/- 13.73
New best mean reward!
[I 2024-12-11 00:10:38,734] Trial 320 pruned. 
Eval num_timesteps=320000, episode_reward=348.64 +/- 42.71
Episode length: 48.05 +/- 14.94
New best mean reward!
[I 2024-12-11 00:10:49,822] Trial 319 pruned. 
Eval num_timesteps=320000, episode_reward=350.26 +/- 43.26
Episode length: 47.10 +/- 14.80
New best mean reward!
[I 2024-12-11 00:10:52,829] Trial 318 pruned. 
Eval num_timesteps=320000, episode_reward=348.96 +/- 41.91
Episode length: 47.45 +/- 14.53
New best mean reward!
[I 2024-12-11 00:11:05,441] Trial 325 pruned. 
Eval num_timesteps=320000, episode_reward=348.92 +/- 45.32
Episode length: 47.05 +/- 15.30
New best mean reward!
[I 2024-12-11 00:11:51,605] Trial 326 pruned. 
Eval num_timesteps=320000, episode_reward=343.82 +/- 54.61
Episode length: 48.95 +/- 17.74
New best mean reward!
[I 2024-12-11 00:11:57,424] Trial 327 pruned. 
Eval num_timesteps=320000, episode_reward=353.96 +/- 38.38
Episode length: 45.45 +/- 13.21
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.39 +/- 38.63
Episode length: 45.85 +/- 13.15
New best mean reward!
Eval num_timesteps=320000, episode_reward=350.67 +/- 40.28
Episode length: 46.80 +/- 13.74
New best mean reward!
[I 2024-12-11 00:12:34,135] Trial 330 pruned. 
Eval num_timesteps=320000, episode_reward=353.21 +/- 38.65
Episode length: 45.80 +/- 13.26
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.46 +/- 38.55
Episode length: 45.85 +/- 13.23
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.03 +/- 38.94
Episode length: 46.00 +/- 13.40
New best mean reward!
[I 2024-12-11 00:13:05,069] Trial 333 pruned. 
Eval num_timesteps=320000, episode_reward=318.03 +/- 59.96
Episode length: 67.60 +/- 28.41
New best mean reward!
[I 2024-12-11 00:13:08,501] Trial 334 pruned. 
[I 2024-12-11 00:13:43,176] Trial 331 finished with value: 341.12059075 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.000689865620054737, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 00:13:48,138] Trial 328 finished with value: 341.23095515 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0006843787786194925, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 00:13:56,841] Trial 329 finished with value: 341.2259386 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0007230655699164307, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=350.62 +/- 39.71
Episode length: 47.45 +/- 13.98
New best mean reward!
[I 2024-12-11 00:14:00,159] Trial 335 pruned. 
Eval num_timesteps=320000, episode_reward=325.82 +/- 53.54
Episode length: 61.20 +/- 20.38
New best mean reward!
[I 2024-12-11 00:14:06,080] Trial 336 pruned. 
[I 2024-12-11 00:14:15,906] Trial 332 finished with value: 341.02784510000004 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0007229058295424956, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=353.52 +/- 38.01
Episode length: 46.10 +/- 13.13
New best mean reward!
Eval num_timesteps=320000, episode_reward=342.37 +/- 43.94
Episode length: 51.60 +/- 15.99
New best mean reward!
[I 2024-12-11 00:14:57,946] Trial 340 pruned. 
Eval num_timesteps=320000, episode_reward=351.96 +/- 39.17
Episode length: 46.35 +/- 13.36
New best mean reward!
[I 2024-12-11 00:15:10,371] Trial 339 pruned. 
Eval num_timesteps=320000, episode_reward=334.84 +/- 47.88
Episode length: 57.25 +/- 20.38
New best mean reward!
[I 2024-12-11 00:15:11,918] Trial 341 pruned. 
Eval num_timesteps=320000, episode_reward=342.57 +/- 44.35
Episode length: 48.95 +/- 14.86
New best mean reward!
[I 2024-12-11 00:15:16,572] Trial 342 pruned. 
Eval num_timesteps=320000, episode_reward=345.55 +/- 43.39
Episode length: 48.35 +/- 14.76
New best mean reward!
[I 2024-12-11 00:15:21,381] Trial 343 pruned. 
Eval num_timesteps=320000, episode_reward=349.04 +/- 41.03
Episode length: 47.05 +/- 14.05
New best mean reward!
[I 2024-12-11 00:15:25,602] Trial 338 pruned. 
Eval num_timesteps=320000, episode_reward=344.27 +/- 41.54
Episode length: 50.15 +/- 15.01
New best mean reward!
[I 2024-12-11 00:15:36,516] Trial 345 pruned. 
[I 2024-12-11 00:15:56,428] Trial 337 finished with value: 341.02775005 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.00037438897231422465, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=353.44 +/- 38.75
Episode length: 45.75 +/- 13.46
New best mean reward!
Eval num_timesteps=320000, episode_reward=341.93 +/- 37.63
Episode length: 55.80 +/- 17.32
New best mean reward!
[I 2024-12-11 00:16:17,140] Trial 346 pruned. 
Eval num_timesteps=320000, episode_reward=350.85 +/- 39.72
Episode length: 47.15 +/- 13.88
New best mean reward!
[I 2024-12-11 00:16:44,927] Trial 347 pruned. 
Eval num_timesteps=320000, episode_reward=-910.26 +/- 270.84
Episode length: 56.50 +/- 23.43
New best mean reward!
[I 2024-12-11 00:16:46,944] Trial 348 pruned. 
Eval num_timesteps=320000, episode_reward=346.83 +/- 39.51
Episode length: 51.15 +/- 16.21
New best mean reward!
[I 2024-12-11 00:17:25,420] Trial 350 pruned. 
[I 2024-12-11 00:17:25,897] Trial 344 finished with value: 341.1506739 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0007244202564367855, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=352.96 +/- 38.21
Episode length: 46.25 +/- 13.13
New best mean reward!
[I 2024-12-11 00:17:39,238] Trial 352 pruned. 
Eval num_timesteps=320000, episode_reward=347.46 +/- 37.64
Episode length: 49.80 +/- 13.87
New best mean reward!
[I 2024-12-11 00:17:42,470] Trial 351 pruned. 
Eval num_timesteps=320000, episode_reward=351.37 +/- 38.65
Episode length: 48.30 +/- 15.36
New best mean reward!
[I 2024-12-11 00:17:58,241] Trial 353 pruned. 
Eval num_timesteps=320000, episode_reward=350.14 +/- 38.81
Episode length: 48.35 +/- 14.01
New best mean reward!
[I 2024-12-11 00:18:34,175] Trial 349 pruned. 
Eval num_timesteps=320000, episode_reward=352.70 +/- 38.62
Episode length: 46.25 +/- 13.53
New best mean reward!
[I 2024-12-11 00:19:16,204] Trial 354 pruned. 
Eval num_timesteps=40000, episode_reward=-2152.53 +/- 2853.10
Episode length: 313.60 +/- 284.84
New best mean reward!
[I 2024-12-11 00:19:17,097] Trial 362 pruned. 
Eval num_timesteps=320000, episode_reward=-545.29 +/- 1037.80
Episode length: 150.10 +/- 83.43
New best mean reward!
[I 2024-12-11 00:19:55,000] Trial 360 pruned. 
Eval num_timesteps=40000, episode_reward=-3818.20 +/- 3621.80
Episode length: 475.75 +/- 325.84
New best mean reward!
[I 2024-12-11 00:19:55,232] Trial 363 pruned. 
Eval num_timesteps=320000, episode_reward=349.68 +/- 40.40
Episode length: 47.80 +/- 13.94
New best mean reward!
[I 2024-12-11 00:20:04,545] Trial 361 pruned. 
Eval num_timesteps=320000, episode_reward=353.64 +/- 38.12
Episode length: 45.70 +/- 13.03
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.14 +/- 37.94
Episode length: 46.00 +/- 13.10
New best mean reward!
[I 2024-12-11 00:20:08,024] Trial 356 pruned. 
Eval num_timesteps=320000, episode_reward=306.62 +/- 123.10
Episode length: 60.45 +/- 33.32
New best mean reward!
[I 2024-12-11 00:20:50,059] Trial 357 pruned. 
Eval num_timesteps=320000, episode_reward=354.15 +/- 38.29
Episode length: 45.45 +/- 13.18
New best mean reward!
Eval num_timesteps=320000, episode_reward=351.84 +/- 39.86
Episode length: 46.45 +/- 13.95
New best mean reward!
[I 2024-12-11 00:20:54,626] Trial 359 pruned. 
Eval num_timesteps=320000, episode_reward=351.55 +/- 40.15
Episode length: 46.65 +/- 14.09
New best mean reward!
[I 2024-12-11 00:21:22,836] Trial 364 pruned. 
Eval num_timesteps=320000, episode_reward=351.81 +/- 40.45
Episode length: 47.95 +/- 17.65
New best mean reward!
[I 2024-12-11 00:22:06,490] Trial 365 pruned. 
[I 2024-12-11 00:22:18,870] Trial 355 finished with value: 341.0126326 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.0005821363630661247, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 00:23:03,693] Trial 358 finished with value: 341.0260969 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.0005629720410959739, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=353.47 +/- 38.68
Episode length: 45.90 +/- 13.31
New best mean reward!
Eval num_timesteps=320000, episode_reward=351.26 +/- 41.16
Episode length: 48.15 +/- 17.38
New best mean reward!
[I 2024-12-11 00:25:07,323] Trial 367 pruned. 
Eval num_timesteps=320000, episode_reward=353.54 +/- 38.34
Episode length: 45.75 +/- 13.05
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.79 +/- 38.19
Episode length: 45.75 +/- 13.11
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.26 +/- 38.85
Episode length: 45.90 +/- 13.37
New best mean reward!
[I 2024-12-11 00:27:18,962] Trial 366 finished with value: 340.96874415 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.00045014975608565123, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=352.43 +/- 39.70
Episode length: 46.20 +/- 13.75
New best mean reward!
[I 2024-12-11 00:28:09,099] Trial 374 pruned. 
Eval num_timesteps=320000, episode_reward=344.92 +/- 43.44
Episode length: 51.55 +/- 17.65
New best mean reward!
[I 2024-12-11 00:29:01,492] Trial 376 pruned. 
Eval num_timesteps=320000, episode_reward=352.27 +/- 39.70
Episode length: 46.00 +/- 13.63
New best mean reward!
[I 2024-12-11 00:29:11,846] Trial 368 pruned. 
[I 2024-12-11 00:29:12,430] Trial 370 finished with value: 340.89984710000004 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.00047944848030302684, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 00:29:27,139] Trial 371 finished with value: 341.20004895 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.0002764320704444358, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=351.41 +/- 41.12
Episode length: 49.55 +/- 19.96
New best mean reward!
[I 2024-12-11 00:29:47,765] Trial 369 pruned. 
[I 2024-12-11 00:30:31,595] Trial 372 finished with value: 340.72104735000005 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.0001402302683266699, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=352.65 +/- 39.25
Episode length: 46.10 +/- 13.48
New best mean reward!
[I 2024-12-11 00:30:43,607] Trial 378 pruned. 
Eval num_timesteps=320000, episode_reward=352.43 +/- 38.97
Episode length: 46.10 +/- 13.28
New best mean reward!
[I 2024-12-11 00:31:15,971] Trial 379 pruned. 
Eval num_timesteps=320000, episode_reward=352.38 +/- 39.49
Episode length: 46.00 +/- 13.50
New best mean reward!
[I 2024-12-11 00:31:16,396] Trial 380 pruned. 
Eval num_timesteps=320000, episode_reward=352.61 +/- 38.30
Episode length: 46.05 +/- 13.16
New best mean reward!
[I 2024-12-11 00:31:47,924] Trial 373 pruned. 
Eval num_timesteps=320000, episode_reward=353.74 +/- 38.37
Episode length: 45.70 +/- 13.27
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.06 +/- 39.32
Episode length: 46.25 +/- 13.46
New best mean reward!
[I 2024-12-11 00:33:37,461] Trial 385 pruned. 
Eval num_timesteps=320000, episode_reward=340.04 +/- 49.54
Episode length: 51.95 +/- 19.52
New best mean reward!
[I 2024-12-11 00:34:16,399] Trial 381 pruned. 
Eval num_timesteps=320000, episode_reward=352.69 +/- 39.08
Episode length: 46.10 +/- 13.36
New best mean reward!
[I 2024-12-11 00:34:32,653] Trial 382 pruned. 
Eval num_timesteps=320000, episode_reward=353.83 +/- 38.28
Episode length: 45.70 +/- 13.33
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.31 +/- 38.67
Episode length: 45.50 +/- 13.11
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.99 +/- 38.22
Episode length: 45.50 +/- 13.10
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.52 +/- 38.65
Episode length: 45.75 +/- 13.27
New best mean reward!
Eval num_timesteps=320000, episode_reward=351.84 +/- 39.53
Episode length: 46.40 +/- 13.52
New best mean reward!
[I 2024-12-11 00:36:29,279] Trial 390 pruned. 
Eval num_timesteps=320000, episode_reward=345.89 +/- 44.41
Episode length: 48.85 +/- 15.60
New best mean reward!
[I 2024-12-11 00:36:44,122] Trial 377 pruned. 
[I 2024-12-11 00:37:26,020] Trial 384 finished with value: 341.1182475 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.00021018473398967314, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=353.43 +/- 38.40
Episode length: 45.75 +/- 13.20
New best mean reward!
[I 2024-12-11 00:38:13,859] Trial 375 finished with value: 341.24160965 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.00012706782682556786, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=353.58 +/- 38.99
Episode length: 46.65 +/- 15.12
New best mean reward!
Eval num_timesteps=320000, episode_reward=351.21 +/- 40.21
Episode length: 46.60 +/- 13.84
New best mean reward!
[I 2024-12-11 00:38:49,036] Trial 391 pruned. 
[I 2024-12-11 00:38:56,474] Trial 386 finished with value: 337.181244 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.00019506487896089794, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=351.71 +/- 39.28
Episode length: 46.35 +/- 13.32
New best mean reward!
[I 2024-12-11 00:39:02,867] Trial 392 pruned. 
[I 2024-12-11 00:39:21,004] Trial 383 finished with value: 340.4049411 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.0002842143477499816, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 00:39:25,529] Trial 387 finished with value: 341.1821875 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.00017510224724121899, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=352.88 +/- 39.01
Episode length: 46.25 +/- 13.58
New best mean reward!
[I 2024-12-11 00:39:32,186] Trial 393 pruned. 
Eval num_timesteps=320000, episode_reward=348.97 +/- 39.51
Episode length: 48.35 +/- 13.90
New best mean reward!
[I 2024-12-11 00:40:47,388] Trial 396 pruned. 
[I 2024-12-11 00:41:17,023] Trial 388 finished with value: 340.94468145 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.00015600675353502608, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 00:41:52,063] Trial 389 finished with value: 340.21369560000005 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.00018303314031618107, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=160000, episode_reward=268.05 +/- 69.70
Episode length: 100.05 +/- 52.59
New best mean reward!
[I 2024-12-11 00:43:07,677] Trial 400 pruned. 
Eval num_timesteps=160000, episode_reward=311.12 +/- 35.24
Episode length: 65.75 +/- 15.32
New best mean reward!
[I 2024-12-11 00:44:50,154] Trial 401 pruned. 
Eval num_timesteps=160000, episode_reward=321.80 +/- 32.04
Episode length: 58.00 +/- 11.26
New best mean reward!
[I 2024-12-11 00:45:21,562] Trial 402 pruned. 
Eval num_timesteps=320000, episode_reward=354.22 +/- 38.17
Episode length: 45.20 +/- 13.01
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.15 +/- 39.58
Episode length: 46.15 +/- 13.54
New best mean reward!
[I 2024-12-11 00:46:48,245] Trial 395 pruned. 
Eval num_timesteps=320000, episode_reward=353.41 +/- 38.64
Episode length: 45.70 +/- 13.30
New best mean reward!
Eval num_timesteps=320000, episode_reward=351.50 +/- 39.32
Episode length: 47.10 +/- 13.70
New best mean reward!
[I 2024-12-11 00:47:26,732] Trial 399 pruned. 
Eval num_timesteps=320000, episode_reward=351.88 +/- 39.92
Episode length: 46.30 +/- 13.85
New best mean reward!
[I 2024-12-11 00:48:09,050] Trial 398 pruned. 
[W 2024-12-11 00:48:42,992] Trial 408 failed with parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.9406223246311376, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'} because of the following error: ValueError('Expected parameter loc (Tensor of shape (64, 2)) of distribution Normal(loc: torch.Size([64, 2]), scale: torch.Size([64, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]], grad_fn=<AddmmBackward0>)').
Traceback (most recent call last):
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/optuna/study/_optimize.py", line 197, in _run_trial
    value_or_values = func(trial)
  File "/mnt/personal/mrkosmic/synced/RL-cable/experiments/hyperopt/../../scripts/hyperopt.py", line 85, in objectiveRect2D
    model.learn(total_timesteps=TIMESTEPS, callback=[save_norm_clb, eval_clb])
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 336, in learn
    self.train()
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 213, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 737, in evaluate_actions
    distribution = self._get_action_dist_from_latent(latent_pi)
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 694, in _get_action_dist_from_latent
    return self.action_dist.proba_distribution(mean_actions, self.log_std)
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/common/distributions.py", line 164, in proba_distribution
    self.distribution = Normal(mean_actions, action_std)
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/torch/distributions/normal.py", line 59, in __init__
    super().__init__(batch_shape, validate_args=validate_args)
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/torch/distributions/distribution.py", line 71, in __init__
    raise ValueError(
ValueError: Expected parameter loc (Tensor of shape (64, 2)) of distribution Normal(loc: torch.Size([64, 2]), scale: torch.Size([64, 2])) to satisfy the constraint Real(), but found invalid values:
tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], grad_fn=<AddmmBackward0>)
[W 2024-12-11 00:48:42,995] Trial 408 failed with value None.
Eval num_timesteps=40000, episode_reward=324.50 +/- 38.58
Episode length: 64.30 +/- 21.59
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.04 +/- 39.01
Episode length: 46.00 +/- 13.48
New best mean reward!
[I 2024-12-11 00:49:23,874] Trial 405 pruned. 
Eval num_timesteps=320000, episode_reward=336.04 +/- 43.71
Episode length: 59.15 +/- 21.24
New best mean reward!
[I 2024-12-11 00:49:39,679] Trial 403 pruned. 
Eval num_timesteps=320000, episode_reward=353.11 +/- 39.24
Episode length: 45.80 +/- 13.56
New best mean reward!
[I 2024-12-11 00:49:52,697] Trial 404 pruned. 
Eval num_timesteps=320000, episode_reward=352.71 +/- 38.94
Episode length: 45.95 +/- 13.49
New best mean reward!
[I 2024-12-11 00:49:59,454] Trial 406 pruned. 
Eval num_timesteps=80000, episode_reward=350.35 +/- 62.53
Episode length: 49.45 +/- 22.22
New best mean reward!
Eval num_timesteps=40000, episode_reward=-2405.45 +/- 430.50
Episode length: 1000.00 +/- 0.00
New best mean reward!
[I 2024-12-11 00:50:30,810] Trial 411 pruned. 
Eval num_timesteps=40000, episode_reward=-1898.86 +/- 1081.24
Episode length: 855.40 +/- 344.27
New best mean reward!
[I 2024-12-11 00:50:50,625] Trial 413 pruned. 
Eval num_timesteps=120000, episode_reward=326.60 +/- 46.58
Episode length: 56.75 +/- 18.66
[I 2024-12-11 00:52:02,321] Trial 394 finished with value: 341.2454032 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.00016360862638946822, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 00:52:10,475] Trial 397 finished with value: 341.2503921 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.0001148160109094425, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=160000, episode_reward=337.13 +/- 49.73
Episode length: 51.35 +/- 17.03
Eval num_timesteps=320000, episode_reward=352.93 +/- 38.90
Episode length: 45.95 +/- 13.26
New best mean reward!
[I 2024-12-11 00:52:34,636] Trial 414 pruned. 
Eval num_timesteps=200000, episode_reward=359.82 +/- 52.09
Episode length: 43.50 +/- 17.77
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.94 +/- 38.59
Episode length: 46.05 +/- 13.33
New best mean reward!
[I 2024-12-11 00:53:54,051] Trial 410 pruned. 
Eval num_timesteps=320000, episode_reward=353.56 +/- 38.48
Episode length: 45.60 +/- 13.10
New best mean reward!
Eval num_timesteps=240000, episode_reward=340.15 +/- 42.03
Episode length: 55.05 +/- 23.76
Eval num_timesteps=320000, episode_reward=352.15 +/- 39.63
Episode length: 46.60 +/- 13.92
New best mean reward!
[I 2024-12-11 00:54:31,296] Trial 412 pruned. 
Eval num_timesteps=280000, episode_reward=326.33 +/- 31.75
Episode length: 54.70 +/- 10.83
Eval num_timesteps=320000, episode_reward=352.38 +/- 39.78
Episode length: 46.20 +/- 13.68
New best mean reward!
[I 2024-12-11 00:55:55,997] Trial 415 pruned. 
Eval num_timesteps=320000, episode_reward=345.30 +/- 39.08
Episode length: 48.25 +/- 13.40
Eval num_timesteps=360000, episode_reward=326.18 +/- 34.54
Episode length: 54.80 +/- 11.88
Eval num_timesteps=400000, episode_reward=330.41 +/- 16.49
Episode length: 53.40 +/- 5.54
[I 2024-12-11 00:59:17,172] Trial 407 finished with value: 341.13229334999994 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 9.756815101773399e-05, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=440000, episode_reward=338.75 +/- 44.03
Episode length: 52.25 +/- 19.69
Eval num_timesteps=320000, episode_reward=352.81 +/- 39.23
Episode length: 46.00 +/- 13.51
New best mean reward!
[I 2024-12-11 00:59:22,342] Trial 416 pruned. 
Eval num_timesteps=320000, episode_reward=353.30 +/- 38.77
Episode length: 45.85 +/- 13.41
New best mean reward!
[I 2024-12-11 00:59:34,772] Trial 417 pruned. 
Eval num_timesteps=320000, episode_reward=351.39 +/- 40.09
Episode length: 47.95 +/- 16.53
New best mean reward!
[I 2024-12-11 00:59:54,022] Trial 418 pruned. 
Eval num_timesteps=480000, episode_reward=327.48 +/- 31.03
Episode length: 58.60 +/- 17.16
[I 2024-12-11 01:00:57,061] Trial 409 finished with value: 321.97552145 and parameters: {'n_envs': 4, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.00013482710979296014, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=352.19 +/- 39.67
Episode length: 46.30 +/- 13.75
New best mean reward!
[I 2024-12-11 01:01:14,971] Trial 420 pruned. 
Eval num_timesteps=320000, episode_reward=352.17 +/- 40.06
Episode length: 46.25 +/- 13.86
New best mean reward!
[I 2024-12-11 01:01:26,199] Trial 419 pruned. 
Eval num_timesteps=320000, episode_reward=353.10 +/- 39.35
Episode length: 45.80 +/- 13.41
New best mean reward!
[I 2024-12-11 01:03:25,815] Trial 421 pruned. 
Eval num_timesteps=320000, episode_reward=352.74 +/- 39.51
Episode length: 45.95 +/- 13.43
New best mean reward!
[I 2024-12-11 01:06:49,274] Trial 422 pruned. 
Eval num_timesteps=320000, episode_reward=354.18 +/- 38.21
Episode length: 45.35 +/- 13.13
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.96 +/- 38.18
Episode length: 45.40 +/- 12.96
New best mean reward!
Eval num_timesteps=320000, episode_reward=-1245.60 +/- 2020.40
Episode length: 488.20 +/- 463.28
New best mean reward!
[I 2024-12-11 01:07:21,487] Trial 424 pruned. 
Eval num_timesteps=320000, episode_reward=349.87 +/- 43.37
Episode length: 48.65 +/- 19.99
New best mean reward!
[I 2024-12-11 01:08:20,868] Trial 427 pruned. 
Eval num_timesteps=320000, episode_reward=-1506.25 +/- 1626.45
Episode length: 570.35 +/- 475.17
New best mean reward!
[I 2024-12-11 01:09:21,849] Trial 428 pruned. 
Eval num_timesteps=320000, episode_reward=-264.11 +/- 624.41
Episode length: 69.95 +/- 26.74
New best mean reward!
[I 2024-12-11 01:10:02,328] Trial 426 pruned. 
Eval num_timesteps=320000, episode_reward=352.32 +/- 39.22
Episode length: 46.15 +/- 13.45
New best mean reward!
[I 2024-12-11 01:10:19,291] Trial 432 pruned. 
Eval num_timesteps=320000, episode_reward=351.84 +/- 37.89
Episode length: 47.30 +/- 13.25
New best mean reward!
[I 2024-12-11 01:11:08,274] Trial 429 pruned. 
[I 2024-12-11 01:12:02,402] Trial 423 finished with value: 338.0180457 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.0001404190420373815, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 01:12:21,427] Trial 425 finished with value: 338.13502030000006 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.00012972654644219475, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=353.07 +/- 39.05
Episode length: 45.80 +/- 13.40
New best mean reward!
[I 2024-12-11 01:13:28,676] Trial 438 pruned. 
Eval num_timesteps=320000, episode_reward=353.53 +/- 38.53
Episode length: 45.70 +/- 13.24
New best mean reward!
[I 2024-12-11 01:15:00,578] Trial 437 finished with value: 341.17745729999996 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0003269328373326166, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=343.93 +/- 50.53
Episode length: 55.10 +/- 27.69
New best mean reward!
[I 2024-12-11 01:15:09,446] Trial 431 pruned. 
Eval num_timesteps=320000, episode_reward=-441.41 +/- 464.62
Episode length: 521.00 +/- 290.90
New best mean reward!
[I 2024-12-11 01:15:24,185] Trial 439 pruned. 
Eval num_timesteps=320000, episode_reward=-2163.09 +/- 1118.78
Episode length: 226.95 +/- 109.69
New best mean reward!
[I 2024-12-11 01:16:25,957] Trial 430 pruned. 
Eval num_timesteps=320000, episode_reward=352.00 +/- 39.35
Episode length: 46.25 +/- 13.51
New best mean reward!
[I 2024-12-11 01:16:59,359] Trial 440 pruned. 
Eval num_timesteps=320000, episode_reward=350.74 +/- 40.32
Episode length: 46.60 +/- 13.56
New best mean reward!
[I 2024-12-11 01:17:06,892] Trial 441 pruned. 
Eval num_timesteps=320000, episode_reward=353.48 +/- 38.45
Episode length: 46.00 +/- 13.35
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.40 +/- 39.34
Episode length: 46.15 +/- 13.44
New best mean reward!
[I 2024-12-11 01:17:20,412] Trial 442 pruned. 
Eval num_timesteps=320000, episode_reward=352.86 +/- 39.17
Episode length: 46.10 +/- 13.55
New best mean reward!
[I 2024-12-11 01:17:58,594] Trial 435 pruned. 
Eval num_timesteps=320000, episode_reward=350.62 +/- 40.76
Episode length: 46.45 +/- 13.75
New best mean reward!
[I 2024-12-11 01:18:25,275] Trial 443 pruned. 
Eval num_timesteps=320000, episode_reward=347.62 +/- 43.39
Episode length: 52.55 +/- 22.51
New best mean reward!
[I 2024-12-11 01:18:40,750] Trial 434 pruned. 
Eval num_timesteps=320000, episode_reward=352.98 +/- 38.59
Episode length: 46.10 +/- 13.21
New best mean reward!
[I 2024-12-11 01:19:00,603] Trial 444 pruned. 
Eval num_timesteps=320000, episode_reward=351.27 +/- 40.34
Episode length: 46.25 +/- 13.76
New best mean reward!
[I 2024-12-11 01:19:01,284] Trial 445 pruned. 
Eval num_timesteps=320000, episode_reward=272.32 +/- 265.29
Episode length: 60.85 +/- 21.25
New best mean reward!
[I 2024-12-11 01:19:17,855] Trial 446 pruned. 
Eval num_timesteps=320000, episode_reward=-2957.34 +/- 2697.96
Episode length: 765.10 +/- 407.33
New best mean reward!
[I 2024-12-11 01:19:55,358] Trial 447 pruned. 
Eval num_timesteps=320000, episode_reward=346.26 +/- 41.56
Episode length: 49.35 +/- 14.53
New best mean reward!
[I 2024-12-11 01:20:03,594] Trial 449 pruned. 
Eval num_timesteps=320000, episode_reward=-1964.85 +/- 1161.50
Episode length: 911.10 +/- 267.35
New best mean reward!
[I 2024-12-11 01:20:25,909] Trial 448 pruned. 
Eval num_timesteps=320000, episode_reward=-2619.00 +/- 1445.84
Episode length: 307.55 +/- 152.29
New best mean reward!
[I 2024-12-11 01:20:40,824] Trial 436 pruned. 
Eval num_timesteps=320000, episode_reward=338.83 +/- 34.94
Episode length: 54.80 +/- 13.82
New best mean reward!
[I 2024-12-11 01:21:40,885] Trial 450 pruned. 
Eval num_timesteps=320000, episode_reward=339.09 +/- 43.84
Episode length: 58.05 +/- 27.28
New best mean reward!
[I 2024-12-11 01:21:49,273] Trial 452 pruned. 
Eval num_timesteps=320000, episode_reward=-4869.21 +/- 3198.47
Episode length: 823.85 +/- 356.04
New best mean reward!
[I 2024-12-11 01:21:51,666] Trial 451 pruned. 
[I 2024-12-11 01:22:43,155] Trial 433 finished with value: 341.09165445 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.00019267033160655963, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=351.55 +/- 39.49
Episode length: 46.65 +/- 13.58
New best mean reward!
[I 2024-12-11 01:23:59,706] Trial 453 pruned. 
Eval num_timesteps=320000, episode_reward=348.47 +/- 40.37
Episode length: 48.35 +/- 14.03
New best mean reward!
[I 2024-12-11 01:24:04,657] Trial 458 pruned. 
Eval num_timesteps=320000, episode_reward=347.46 +/- 37.21
Episode length: 48.35 +/- 13.28
New best mean reward!
[I 2024-12-11 01:24:08,366] Trial 457 pruned. 
Eval num_timesteps=320000, episode_reward=354.20 +/- 38.02
Episode length: 45.40 +/- 12.93
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.11 +/- 38.86
Episode length: 45.70 +/- 13.24
New best mean reward!
[I 2024-12-11 01:25:01,786] Trial 460 pruned. 
Eval num_timesteps=320000, episode_reward=342.74 +/- 47.14
Episode length: 51.60 +/- 19.21
New best mean reward!
[I 2024-12-11 01:25:08,194] Trial 454 pruned. 
Eval num_timesteps=320000, episode_reward=352.48 +/- 39.12
Episode length: 46.35 +/- 13.38
New best mean reward!
[I 2024-12-11 01:25:11,097] Trial 456 pruned. 
[I 2024-12-11 01:25:47,960] Trial 459 finished with value: 341.1334648 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.99, 'learning_rate': 0.0004377691830188811, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=-915.35 +/- 240.24
Episode length: 61.55 +/- 22.60
New best mean reward!
[I 2024-12-11 01:26:00,446] Trial 455 pruned. 
Eval num_timesteps=320000, episode_reward=353.02 +/- 38.60
Episode length: 46.00 +/- 13.18
New best mean reward!
[I 2024-12-11 01:26:04,218] Trial 462 pruned. 
Eval num_timesteps=320000, episode_reward=348.87 +/- 42.10
Episode length: 47.15 +/- 14.38
New best mean reward!
[I 2024-12-11 01:26:05,116] Trial 461 pruned. 
Eval num_timesteps=320000, episode_reward=353.29 +/- 38.65
Episode length: 45.80 +/- 13.24
New best mean reward!
[I 2024-12-11 01:26:05,212] Trial 463 pruned. 
Eval num_timesteps=320000, episode_reward=351.28 +/- 40.17
Episode length: 46.60 +/- 13.74
New best mean reward!
[I 2024-12-11 01:26:18,869] Trial 464 pruned. 
Eval num_timesteps=160000, episode_reward=300.16 +/- 43.95
Episode length: 67.40 +/- 16.24
New best mean reward!
[I 2024-12-11 01:26:25,135] Trial 466 pruned. 
Eval num_timesteps=160000, episode_reward=320.50 +/- 32.63
Episode length: 59.40 +/- 15.67
New best mean reward!
[I 2024-12-11 01:26:37,128] Trial 467 pruned. 
Eval num_timesteps=320000, episode_reward=351.92 +/- 39.30
Episode length: 46.55 +/- 13.53
New best mean reward!
[I 2024-12-11 01:26:38,858] Trial 465 pruned. 
Eval num_timesteps=160000, episode_reward=320.51 +/- 32.14
Episode length: 60.45 +/- 16.19
New best mean reward!
[I 2024-12-11 01:26:47,921] Trial 468 pruned. 
Eval num_timesteps=40000, episode_reward=114.91 +/- 125.55
Episode length: 176.20 +/- 57.45
New best mean reward!
[I 2024-12-11 01:26:50,384] Trial 471 pruned. 
Eval num_timesteps=40000, episode_reward=-264.28 +/- 579.11
Episode length: 80.30 +/- 31.38
New best mean reward!
[I 2024-12-11 01:27:09,446] Trial 473 pruned. 
Eval num_timesteps=160000, episode_reward=325.06 +/- 31.19
Episode length: 55.95 +/- 10.74
New best mean reward!
[I 2024-12-11 01:27:21,443] Trial 470 pruned. 
Eval num_timesteps=160000, episode_reward=322.46 +/- 33.02
Episode length: 56.90 +/- 11.59
New best mean reward!
[I 2024-12-11 01:27:21,685] Trial 469 pruned. 
Eval num_timesteps=40000, episode_reward=-943.55 +/- 465.67
Episode length: 117.25 +/- 114.54
New best mean reward!
[I 2024-12-11 01:27:30,512] Trial 475 pruned. 
Eval num_timesteps=40000, episode_reward=-4401.94 +/- 4189.28
Episode length: 763.60 +/- 409.53
New best mean reward!
[I 2024-12-11 01:27:34,941] Trial 474 pruned. 
Eval num_timesteps=160000, episode_reward=314.44 +/- 40.81
Episode length: 60.95 +/- 17.14
New best mean reward!
[I 2024-12-11 01:28:13,666] Trial 472 pruned. 
Eval num_timesteps=320000, episode_reward=304.15 +/- 51.93
Episode length: 75.05 +/- 29.89
New best mean reward!
[I 2024-12-11 01:29:14,271] Trial 478 pruned. 
Eval num_timesteps=320000, episode_reward=339.61 +/- 42.16
Episode length: 56.35 +/- 18.08
New best mean reward!
[I 2024-12-11 01:29:19,362] Trial 479 pruned. 
Eval num_timesteps=320000, episode_reward=336.15 +/- 46.18
Episode length: 57.70 +/- 20.38
New best mean reward!
[I 2024-12-11 01:29:23,144] Trial 480 pruned. 
Eval num_timesteps=320000, episode_reward=348.56 +/- 39.20
Episode length: 50.25 +/- 15.83
New best mean reward!
[I 2024-12-11 01:29:38,278] Trial 482 pruned. 
Eval num_timesteps=320000, episode_reward=350.07 +/- 37.02
Episode length: 48.55 +/- 13.31
New best mean reward!
[I 2024-12-11 01:29:41,066] Trial 481 pruned. 
Eval num_timesteps=320000, episode_reward=332.73 +/- 44.80
Episode length: 57.85 +/- 20.34
New best mean reward!
[I 2024-12-11 01:29:56,593] Trial 477 pruned. 
Eval num_timesteps=320000, episode_reward=322.57 +/- 57.98
Episode length: 63.60 +/- 26.13
New best mean reward!
[I 2024-12-11 01:30:07,362] Trial 476 pruned. 
Eval num_timesteps=320000, episode_reward=344.83 +/- 44.15
Episode length: 51.30 +/- 16.90
New best mean reward!
[I 2024-12-11 01:30:11,508] Trial 483 pruned. 
Eval num_timesteps=320000, episode_reward=352.59 +/- 37.88
Episode length: 46.75 +/- 13.25
New best mean reward!
[I 2024-12-11 01:30:48,989] Trial 486 pruned. 
Eval num_timesteps=320000, episode_reward=351.39 +/- 39.57
Episode length: 46.60 +/- 13.46
New best mean reward!
[I 2024-12-11 01:31:01,749] Trial 485 pruned. 
Eval num_timesteps=320000, episode_reward=353.60 +/- 37.64
Episode length: 45.65 +/- 12.88
New best mean reward!
Eval num_timesteps=320000, episode_reward=166.32 +/- 109.89
Episode length: 188.60 +/- 79.97
New best mean reward!
[I 2024-12-11 01:31:18,475] Trial 488 pruned. 
Eval num_timesteps=320000, episode_reward=349.97 +/- 39.31
Episode length: 49.05 +/- 14.68
New best mean reward!
[I 2024-12-11 01:31:23,550] Trial 489 pruned. 
Eval num_timesteps=320000, episode_reward=350.39 +/- 39.93
Episode length: 48.40 +/- 15.73
New best mean reward!
[I 2024-12-11 01:31:40,906] Trial 487 pruned. 
Eval num_timesteps=320000, episode_reward=351.52 +/- 39.89
Episode length: 46.50 +/- 13.78
New best mean reward!
[I 2024-12-11 01:32:03,711] Trial 490 pruned. 
[I 2024-12-11 01:32:28,638] Trial 484 finished with value: 341.03279075 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0016477181796940269, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[W 2024-12-11 01:34:18,214] Trial 496 failed with parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.9999, 'learning_rate': 0.9383056454993017, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'} because of the following error: ValueError('Expected parameter loc (Tensor of shape (32, 2)) of distribution Normal(loc: torch.Size([32, 2]), scale: torch.Size([32, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]], grad_fn=<AddmmBackward0>)').
Traceback (most recent call last):
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/optuna/study/_optimize.py", line 197, in _run_trial
    value_or_values = func(trial)
  File "/mnt/personal/mrkosmic/synced/RL-cable/experiments/hyperopt/../../scripts/hyperopt.py", line 85, in objectiveRect2D
    model.learn(total_timesteps=TIMESTEPS, callback=[save_norm_clb, eval_clb])
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 336, in learn
    self.train()
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 213, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 737, in evaluate_actions
    distribution = self._get_action_dist_from_latent(latent_pi)
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 694, in _get_action_dist_from_latent
    return self.action_dist.proba_distribution(mean_actions, self.log_std)
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/stable_baselines3/common/distributions.py", line 164, in proba_distribution
    self.distribution = Normal(mean_actions, action_std)
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/torch/distributions/normal.py", line 59, in __init__
    super().__init__(batch_shape, validate_args=validate_args)
  File "/home/mrkosmic/.local/lib/python3.10/site-packages/torch/distributions/distribution.py", line 71, in __init__
    raise ValueError(
ValueError: Expected parameter loc (Tensor of shape (32, 2)) of distribution Normal(loc: torch.Size([32, 2]), scale: torch.Size([32, 2])) to satisfy the constraint Real(), but found invalid values:
tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], grad_fn=<AddmmBackward0>)
[W 2024-12-11 01:34:18,217] Trial 496 failed with value None.
Eval num_timesteps=320000, episode_reward=352.57 +/- 39.16
Episode length: 46.40 +/- 13.62
New best mean reward!
[I 2024-12-11 01:37:48,776] Trial 493 pruned. 
Eval num_timesteps=320000, episode_reward=353.60 +/- 38.75
Episode length: 45.70 +/- 13.44
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.10 +/- 40.91
Episode length: 47.75 +/- 17.87
New best mean reward!
[I 2024-12-11 01:38:55,813] Trial 491 pruned. 
Eval num_timesteps=320000, episode_reward=351.74 +/- 40.07
Episode length: 46.20 +/- 13.65
New best mean reward!
[I 2024-12-11 01:39:57,287] Trial 495 pruned. 
Eval num_timesteps=320000, episode_reward=345.71 +/- 47.43
Episode length: 51.90 +/- 22.34
New best mean reward!
[I 2024-12-11 01:40:50,330] Trial 497 pruned. 
Eval num_timesteps=320000, episode_reward=346.06 +/- 43.78
Episode length: 51.15 +/- 17.38
New best mean reward!
[I 2024-12-11 01:41:15,367] Trial 498 pruned. 
Eval num_timesteps=320000, episode_reward=348.12 +/- 43.01
Episode length: 50.00 +/- 17.70
New best mean reward!
[I 2024-12-11 01:41:26,142] Trial 494 pruned. 
Eval num_timesteps=320000, episode_reward=352.48 +/- 38.99
Episode length: 46.00 +/- 13.22
New best mean reward!
[I 2024-12-11 01:41:42,909] Trial 501 pruned. 
Eval num_timesteps=320000, episode_reward=351.15 +/- 41.69
Episode length: 48.10 +/- 18.26
New best mean reward!
[I 2024-12-11 01:42:43,453] Trial 502 pruned. 
[I 2024-12-11 01:42:46,429] Trial 492 finished with value: 341.13959619999997 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.00011647272753950617, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=353.32 +/- 38.83
Episode length: 45.75 +/- 13.28
New best mean reward!
[I 2024-12-11 01:43:07,498] Trial 503 pruned. 
Eval num_timesteps=320000, episode_reward=353.30 +/- 38.36
Episode length: 45.85 +/- 13.29
New best mean reward!
[I 2024-12-11 01:43:30,685] Trial 504 pruned. 
Eval num_timesteps=320000, episode_reward=349.41 +/- 41.57
Episode length: 47.35 +/- 14.29
New best mean reward!
[I 2024-12-11 01:43:34,761] Trial 505 pruned. 
Eval num_timesteps=320000, episode_reward=351.35 +/- 39.79
Episode length: 46.65 +/- 13.54
New best mean reward!
[I 2024-12-11 01:44:31,638] Trial 506 pruned. 
Eval num_timesteps=320000, episode_reward=353.32 +/- 38.49
Episode length: 45.95 +/- 13.24
New best mean reward!
[I 2024-12-11 01:44:36,571] Trial 507 pruned. 
Eval num_timesteps=320000, episode_reward=352.83 +/- 38.80
Episode length: 46.20 +/- 13.59
New best mean reward!
[I 2024-12-11 01:44:58,547] Trial 499 pruned. 
Eval num_timesteps=320000, episode_reward=350.49 +/- 41.40
Episode length: 48.30 +/- 17.74
New best mean reward!
[I 2024-12-11 01:44:59,341] Trial 508 pruned. 
Eval num_timesteps=320000, episode_reward=352.77 +/- 38.84
Episode length: 46.05 +/- 13.37
New best mean reward!
[I 2024-12-11 01:45:29,629] Trial 510 pruned. 
Eval num_timesteps=320000, episode_reward=351.70 +/- 40.28
Episode length: 47.95 +/- 16.83
New best mean reward!
[I 2024-12-11 01:45:37,328] Trial 509 pruned. 
Eval num_timesteps=320000, episode_reward=351.45 +/- 39.93
Episode length: 46.25 +/- 13.60
New best mean reward!
[I 2024-12-11 01:46:28,982] Trial 511 pruned. 
Eval num_timesteps=320000, episode_reward=347.85 +/- 42.65
Episode length: 47.80 +/- 14.66
New best mean reward!
[I 2024-12-11 01:46:34,975] Trial 512 pruned. 
Eval num_timesteps=320000, episode_reward=335.36 +/- 61.20
Episode length: 53.65 +/- 24.25
New best mean reward!
[I 2024-12-11 01:46:50,344] Trial 513 pruned. 
Eval num_timesteps=320000, episode_reward=349.98 +/- 40.80
Episode length: 46.85 +/- 13.86
New best mean reward!
[I 2024-12-11 01:46:51,712] Trial 514 pruned. 
Eval num_timesteps=320000, episode_reward=353.40 +/- 38.56
Episode length: 45.70 +/- 13.24
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.23 +/- 39.39
Episode length: 46.00 +/- 13.48
New best mean reward!
[I 2024-12-11 01:47:49,139] Trial 500 pruned. 
[I 2024-12-11 01:49:03,739] Trial 516 finished with value: 341.15537865 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0003770088284833563, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=349.02 +/- 41.65
Episode length: 49.20 +/- 15.79
New best mean reward!
[I 2024-12-11 01:50:37,631] Trial 515 pruned. 
Eval num_timesteps=320000, episode_reward=353.61 +/- 38.66
Episode length: 45.70 +/- 13.33
New best mean reward!
Eval num_timesteps=320000, episode_reward=-809.39 +/- 462.74
Episode length: 62.85 +/- 21.62
New best mean reward!
[I 2024-12-11 01:51:17,558] Trial 522 pruned. 
Eval num_timesteps=320000, episode_reward=351.99 +/- 39.55
Episode length: 46.55 +/- 13.79
New best mean reward!
[I 2024-12-11 01:51:18,936] Trial 519 pruned. 
Eval num_timesteps=320000, episode_reward=352.07 +/- 39.96
Episode length: 46.15 +/- 13.58
New best mean reward!
[I 2024-12-11 01:51:24,961] Trial 518 pruned. 
Eval num_timesteps=320000, episode_reward=191.36 +/- 345.47
Episode length: 61.05 +/- 21.36
New best mean reward!
[I 2024-12-11 01:51:37,378] Trial 520 pruned. 
Eval num_timesteps=320000, episode_reward=339.63 +/- 49.70
Episode length: 53.35 +/- 21.05
New best mean reward!
[I 2024-12-11 01:52:38,090] Trial 521 pruned. 
Eval num_timesteps=320000, episode_reward=-4937.65 +/- 2917.72
Episode length: 958.40 +/- 181.33
New best mean reward!
[I 2024-12-11 01:52:51,683] Trial 523 pruned. 
Eval num_timesteps=320000, episode_reward=353.81 +/- 38.21
Episode length: 45.65 +/- 13.21
New best mean reward!
Eval num_timesteps=320000, episode_reward=351.64 +/- 39.79
Episode length: 46.40 +/- 13.66
New best mean reward!
[I 2024-12-11 01:53:22,327] Trial 526 pruned. 
Eval num_timesteps=320000, episode_reward=353.69 +/- 38.45
Episode length: 45.65 +/- 13.13
New best mean reward!
Eval num_timesteps=320000, episode_reward=350.13 +/- 40.59
Episode length: 47.10 +/- 13.90
New best mean reward!
[I 2024-12-11 01:53:33,572] Trial 527 pruned. 
[I 2024-12-11 01:54:17,393] Trial 525 finished with value: 341.13068585 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.00131771355374286, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=350.12 +/- 39.74
Episode length: 47.50 +/- 13.72
New best mean reward!
[I 2024-12-11 01:54:23,889] Trial 528 pruned. 
[I 2024-12-11 01:54:37,169] Trial 517 finished with value: 340.96544654999997 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.0006599423606415641, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=349.71 +/- 40.69
Episode length: 47.25 +/- 13.88
New best mean reward!
[I 2024-12-11 01:54:43,293] Trial 529 pruned. 
[I 2024-12-11 01:54:48,556] Trial 524 finished with value: 341.17556594999996 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0006716227277619366, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=348.11 +/- 41.60
Episode length: 48.20 +/- 14.74
New best mean reward!
[I 2024-12-11 01:55:17,456] Trial 530 pruned. 
Eval num_timesteps=320000, episode_reward=352.23 +/- 39.34
Episode length: 46.25 +/- 13.62
New best mean reward!
[I 2024-12-11 01:55:29,262] Trial 531 pruned. 
Eval num_timesteps=320000, episode_reward=348.89 +/- 36.22
Episode length: 49.10 +/- 13.02
New best mean reward!
[I 2024-12-11 01:56:02,213] Trial 536 pruned. 
Eval num_timesteps=320000, episode_reward=353.09 +/- 38.93
Episode length: 45.85 +/- 13.37
New best mean reward!
[I 2024-12-11 01:56:04,156] Trial 532 pruned. 
Eval num_timesteps=320000, episode_reward=353.31 +/- 39.00
Episode length: 45.75 +/- 13.45
New best mean reward!
[I 2024-12-11 01:56:22,427] Trial 533 pruned. 
Eval num_timesteps=320000, episode_reward=352.32 +/- 39.50
Episode length: 46.15 +/- 13.58
New best mean reward!
Eval num_timesteps=320000, episode_reward=351.58 +/- 39.98
Episode length: 46.40 +/- 13.74
New best mean reward!
[I 2024-12-11 01:56:35,551] Trial 534 pruned. 
[I 2024-12-11 01:56:35,552] Trial 535 pruned. 
Eval num_timesteps=320000, episode_reward=346.53 +/- 34.37
Episode length: 49.20 +/- 11.63
New best mean reward!
[I 2024-12-11 01:57:11,403] Trial 537 pruned. 
Eval num_timesteps=320000, episode_reward=348.57 +/- 36.16
Episode length: 48.95 +/- 12.35
New best mean reward!
[I 2024-12-11 01:57:21,950] Trial 538 pruned. 
Eval num_timesteps=320000, episode_reward=336.06 +/- 46.89
Episode length: 54.75 +/- 19.41
New best mean reward!
[I 2024-12-11 01:57:48,346] Trial 540 pruned. 
Eval num_timesteps=320000, episode_reward=325.20 +/- 62.29
Episode length: 68.00 +/- 38.73
New best mean reward!
[I 2024-12-11 01:59:12,231] Trial 545 pruned. 
Eval num_timesteps=320000, episode_reward=341.04 +/- 45.83
Episode length: 49.20 +/- 15.06
New best mean reward!
[I 2024-12-11 01:59:17,981] Trial 541 pruned. 
Eval num_timesteps=320000, episode_reward=-656.91 +/- 606.15
Episode length: 85.65 +/- 46.67
New best mean reward!
[I 2024-12-11 01:59:35,054] Trial 542 pruned. 
Eval num_timesteps=320000, episode_reward=337.68 +/- 50.06
Episode length: 49.95 +/- 15.57
New best mean reward!
[I 2024-12-11 01:59:35,536] Trial 539 pruned. 
Eval num_timesteps=320000, episode_reward=-1666.35 +/- 1710.35
Episode length: 321.80 +/- 393.68
New best mean reward!
[I 2024-12-11 01:59:39,291] Trial 543 pruned. 
Eval num_timesteps=320000, episode_reward=257.38 +/- 146.12
Episode length: 84.15 +/- 52.17
New best mean reward!
[I 2024-12-11 02:00:05,077] Trial 544 pruned. 
Eval num_timesteps=320000, episode_reward=351.47 +/- 39.85
Episode length: 46.35 +/- 13.53
New best mean reward!
[I 2024-12-11 02:02:49,286] Trial 547 pruned. 
Eval num_timesteps=320000, episode_reward=352.04 +/- 39.95
Episode length: 48.50 +/- 18.98
New best mean reward!
[I 2024-12-11 02:03:12,437] Trial 548 pruned. 
Eval num_timesteps=320000, episode_reward=352.50 +/- 39.32
Episode length: 46.45 +/- 13.80
New best mean reward!
[I 2024-12-11 02:04:22,461] Trial 550 pruned. 
Eval num_timesteps=320000, episode_reward=353.38 +/- 38.30
Episode length: 45.75 +/- 12.99
New best mean reward!
[I 2024-12-11 02:04:36,104] Trial 553 pruned. 
Eval num_timesteps=320000, episode_reward=346.03 +/- 43.65
Episode length: 51.20 +/- 20.25
New best mean reward!
[I 2024-12-11 02:05:01,927] Trial 546 pruned. 
Eval num_timesteps=320000, episode_reward=353.10 +/- 38.94
Episode length: 45.75 +/- 13.28
New best mean reward!
[I 2024-12-11 02:05:08,239] Trial 554 pruned. 
Eval num_timesteps=40000, episode_reward=-1095.22 +/- 597.41
Episode length: 393.35 +/- 445.52
New best mean reward!
[I 2024-12-11 02:05:45,028] Trial 557 pruned. 
Eval num_timesteps=40000, episode_reward=-2024.97 +/- 1451.77
Episode length: 808.70 +/- 382.63
New best mean reward!
[I 2024-12-11 02:06:01,169] Trial 558 pruned. 
Eval num_timesteps=320000, episode_reward=353.39 +/- 38.72
Episode length: 45.65 +/- 13.23
New best mean reward!
[I 2024-12-11 02:06:30,671] Trial 556 pruned. 
Eval num_timesteps=320000, episode_reward=352.41 +/- 39.46
Episode length: 45.90 +/- 13.45
New best mean reward!
[I 2024-12-11 02:06:37,411] Trial 555 pruned. 
Eval num_timesteps=320000, episode_reward=353.41 +/- 38.90
Episode length: 45.65 +/- 13.27
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.61 +/- 39.44
Episode length: 46.15 +/- 13.64
New best mean reward!
[I 2024-12-11 02:08:00,139] Trial 560 pruned. 
Eval num_timesteps=320000, episode_reward=351.27 +/- 40.37
Episode length: 46.75 +/- 14.23
New best mean reward!
[I 2024-12-11 02:08:12,799] Trial 551 pruned. 
Eval num_timesteps=320000, episode_reward=352.73 +/- 39.15
Episode length: 45.95 +/- 13.47
New best mean reward!
[I 2024-12-11 02:08:28,248] Trial 561 pruned. 
Eval num_timesteps=320000, episode_reward=346.72 +/- 43.05
Episode length: 49.60 +/- 17.80
New best mean reward!
[I 2024-12-11 02:08:29,730] Trial 549 pruned. 
[I 2024-12-11 02:08:47,584] Trial 559 finished with value: 340.94966320000003 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0009474075588259363, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=353.66 +/- 38.39
Episode length: 45.60 +/- 13.11
New best mean reward!
Eval num_timesteps=320000, episode_reward=350.34 +/- 40.95
Episode length: 47.20 +/- 14.27
New best mean reward!
[I 2024-12-11 02:08:56,528] Trial 552 pruned. 
Eval num_timesteps=320000, episode_reward=347.63 +/- 41.93
Episode length: 48.00 +/- 14.34
New best mean reward!
[I 2024-12-11 02:10:10,039] Trial 564 pruned. 
[I 2024-12-11 02:10:18,681] Trial 562 finished with value: 341.17548035 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0009396027046428229, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=349.01 +/- 40.50
Episode length: 47.85 +/- 13.97
New best mean reward!
[I 2024-12-11 02:10:20,663] Trial 565 pruned. 
Eval num_timesteps=320000, episode_reward=350.57 +/- 40.63
Episode length: 47.20 +/- 14.19
New best mean reward!
[I 2024-12-11 02:10:44,713] Trial 563 pruned. 
Eval num_timesteps=320000, episode_reward=350.34 +/- 39.89
Episode length: 47.25 +/- 13.83
New best mean reward!
[I 2024-12-11 02:11:11,608] Trial 566 pruned. 
Eval num_timesteps=320000, episode_reward=349.97 +/- 39.42
Episode length: 47.70 +/- 13.76
New best mean reward!
[I 2024-12-11 02:11:24,625] Trial 567 pruned. 
Eval num_timesteps=320000, episode_reward=351.51 +/- 39.99
Episode length: 46.90 +/- 13.99
New best mean reward!
[I 2024-12-11 02:11:43,646] Trial 568 pruned. 
Eval num_timesteps=320000, episode_reward=353.39 +/- 38.68
Episode length: 45.70 +/- 13.13
New best mean reward!
[I 2024-12-11 02:12:09,644] Trial 569 pruned. 
Eval num_timesteps=320000, episode_reward=352.47 +/- 39.27
Episode length: 46.05 +/- 13.49
New best mean reward!
[I 2024-12-11 02:12:35,055] Trial 570 pruned. 
Eval num_timesteps=320000, episode_reward=317.89 +/- 48.06
Episode length: 67.60 +/- 22.96
New best mean reward!
[I 2024-12-11 02:12:46,006] Trial 572 pruned. 
Eval num_timesteps=320000, episode_reward=347.12 +/- 43.55
Episode length: 47.85 +/- 14.86
New best mean reward!
[I 2024-12-11 02:13:06,266] Trial 571 pruned. 
Eval num_timesteps=320000, episode_reward=346.16 +/- 46.15
Episode length: 49.35 +/- 17.33
New best mean reward!
[I 2024-12-11 02:13:12,327] Trial 574 pruned. 
Eval num_timesteps=320000, episode_reward=352.33 +/- 38.96
Episode length: 46.50 +/- 13.49
New best mean reward!
[I 2024-12-11 02:13:12,489] Trial 573 pruned. 
Eval num_timesteps=320000, episode_reward=350.28 +/- 40.25
Episode length: 47.15 +/- 13.84
New best mean reward!
[I 2024-12-11 02:13:39,779] Trial 575 pruned. 
Eval num_timesteps=320000, episode_reward=352.42 +/- 39.00
Episode length: 46.40 +/- 13.62
New best mean reward!
[I 2024-12-11 02:14:00,149] Trial 576 pruned. 
Eval num_timesteps=320000, episode_reward=354.00 +/- 37.74
Episode length: 45.40 +/- 12.80
New best mean reward!
Eval num_timesteps=320000, episode_reward=-442.23 +/- 1141.99
Episode length: 387.60 +/- 449.58
New best mean reward!
Eval num_timesteps=320000, episode_reward=-922.89 +/- 239.36
Episode length: 62.95 +/- 24.44
New best mean reward!
[I 2024-12-11 02:17:43,903] Trial 578 pruned. 
[I 2024-12-11 02:17:43,905] Trial 580 pruned. 
Eval num_timesteps=320000, episode_reward=353.81 +/- 38.39
Episode length: 45.60 +/- 13.25
New best mean reward!
Eval num_timesteps=320000, episode_reward=330.68 +/- 65.90
Episode length: 55.30 +/- 24.72
New best mean reward!
[I 2024-12-11 02:17:57,393] Trial 582 pruned. 
Eval num_timesteps=320000, episode_reward=268.57 +/- 220.09
Episode length: 55.15 +/- 25.01
New best mean reward!
[I 2024-12-11 02:17:59,540] Trial 581 pruned. 
Eval num_timesteps=320000, episode_reward=331.54 +/- 66.33
Episode length: 57.00 +/- 29.14
New best mean reward!
[I 2024-12-11 02:18:43,485] Trial 583 pruned. 
Eval num_timesteps=320000, episode_reward=352.80 +/- 38.86
Episode length: 45.85 +/- 13.22
New best mean reward!
[I 2024-12-11 02:19:41,262] Trial 584 pruned. 
Eval num_timesteps=320000, episode_reward=352.69 +/- 38.93
Episode length: 45.95 +/- 13.35
New best mean reward!
[I 2024-12-11 02:19:44,478] Trial 585 pruned. 
Eval num_timesteps=320000, episode_reward=352.54 +/- 38.48
Episode length: 46.30 +/- 13.26
New best mean reward!
[I 2024-12-11 02:19:57,213] Trial 587 pruned. 
Eval num_timesteps=320000, episode_reward=353.66 +/- 38.05
Episode length: 45.60 +/- 13.10
New best mean reward!
[I 2024-12-11 02:20:18,749] Trial 579 finished with value: 334.80504175 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.0002644558533249664, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 02:21:12,126] Trial 577 finished with value: 341.2053873 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.0002636633460976314, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 02:21:12,205] Trial 586 finished with value: 340.7706271 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.0014831205459384733, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'small'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=351.84 +/- 39.17
Episode length: 47.00 +/- 13.71
New best mean reward!
[I 2024-12-11 02:21:27,595] Trial 588 pruned. 
Eval num_timesteps=160000, episode_reward=322.29 +/- 31.40
Episode length: 57.55 +/- 11.14
New best mean reward!
[I 2024-12-11 02:23:22,069] Trial 594 pruned. 
Eval num_timesteps=160000, episode_reward=300.03 +/- 69.49
Episode length: 65.30 +/- 22.11
New best mean reward!
[I 2024-12-11 02:23:40,921] Trial 595 pruned. 
Eval num_timesteps=160000, episode_reward=312.65 +/- 43.79
Episode length: 66.70 +/- 21.62
New best mean reward!
[I 2024-12-11 02:23:47,065] Trial 590 pruned. 
Eval num_timesteps=160000, episode_reward=315.48 +/- 35.53
Episode length: 60.60 +/- 12.28
New best mean reward!
[I 2024-12-11 02:24:00,775] Trial 591 pruned. 
Eval num_timesteps=160000, episode_reward=312.06 +/- 43.02
Episode length: 67.05 +/- 24.75
New best mean reward!
[I 2024-12-11 02:24:23,301] Trial 592 pruned. 
Eval num_timesteps=320000, episode_reward=353.85 +/- 38.20
Episode length: 45.50 +/- 13.06
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.66 +/- 40.49
Episode length: 47.55 +/- 17.99
New best mean reward!
[I 2024-12-11 02:27:55,114] Trial 596 pruned. 
Eval num_timesteps=320000, episode_reward=353.71 +/- 38.10
Episode length: 45.70 +/- 13.01
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.89 +/- 38.15
Episode length: 45.55 +/- 13.08
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.92 +/- 38.20
Episode length: 45.45 +/- 13.03
New best mean reward!
Eval num_timesteps=320000, episode_reward=346.35 +/- 41.61
Episode length: 51.50 +/- 17.61
New best mean reward!
[I 2024-12-11 02:30:24,349] Trial 593 pruned. 
[I 2024-12-11 02:31:04,651] Trial 598 finished with value: 341.20741754999995 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.00021819827763280383, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 02:31:19,363] Trial 599 finished with value: 341.17394845 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.0002171455265977068, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 02:31:31,843] Trial 600 finished with value: 340.2286037 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.0001998324469840417, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=351.47 +/- 40.41
Episode length: 46.50 +/- 13.95
New best mean reward!
[I 2024-12-11 02:32:34,783] Trial 601 pruned. 
Eval num_timesteps=320000, episode_reward=350.61 +/- 41.58
Episode length: 48.45 +/- 18.18
New best mean reward!
[I 2024-12-11 02:32:39,387] Trial 597 pruned. 
Eval num_timesteps=320000, episode_reward=352.46 +/- 38.43
Episode length: 46.60 +/- 13.22
New best mean reward!
[I 2024-12-11 02:33:32,123] Trial 602 pruned. 
Eval num_timesteps=40000, episode_reward=309.35 +/- 46.60
Episode length: 78.95 +/- 26.81
New best mean reward!
[I 2024-12-11 02:33:34,886] Trial 589 finished with value: 341.1996472999999 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.0005190532034654536, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=344.42 +/- 40.84
Episode length: 54.70 +/- 23.09
New best mean reward!
[I 2024-12-11 02:33:35,383] Trial 603 pruned. 
Eval num_timesteps=320000, episode_reward=345.68 +/- 42.68
Episode length: 55.85 +/- 25.32
New best mean reward!
[I 2024-12-11 02:33:52,421] Trial 604 pruned. 
Eval num_timesteps=320000, episode_reward=348.01 +/- 41.28
Episode length: 51.85 +/- 21.86
New best mean reward!
[I 2024-12-11 02:34:00,900] Trial 605 pruned. 
Eval num_timesteps=80000, episode_reward=352.83 +/- 61.14
Episode length: 47.10 +/- 20.14
New best mean reward!
Eval num_timesteps=40000, episode_reward=311.33 +/- 46.15
Episode length: 72.40 +/- 23.84
New best mean reward!
Eval num_timesteps=40000, episode_reward=-1092.76 +/- 750.68
Episode length: 691.55 +/- 424.35
New best mean reward!
[I 2024-12-11 02:34:34,974] Trial 609 pruned. 
Eval num_timesteps=40000, episode_reward=176.94 +/- 173.57
Episode length: 134.60 +/- 74.67
New best mean reward!
Eval num_timesteps=120000, episode_reward=324.10 +/- 49.69
Episode length: 61.75 +/- 25.27
Eval num_timesteps=80000, episode_reward=351.53 +/- 61.73
Episode length: 47.60 +/- 20.19
New best mean reward!
Eval num_timesteps=80000, episode_reward=339.82 +/- 71.14
Episode length: 57.90 +/- 30.25
New best mean reward!
Eval num_timesteps=160000, episode_reward=335.05 +/- 51.44
Episode length: 54.85 +/- 22.05
Eval num_timesteps=120000, episode_reward=325.57 +/- 47.83
Episode length: 58.95 +/- 21.94
Eval num_timesteps=120000, episode_reward=325.49 +/- 44.50
Episode length: 58.70 +/- 17.55
Eval num_timesteps=320000, episode_reward=353.23 +/- 38.46
Episode length: 45.85 +/- 13.21
New best mean reward!
[I 2024-12-11 02:37:04,159] Trial 606 pruned. 
Eval num_timesteps=200000, episode_reward=359.18 +/- 52.10
Episode length: 43.85 +/- 17.89
New best mean reward!
Eval num_timesteps=160000, episode_reward=329.32 +/- 54.52
Episode length: 58.30 +/- 24.64
Eval num_timesteps=160000, episode_reward=325.69 +/- 55.87
Episode length: 60.05 +/- 25.57
Eval num_timesteps=240000, episode_reward=345.41 +/- 36.10
Episode length: 48.35 +/- 12.46
Eval num_timesteps=320000, episode_reward=353.38 +/- 38.62
Episode length: 45.70 +/- 13.23
New best mean reward!
[I 2024-12-11 02:38:36,377] Trial 610 pruned. 
Eval num_timesteps=200000, episode_reward=358.92 +/- 53.06
Episode length: 45.20 +/- 21.29
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.07 +/- 39.84
Episode length: 46.20 +/- 13.76
New best mean reward!
[I 2024-12-11 02:38:53,272] Trial 612 pruned. 
Eval num_timesteps=200000, episode_reward=352.80 +/- 51.83
Episode length: 49.20 +/- 21.26
New best mean reward!
[I 2024-12-11 02:38:58,940] Trial 611 pruned. 
Eval num_timesteps=320000, episode_reward=350.49 +/- 41.06
Episode length: 46.75 +/- 13.99
New best mean reward!
[I 2024-12-11 02:39:14,118] Trial 613 pruned. 
Eval num_timesteps=280000, episode_reward=326.69 +/- 31.68
Episode length: 54.65 +/- 10.91
Eval num_timesteps=240000, episode_reward=344.70 +/- 35.81
Episode length: 48.55 +/- 12.25
Eval num_timesteps=320000, episode_reward=342.22 +/- 43.98
Episode length: 52.20 +/- 24.24
Eval num_timesteps=280000, episode_reward=326.75 +/- 34.11
Episode length: 56.60 +/- 17.13
Eval num_timesteps=360000, episode_reward=325.22 +/- 34.72
Episode length: 55.20 +/- 11.97
Eval num_timesteps=320000, episode_reward=353.88 +/- 37.56
Episode length: 45.65 +/- 12.91
New best mean reward!
Eval num_timesteps=320000, episode_reward=348.62 +/- 37.80
Episode length: 49.85 +/- 14.32
New best mean reward!
[I 2024-12-11 02:41:52,094] Trial 614 pruned. 
Eval num_timesteps=400000, episode_reward=329.51 +/- 18.31
Episode length: 53.60 +/- 6.21
Eval num_timesteps=320000, episode_reward=340.45 +/- 46.67
Episode length: 52.00 +/- 23.20
Eval num_timesteps=440000, episode_reward=337.27 +/- 44.90
Episode length: 52.85 +/- 20.31
Eval num_timesteps=360000, episode_reward=324.40 +/- 34.04
Episode length: 55.70 +/- 11.82
[I 2024-12-11 02:43:19,899] Trial 618 finished with value: 340.0221681 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.0010277166936186347, 'clip_range': 0.3, 'n_epochs': 4, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=349.33 +/- 40.86
Episode length: 49.95 +/- 17.87
New best mean reward!
[I 2024-12-11 02:43:29,592] Trial 615 pruned. 
Eval num_timesteps=320000, episode_reward=353.54 +/- 38.17
Episode length: 45.65 +/- 12.94
New best mean reward!
Eval num_timesteps=320000, episode_reward=351.56 +/- 39.40
Episode length: 48.85 +/- 17.09
New best mean reward!
[I 2024-12-11 02:43:52,725] Trial 617 pruned. 
Eval num_timesteps=480000, episode_reward=329.40 +/- 35.82
Episode length: 55.55 +/- 17.52
Eval num_timesteps=400000, episode_reward=325.96 +/- 21.71
Episode length: 56.75 +/- 12.49
[I 2024-12-11 02:44:30,521] Trial 607 finished with value: 322.80167205000004 and parameters: {'n_envs': 4, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.00011685747002850922, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=440000, episode_reward=335.43 +/- 46.32
Episode length: 53.70 +/- 20.55
Eval num_timesteps=480000, episode_reward=332.51 +/- 29.76
Episode length: 52.70 +/- 10.21
Eval num_timesteps=320000, episode_reward=305.84 +/- 109.14
Episode length: 66.40 +/- 32.80
New best mean reward!
[I 2024-12-11 02:46:45,661] Trial 619 pruned. 
[I 2024-12-11 02:47:00,639] Trial 608 finished with value: 321.50035360000004 and parameters: {'n_envs': 4, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.0001512768796363563, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 02:47:14,668] Trial 616 finished with value: 278.31627485 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.0009359807190074131, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=353.92 +/- 38.03
Episode length: 45.60 +/- 13.05
New best mean reward!
[I 2024-12-11 02:50:34,310] Trial 620 finished with value: 340.72920700000003 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.00047266006581034417, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=352.47 +/- 40.71
Episode length: 47.70 +/- 18.32
New best mean reward!
[I 2024-12-11 02:51:52,026] Trial 621 pruned. 
Eval num_timesteps=320000, episode_reward=353.27 +/- 38.34
Episode length: 45.85 +/- 13.14
New best mean reward!
[I 2024-12-11 02:52:21,048] Trial 622 pruned. 
Eval num_timesteps=320000, episode_reward=344.73 +/- 42.95
Episode length: 49.30 +/- 15.34
New best mean reward!
[I 2024-12-11 02:52:51,265] Trial 623 pruned. 
Eval num_timesteps=320000, episode_reward=353.65 +/- 38.38
Episode length: 45.75 +/- 13.24
New best mean reward!
Eval num_timesteps=320000, episode_reward=350.06 +/- 43.60
Episode length: 48.70 +/- 20.06
New best mean reward!
[I 2024-12-11 02:55:37,533] Trial 626 pruned. 
Eval num_timesteps=320000, episode_reward=352.67 +/- 38.22
Episode length: 46.15 +/- 13.22
New best mean reward!
[I 2024-12-11 02:56:56,888] Trial 625 pruned. 
Eval num_timesteps=320000, episode_reward=352.32 +/- 39.26
Episode length: 46.00 +/- 13.44
New best mean reward!
[I 2024-12-11 02:58:31,688] Trial 627 pruned. 
Eval num_timesteps=320000, episode_reward=353.69 +/- 38.49
Episode length: 45.70 +/- 13.25
New best mean reward!
Eval num_timesteps=320000, episode_reward=197.55 +/- 366.39
Episode length: 47.50 +/- 19.78
New best mean reward!
[I 2024-12-11 03:00:45,868] Trial 629 pruned. 
Eval num_timesteps=320000, episode_reward=353.99 +/- 37.83
Episode length: 45.55 +/- 12.96
New best mean reward!
[I 2024-12-11 03:01:17,246] Trial 624 finished with value: 323.6580351 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.0005794071166293609, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=353.16 +/- 38.43
Episode length: 45.70 +/- 13.11
New best mean reward!
[I 2024-12-11 03:02:33,397] Trial 631 pruned. 
[I 2024-12-11 03:03:05,829] Trial 628 finished with value: 340.94124725 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 6.427418171933498e-05, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=339.84 +/- 43.48
Episode length: 58.00 +/- 22.08
New best mean reward!
[I 2024-12-11 03:05:04,817] Trial 633 pruned. 
Eval num_timesteps=320000, episode_reward=349.02 +/- 42.66
Episode length: 49.35 +/- 18.70
New best mean reward!
[I 2024-12-11 03:05:38,139] Trial 635 pruned. 
Eval num_timesteps=320000, episode_reward=351.89 +/- 40.21
Episode length: 48.65 +/- 18.03
New best mean reward!
[I 2024-12-11 03:05:40,602] Trial 637 pruned. 
Eval num_timesteps=320000, episode_reward=354.09 +/- 37.70
Episode length: 45.55 +/- 12.89
New best mean reward!
[I 2024-12-11 03:06:54,038] Trial 630 finished with value: 341.0348299 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.0003472786450863053, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=347.61 +/- 37.20
Episode length: 48.25 +/- 12.91
New best mean reward!
[I 2024-12-11 03:07:30,361] Trial 636 pruned. 
Eval num_timesteps=320000, episode_reward=347.04 +/- 40.44
Episode length: 50.85 +/- 16.29
New best mean reward!
[I 2024-12-11 03:07:42,939] Trial 634 pruned. 
Eval num_timesteps=320000, episode_reward=353.70 +/- 38.32
Episode length: 45.65 +/- 13.21
New best mean reward!
Eval num_timesteps=320000, episode_reward=330.64 +/- 41.85
Episode length: 64.85 +/- 25.94
New best mean reward!
[I 2024-12-11 03:10:00,566] Trial 639 pruned. 
Eval num_timesteps=320000, episode_reward=352.85 +/- 39.51
Episode length: 45.85 +/- 13.60
New best mean reward!
[I 2024-12-11 03:10:08,424] Trial 640 pruned. 
Eval num_timesteps=320000, episode_reward=318.41 +/- 66.06
Episode length: 61.05 +/- 23.86
New best mean reward!
[I 2024-12-11 03:11:42,360] Trial 641 pruned. 
Eval num_timesteps=320000, episode_reward=312.03 +/- 72.52
Episode length: 63.60 +/- 22.00
New best mean reward!
[I 2024-12-11 03:12:03,807] Trial 642 pruned. 
[I 2024-12-11 03:12:05,122] Trial 638 finished with value: 340.43918885000005 and parameters: {'n_envs': 32, 'batch_size': 64, 'gamma': 0.99, 'learning_rate': 0.00035347798016814586, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=352.33 +/- 39.68
Episode length: 46.40 +/- 13.95
New best mean reward!
[I 2024-12-11 03:12:18,587] Trial 643 pruned. 
[I 2024-12-11 03:13:18,431] Trial 632 finished with value: 340.1165070999999 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.00032360118094096384, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=353.42 +/- 38.70
Episode length: 45.65 +/- 13.30
New best mean reward!
[I 2024-12-11 03:14:25,776] Trial 644 pruned. 
Eval num_timesteps=320000, episode_reward=335.40 +/- 50.15
Episode length: 55.15 +/- 20.34
New best mean reward!
[I 2024-12-11 03:14:46,982] Trial 645 pruned. 
Eval num_timesteps=320000, episode_reward=354.10 +/- 38.14
Episode length: 45.45 +/- 13.19
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.98 +/- 38.12
Episode length: 45.45 +/- 13.08
New best mean reward!
Eval num_timesteps=320000, episode_reward=354.08 +/- 37.95
Episode length: 45.55 +/- 12.96
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.01 +/- 39.12
Episode length: 45.90 +/- 13.46
New best mean reward!
[I 2024-12-11 03:19:56,410] Trial 646 pruned. 
Eval num_timesteps=320000, episode_reward=352.68 +/- 39.70
Episode length: 45.95 +/- 13.76
New best mean reward!
[I 2024-12-11 03:22:03,144] Trial 651 pruned. 
Eval num_timesteps=320000, episode_reward=353.99 +/- 38.23
Episode length: 45.60 +/- 13.18
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.58 +/- 38.78
Episode length: 45.65 +/- 13.45
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.01 +/- 39.20
Episode length: 46.40 +/- 13.49
New best mean reward!
[I 2024-12-11 03:23:48,494] Trial 654 pruned. 
[I 2024-12-11 03:23:54,678] Trial 648 finished with value: 341.1061226 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.00019107640083620227, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 03:25:04,905] Trial 649 finished with value: 341.2701066000001 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.0001985639605837631, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 03:25:17,209] Trial 647 finished with value: 340.97614840000006 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.0002443481454954844, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=347.72 +/- 42.48
Episode length: 49.15 +/- 16.65
New best mean reward!
[I 2024-12-11 03:25:28,926] Trial 656 pruned. 
Eval num_timesteps=320000, episode_reward=352.44 +/- 39.27
Episode length: 46.25 +/- 13.57
New best mean reward!
[I 2024-12-11 03:27:21,821] Trial 653 pruned. 
[I 2024-12-11 03:27:31,415] Trial 652 finished with value: 341.05837175 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.00021894494049983678, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 03:28:35,184] Trial 650 finished with value: 341.13495895 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.0001842136870490909, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=352.31 +/- 39.23
Episode length: 46.65 +/- 13.61
New best mean reward!
[I 2024-12-11 03:28:43,915] Trial 659 pruned. 
Eval num_timesteps=320000, episode_reward=317.55 +/- 96.49
Episode length: 58.90 +/- 34.99
New best mean reward!
[I 2024-12-11 03:30:47,553] Trial 661 pruned. 
Eval num_timesteps=320000, episode_reward=353.31 +/- 38.67
Episode length: 46.15 +/- 13.46
New best mean reward!
[I 2024-12-11 03:31:29,395] Trial 655 pruned. 
Eval num_timesteps=320000, episode_reward=204.22 +/- 270.56
Episode length: 82.10 +/- 41.30
New best mean reward!
[I 2024-12-11 03:32:40,282] Trial 662 pruned. 
Eval num_timesteps=320000, episode_reward=-368.85 +/- 950.17
Episode length: 156.20 +/- 110.16
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.04 +/- 38.59
Episode length: 45.95 +/- 13.31
New best mean reward!
[I 2024-12-11 03:32:46,688] Trial 658 pruned. 
[I 2024-12-11 03:32:46,718] Trial 657 pruned. 
Eval num_timesteps=320000, episode_reward=289.68 +/- 99.50
Episode length: 74.60 +/- 40.57
New best mean reward!
[I 2024-12-11 03:34:24,976] Trial 660 pruned. 
Eval num_timesteps=320000, episode_reward=-2387.64 +/- 2405.13
Episode length: 543.25 +/- 457.22
New best mean reward!
[I 2024-12-11 03:35:48,465] Trial 663 pruned. 
Eval num_timesteps=320000, episode_reward=347.64 +/- 43.20
Episode length: 47.40 +/- 14.21
New best mean reward!
[I 2024-12-11 03:37:58,371] Trial 664 pruned. 
Eval num_timesteps=320000, episode_reward=349.83 +/- 40.47
Episode length: 46.95 +/- 13.54
New best mean reward!
[I 2024-12-11 03:38:39,884] Trial 665 pruned. 
Eval num_timesteps=320000, episode_reward=347.69 +/- 44.52
Episode length: 47.65 +/- 14.86
New best mean reward!
[I 2024-12-11 03:39:58,934] Trial 670 pruned. 
Eval num_timesteps=320000, episode_reward=344.02 +/- 47.54
Episode length: 50.00 +/- 17.04
New best mean reward!
[I 2024-12-11 03:40:07,565] Trial 668 pruned. 
Eval num_timesteps=320000, episode_reward=341.26 +/- 39.00
Episode length: 54.00 +/- 16.80
New best mean reward!
[I 2024-12-11 03:40:07,980] Trial 667 pruned. 
Eval num_timesteps=320000, episode_reward=348.31 +/- 41.47
Episode length: 47.10 +/- 14.00
New best mean reward!
[I 2024-12-11 03:41:40,964] Trial 666 pruned. 
Eval num_timesteps=320000, episode_reward=350.92 +/- 39.66
Episode length: 46.85 +/- 13.44
New best mean reward!
[I 2024-12-11 03:41:44,535] Trial 669 pruned. 
Eval num_timesteps=320000, episode_reward=351.70 +/- 39.87
Episode length: 46.45 +/- 13.61
New best mean reward!
[I 2024-12-11 03:42:16,860] Trial 671 pruned. 
Eval num_timesteps=320000, episode_reward=352.35 +/- 39.21
Episode length: 46.15 +/- 13.51
New best mean reward!
[I 2024-12-11 03:43:08,754] Trial 673 pruned. 
Eval num_timesteps=320000, episode_reward=351.91 +/- 39.37
Episode length: 46.20 +/- 13.52
New best mean reward!
[I 2024-12-11 03:44:18,795] Trial 672 pruned. 
Eval num_timesteps=320000, episode_reward=352.59 +/- 38.80
Episode length: 46.05 +/- 13.20
New best mean reward!
[I 2024-12-11 03:44:48,733] Trial 677 pruned. 
Eval num_timesteps=320000, episode_reward=352.67 +/- 39.03
Episode length: 46.25 +/- 13.64
New best mean reward!
[I 2024-12-11 03:44:49,118] Trial 679 pruned. 
Eval num_timesteps=320000, episode_reward=352.96 +/- 38.87
Episode length: 46.10 +/- 13.41
New best mean reward!
[I 2024-12-11 03:45:27,641] Trial 678 pruned. 
Eval num_timesteps=320000, episode_reward=352.48 +/- 37.97
Episode length: 46.40 +/- 12.98
New best mean reward!
[I 2024-12-11 03:45:41,534] Trial 674 pruned. 
Eval num_timesteps=320000, episode_reward=353.33 +/- 38.67
Episode length: 45.60 +/- 13.14
New best mean reward!
[I 2024-12-11 03:46:08,612] Trial 680 pruned. 
Eval num_timesteps=320000, episode_reward=353.03 +/- 38.70
Episode length: 45.85 +/- 13.21
New best mean reward!
[I 2024-12-11 03:46:32,768] Trial 682 pruned. 
Eval num_timesteps=320000, episode_reward=353.58 +/- 38.68
Episode length: 45.55 +/- 13.30
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.95 +/- 38.24
Episode length: 45.45 +/- 13.06
New best mean reward!
Eval num_timesteps=320000, episode_reward=350.78 +/- 39.11
Episode length: 47.85 +/- 13.94
New best mean reward!
[I 2024-12-11 03:47:38,455] Trial 684 pruned. 
[I 2024-12-11 03:47:56,172] Trial 681 finished with value: 341.1188575499999 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0004081565564930529, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=347.18 +/- 44.08
Episode length: 48.40 +/- 15.34
New best mean reward!
[I 2024-12-11 03:48:05,229] Trial 675 pruned. 
Eval num_timesteps=320000, episode_reward=351.12 +/- 40.27
Episode length: 46.60 +/- 13.83
New best mean reward!
[I 2024-12-11 03:48:09,309] Trial 685 pruned. 
Eval num_timesteps=320000, episode_reward=352.22 +/- 39.48
Episode length: 46.25 +/- 13.48
New best mean reward!
[I 2024-12-11 03:48:26,969] Trial 686 pruned. 
[I 2024-12-11 03:48:36,861] Trial 683 finished with value: 341.00691785 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0004346390653285739, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=40000, episode_reward=197.91 +/- 93.48
Episode length: 129.15 +/- 40.47
New best mean reward!
[I 2024-12-11 03:48:43,987] Trial 689 pruned. 
Eval num_timesteps=40000, episode_reward=-1058.14 +/- 516.22
Episode length: 918.60 +/- 244.21
New best mean reward!
[I 2024-12-11 03:48:48,662] Trial 688 pruned. 
Eval num_timesteps=320000, episode_reward=348.33 +/- 40.65
Episode length: 48.45 +/- 14.31
New best mean reward!
[I 2024-12-11 03:48:55,096] Trial 676 pruned. 
Eval num_timesteps=40000, episode_reward=-1267.52 +/- 757.27
Episode length: 141.55 +/- 53.22
New best mean reward!
[I 2024-12-11 03:49:04,053] Trial 690 pruned. 
Eval num_timesteps=40000, episode_reward=207.66 +/- 66.18
Episode length: 144.25 +/- 43.15
New best mean reward!
[I 2024-12-11 03:49:14,472] Trial 691 pruned. 
Eval num_timesteps=320000, episode_reward=351.12 +/- 40.49
Episode length: 46.65 +/- 13.84
New best mean reward!
[I 2024-12-11 03:49:36,670] Trial 687 pruned. 
Eval num_timesteps=40000, episode_reward=-1088.14 +/- 823.28
Episode length: 860.55 +/- 332.16
New best mean reward!
[I 2024-12-11 03:49:58,645] Trial 692 pruned. 
Eval num_timesteps=320000, episode_reward=352.30 +/- 38.85
Episode length: 46.80 +/- 13.56
New best mean reward!
[I 2024-12-11 03:51:43,580] Trial 699 pruned. 
Eval num_timesteps=320000, episode_reward=350.70 +/- 41.19
Episode length: 50.15 +/- 21.16
New best mean reward!
[I 2024-12-11 03:52:59,200] Trial 693 pruned. 
Eval num_timesteps=320000, episode_reward=351.49 +/- 40.33
Episode length: 48.55 +/- 18.89
New best mean reward!
[I 2024-12-11 03:54:09,895] Trial 695 pruned. 
Eval num_timesteps=160000, episode_reward=319.89 +/- 33.07
Episode length: 57.85 +/- 11.34
New best mean reward!
[I 2024-12-11 03:54:16,018] Trial 701 pruned. 
Eval num_timesteps=160000, episode_reward=321.78 +/- 33.07
Episode length: 57.85 +/- 12.92
New best mean reward!
[I 2024-12-11 03:55:27,512] Trial 702 pruned. 
Eval num_timesteps=160000, episode_reward=324.80 +/- 30.97
Episode length: 55.85 +/- 10.69
New best mean reward!
[I 2024-12-11 03:55:32,862] Trial 700 pruned. 
Eval num_timesteps=320000, episode_reward=349.22 +/- 39.73
Episode length: 48.15 +/- 13.83
New best mean reward!
[I 2024-12-11 03:56:00,142] Trial 703 pruned. 
Eval num_timesteps=320000, episode_reward=352.02 +/- 40.43
Episode length: 46.25 +/- 14.17
New best mean reward!
[I 2024-12-11 03:56:28,360] Trial 694 pruned. 
Eval num_timesteps=320000, episode_reward=353.51 +/- 38.77
Episode length: 45.90 +/- 13.42
New best mean reward!
Eval num_timesteps=320000, episode_reward=351.46 +/- 40.45
Episode length: 46.90 +/- 14.22
New best mean reward!
[I 2024-12-11 03:57:32,990] Trial 697 pruned. 
Eval num_timesteps=320000, episode_reward=333.28 +/- 62.71
Episode length: 53.00 +/- 20.29
New best mean reward!
[I 2024-12-11 03:58:43,699] Trial 698 pruned. 
Eval num_timesteps=320000, episode_reward=352.72 +/- 38.95
Episode length: 46.35 +/- 13.47
New best mean reward!
[I 2024-12-11 03:58:55,147] Trial 706 pruned. 
Eval num_timesteps=320000, episode_reward=164.02 +/- 122.93
Episode length: 139.95 +/- 63.96
New best mean reward!
[I 2024-12-11 03:59:25,147] Trial 707 pruned. 
Eval num_timesteps=320000, episode_reward=352.47 +/- 39.65
Episode length: 46.00 +/- 13.56
New best mean reward!
[I 2024-12-11 04:00:12,699] Trial 705 pruned. 
Eval num_timesteps=320000, episode_reward=-235.97 +/- 540.18
Episode length: 478.45 +/- 394.51
New best mean reward!
[I 2024-12-11 04:00:21,980] Trial 708 pruned. 
Eval num_timesteps=320000, episode_reward=352.99 +/- 38.62
Episode length: 46.05 +/- 13.32
New best mean reward!
[I 2024-12-11 04:00:34,381] Trial 704 pruned. 
Eval num_timesteps=320000, episode_reward=333.29 +/- 48.05
Episode length: 56.70 +/- 18.14
New best mean reward!
[I 2024-12-11 04:01:41,975] Trial 709 pruned. 
Eval num_timesteps=320000, episode_reward=352.15 +/- 40.49
Episode length: 46.20 +/- 13.99
New best mean reward!
[I 2024-12-11 04:02:04,121] Trial 712 pruned. 
Eval num_timesteps=320000, episode_reward=352.42 +/- 39.38
Episode length: 45.95 +/- 13.43
New best mean reward!
[I 2024-12-11 04:02:07,751] Trial 713 pruned. 
[I 2024-12-11 04:02:22,752] Trial 696 finished with value: 340.84331 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.00021740681931048332, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=352.99 +/- 38.60
Episode length: 45.80 +/- 13.10
New best mean reward!
[I 2024-12-11 04:02:49,181] Trial 714 pruned. 
Eval num_timesteps=320000, episode_reward=310.76 +/- 60.35
Episode length: 69.55 +/- 28.31
New best mean reward!
[I 2024-12-11 04:03:33,109] Trial 710 pruned. 
Eval num_timesteps=320000, episode_reward=353.37 +/- 38.57
Episode length: 45.75 +/- 13.31
New best mean reward!
[I 2024-12-11 04:03:38,030] Trial 715 pruned. 
Eval num_timesteps=320000, episode_reward=353.73 +/- 38.21
Episode length: 45.70 +/- 13.12
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.44 +/- 38.94
Episode length: 46.15 +/- 13.33
New best mean reward!
[I 2024-12-11 04:03:57,460] Trial 716 pruned. 
Eval num_timesteps=320000, episode_reward=353.44 +/- 38.26
Episode length: 46.00 +/- 13.16
New best mean reward!
[I 2024-12-11 04:04:07,347] Trial 711 pruned. 
Eval num_timesteps=320000, episode_reward=352.15 +/- 39.78
Episode length: 46.05 +/- 13.45
New best mean reward!
[I 2024-12-11 04:04:40,126] Trial 718 pruned. 
[I 2024-12-11 04:05:01,144] Trial 717 finished with value: 340.75151515000005 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.00033785622623728703, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=353.32 +/- 38.29
Episode length: 45.70 +/- 12.88
New best mean reward!
[I 2024-12-11 04:05:29,700] Trial 720 pruned. 
Eval num_timesteps=320000, episode_reward=349.83 +/- 40.32
Episode length: 47.40 +/- 13.84
New best mean reward!
[I 2024-12-11 04:05:30,920] Trial 721 pruned. 
Eval num_timesteps=320000, episode_reward=354.07 +/- 37.94
Episode length: 45.65 +/- 13.08
New best mean reward!
Eval num_timesteps=320000, episode_reward=351.57 +/- 40.32
Episode length: 46.25 +/- 13.69
New best mean reward!
[I 2024-12-11 04:06:10,882] Trial 722 pruned. 
Eval num_timesteps=320000, episode_reward=350.55 +/- 40.58
Episode length: 49.85 +/- 18.57
New best mean reward!
[I 2024-12-11 04:06:20,572] Trial 725 pruned. 
Eval num_timesteps=320000, episode_reward=351.30 +/- 40.33
Episode length: 46.70 +/- 14.13
New best mean reward!
[I 2024-12-11 04:06:23,291] Trial 723 pruned. 
Eval num_timesteps=320000, episode_reward=353.28 +/- 39.38
Episode length: 47.15 +/- 17.15
New best mean reward!
[I 2024-12-11 04:06:32,118] Trial 724 pruned. 
[I 2024-12-11 04:07:11,476] Trial 719 finished with value: 341.15656595 and parameters: {'n_envs': 32, 'batch_size': 256, 'gamma': 0.99, 'learning_rate': 0.0003488605949730342, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=352.69 +/- 38.89
Episode length: 46.20 +/- 13.49
New best mean reward!
[I 2024-12-11 04:07:57,178] Trial 726 pruned. 
Eval num_timesteps=320000, episode_reward=351.45 +/- 38.60
Episode length: 47.70 +/- 13.68
New best mean reward!
[I 2024-12-11 04:08:38,617] Trial 728 pruned. 
Eval num_timesteps=320000, episode_reward=344.32 +/- 44.72
Episode length: 49.70 +/- 15.47
New best mean reward!
[I 2024-12-11 04:10:22,848] Trial 727 pruned. 
Eval num_timesteps=320000, episode_reward=341.11 +/- 43.77
Episode length: 52.75 +/- 17.53
New best mean reward!
[I 2024-12-11 04:10:51,845] Trial 729 pruned. 
Eval num_timesteps=320000, episode_reward=318.51 +/- 43.29
Episode length: 76.40 +/- 25.07
New best mean reward!
[I 2024-12-11 04:11:13,143] Trial 730 pruned. 
Eval num_timesteps=320000, episode_reward=346.64 +/- 40.36
Episode length: 49.15 +/- 13.88
New best mean reward!
[I 2024-12-11 04:11:16,148] Trial 731 pruned. 
Eval num_timesteps=320000, episode_reward=338.24 +/- 48.11
Episode length: 56.05 +/- 22.48
New best mean reward!
[I 2024-12-11 04:12:29,185] Trial 732 pruned. 
Eval num_timesteps=320000, episode_reward=337.85 +/- 52.73
Episode length: 54.70 +/- 21.85
New best mean reward!
[I 2024-12-11 04:12:43,978] Trial 733 pruned. 
Eval num_timesteps=320000, episode_reward=348.17 +/- 41.06
Episode length: 48.10 +/- 14.04
New best mean reward!
[I 2024-12-11 04:16:20,303] Trial 734 pruned. 
Eval num_timesteps=320000, episode_reward=353.36 +/- 38.70
Episode length: 45.75 +/- 13.22
New best mean reward!
[I 2024-12-11 04:17:42,210] Trial 735 pruned. 
Eval num_timesteps=320000, episode_reward=351.01 +/- 40.10
Episode length: 46.65 +/- 13.78
New best mean reward!
[I 2024-12-11 04:18:03,085] Trial 740 pruned. 
Eval num_timesteps=320000, episode_reward=353.12 +/- 38.61
Episode length: 45.90 +/- 13.23
New best mean reward!
[I 2024-12-11 04:18:46,494] Trial 736 pruned. 
Eval num_timesteps=320000, episode_reward=351.88 +/- 40.45
Episode length: 46.55 +/- 14.14
New best mean reward!
[I 2024-12-11 04:19:04,803] Trial 737 pruned. 
Eval num_timesteps=320000, episode_reward=351.97 +/- 39.86
Episode length: 46.40 +/- 13.82
New best mean reward!
[I 2024-12-11 04:19:27,770] Trial 741 pruned. 
Eval num_timesteps=320000, episode_reward=351.63 +/- 39.88
Episode length: 46.20 +/- 13.66
New best mean reward!
[I 2024-12-11 04:19:49,978] Trial 742 pruned. 
Eval num_timesteps=320000, episode_reward=354.16 +/- 38.17
Episode length: 45.35 +/- 13.12
New best mean reward!
Eval num_timesteps=320000, episode_reward=353.09 +/- 39.20
Episode length: 45.75 +/- 13.39
New best mean reward!
[I 2024-12-11 04:20:42,191] Trial 743 pruned. 
Eval num_timesteps=320000, episode_reward=351.44 +/- 40.28
Episode length: 46.45 +/- 13.77
New best mean reward!
[I 2024-12-11 04:20:58,249] Trial 744 pruned. 
Eval num_timesteps=320000, episode_reward=351.86 +/- 39.71
Episode length: 46.35 +/- 13.64
New best mean reward!
[I 2024-12-11 04:21:20,314] Trial 745 pruned. 
Eval num_timesteps=320000, episode_reward=353.92 +/- 38.23
Episode length: 45.60 +/- 13.09
New best mean reward!
Eval num_timesteps=320000, episode_reward=351.68 +/- 39.86
Episode length: 46.60 +/- 13.86
New best mean reward!
[I 2024-12-11 04:21:41,337] Trial 746 pruned. 
Eval num_timesteps=320000, episode_reward=352.58 +/- 38.79
Episode length: 46.15 +/- 13.38
New best mean reward!
[I 2024-12-11 04:22:25,682] Trial 747 pruned. 
Eval num_timesteps=320000, episode_reward=352.24 +/- 38.96
Episode length: 46.45 +/- 13.46
New best mean reward!
[I 2024-12-11 04:23:50,989] Trial 748 pruned. 
Eval num_timesteps=320000, episode_reward=352.07 +/- 39.40
Episode length: 46.25 +/- 13.47
New best mean reward!
[I 2024-12-11 04:24:13,014] Trial 749 pruned. 
Eval num_timesteps=320000, episode_reward=353.55 +/- 38.63
Episode length: 45.70 +/- 13.20
New best mean reward!
[I 2024-12-11 04:25:10,538] Trial 739 finished with value: 341.0298189 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.00017303671421592845, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
[I 2024-12-11 04:26:46,544] Trial 750 finished with value: 341.03828830000003 and parameters: {'n_envs': 32, 'batch_size': 128, 'gamma': 0.99, 'learning_rate': 0.00011076169411829932, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 278 with value: 341.28885329999997.
Eval num_timesteps=320000, episode_reward=352.21 +/- 38.05
Episode length: 46.50 +/- 12.94
New best mean reward!
[I 2024-12-11 04:26:55,285] Trial 753 pruned. 
[I 2024-12-11 04:27:36,564] Trial 738 finished with value: 341.30618304999996 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.00016982394779265357, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 738 with value: 341.30618304999996.
Eval num_timesteps=320000, episode_reward=149.08 +/- 348.97
Episode length: 90.70 +/- 47.75
New best mean reward!
[I 2024-12-11 04:29:20,934] Trial 754 pruned. 
Eval num_timesteps=320000, episode_reward=341.61 +/- 47.17
Episode length: 50.85 +/- 18.03
New best mean reward!
[I 2024-12-11 04:30:52,572] Trial 751 pruned. 
Eval num_timesteps=320000, episode_reward=318.79 +/- 74.73
Episode length: 57.75 +/- 27.20
New best mean reward!
[I 2024-12-11 04:33:01,058] Trial 752 pruned. 
Eval num_timesteps=320000, episode_reward=344.55 +/- 43.95
Episode length: 49.45 +/- 15.71
New best mean reward!
[I 2024-12-11 04:36:17,115] Trial 757 pruned. 
Eval num_timesteps=320000, episode_reward=348.37 +/- 41.39
Episode length: 47.35 +/- 13.98
New best mean reward!
[I 2024-12-11 04:36:30,470] Trial 756 pruned. 
Eval num_timesteps=320000, episode_reward=353.37 +/- 38.65
Episode length: 45.90 +/- 13.30
New best mean reward!
[I 2024-12-11 04:37:43,644] Trial 758 pruned. 
Eval num_timesteps=320000, episode_reward=353.28 +/- 38.87
Episode length: 45.75 +/- 13.32
New best mean reward!
[I 2024-12-11 04:39:58,142] Trial 759 pruned. 
Eval num_timesteps=160000, episode_reward=316.89 +/- 37.40
Episode length: 58.80 +/- 12.56
New best mean reward!
[I 2024-12-11 04:39:58,935] Trial 760 pruned. 
Eval num_timesteps=160000, episode_reward=322.57 +/- 31.46
Episode length: 56.85 +/- 10.76
New best mean reward!
[I 2024-12-11 04:41:03,411] Trial 761 pruned. 
Eval num_timesteps=320000, episode_reward=325.27 +/- 51.18
Episode length: 55.45 +/- 18.99
New best mean reward!
[I 2024-12-11 04:41:09,808] Trial 755 pruned. 
Eval num_timesteps=160000, episode_reward=319.38 +/- 32.86
Episode length: 58.80 +/- 13.40
New best mean reward!
[I 2024-12-11 04:43:28,136] Trial 763 pruned. 
Eval num_timesteps=320000, episode_reward=-809.39 +/- 462.74
Episode length: 62.85 +/- 21.62
New best mean reward!
[I 2024-12-11 04:47:23,161] Trial 762 pruned. 
Eval num_timesteps=320000, episode_reward=350.57 +/- 38.74
Episode length: 49.80 +/- 15.52
New best mean reward!
[I 2024-12-11 04:48:07,727] Trial 764 pruned. 
Eval num_timesteps=320000, episode_reward=-880.75 +/- 364.69
Episode length: 67.30 +/- 28.27
New best mean reward!
[I 2024-12-11 04:51:37,110] Trial 765 pruned. 
Eval num_timesteps=320000, episode_reward=345.78 +/- 45.03
Episode length: 48.65 +/- 15.39
New best mean reward!
[I 2024-12-11 04:51:47,509] Trial 766 pruned. 
Eval num_timesteps=320000, episode_reward=348.82 +/- 39.52
Episode length: 49.20 +/- 14.42
New best mean reward!
[I 2024-12-11 04:53:30,345] Trial 770 pruned. 
Eval num_timesteps=320000, episode_reward=349.66 +/- 38.29
Episode length: 48.65 +/- 14.02
New best mean reward!
[I 2024-12-11 04:53:40,931] Trial 769 pruned. 
Eval num_timesteps=320000, episode_reward=349.92 +/- 40.82
Episode length: 47.05 +/- 13.89
New best mean reward!
[I 2024-12-11 04:55:11,965] Trial 771 pruned. 
Eval num_timesteps=320000, episode_reward=74.41 +/- 515.20
Episode length: 69.00 +/- 25.58
New best mean reward!
[I 2024-12-11 04:55:47,024] Trial 767 pruned. 
Eval num_timesteps=320000, episode_reward=345.67 +/- 43.56
Episode length: 50.05 +/- 15.79
New best mean reward!
[I 2024-12-11 04:55:50,540] Trial 768 pruned. 
Eval num_timesteps=320000, episode_reward=331.13 +/- 51.72
Episode length: 70.50 +/- 37.46
New best mean reward!
[I 2024-12-11 04:56:43,143] Trial 774 pruned. 
Eval num_timesteps=320000, episode_reward=353.68 +/- 38.37
Episode length: 45.60 +/- 13.09
New best mean reward!
Eval num_timesteps=320000, episode_reward=157.79 +/- 180.08
Episode length: 114.20 +/- 57.38
New best mean reward!
[I 2024-12-11 04:59:00,107] Trial 772 pruned. 
[I 2024-12-11 04:59:18,534] Trial 775 finished with value: 340.9944526 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0007188516863144696, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 738 with value: 341.30618304999996.
Eval num_timesteps=320000, episode_reward=353.82 +/- 38.06
Episode length: 45.70 +/- 13.02
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.05 +/- 39.66
Episode length: 46.20 +/- 13.52
New best mean reward!
[I 2024-12-11 05:00:55,041] Trial 776 pruned. 
[I 2024-12-11 05:01:54,006] Trial 777 finished with value: 341.03055205 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0005077969417086798, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 738 with value: 341.30618304999996.
Eval num_timesteps=320000, episode_reward=350.61 +/- 40.84
Episode length: 46.85 +/- 14.04
New best mean reward!
[I 2024-12-11 05:02:53,831] Trial 778 pruned. 
Eval num_timesteps=320000, episode_reward=288.24 +/- 180.79
Episode length: 61.85 +/- 44.51
New best mean reward!
[I 2024-12-11 05:03:20,864] Trial 773 pruned. 
Eval num_timesteps=320000, episode_reward=348.87 +/- 43.39
Episode length: 50.65 +/- 21.32
New best mean reward!
[I 2024-12-11 05:04:29,291] Trial 779 pruned. 
Eval num_timesteps=320000, episode_reward=351.29 +/- 38.88
Episode length: 47.20 +/- 13.59
New best mean reward!
[I 2024-12-11 05:05:54,944] Trial 781 pruned. 
Eval num_timesteps=320000, episode_reward=351.05 +/- 39.91
Episode length: 46.90 +/- 13.69
New best mean reward!
[I 2024-12-11 05:06:02,712] Trial 782 pruned. 
Eval num_timesteps=320000, episode_reward=353.14 +/- 38.65
Episode length: 45.95 +/- 13.28
New best mean reward!
[I 2024-12-11 05:06:11,102] Trial 780 pruned. 
Eval num_timesteps=320000, episode_reward=351.07 +/- 40.49
Episode length: 46.65 +/- 13.84
New best mean reward!
[I 2024-12-11 05:07:24,492] Trial 783 pruned. 
Eval num_timesteps=320000, episode_reward=350.67 +/- 41.42
Episode length: 49.65 +/- 19.46
New best mean reward!
[I 2024-12-11 05:10:43,732] Trial 784 pruned. 
Eval num_timesteps=320000, episode_reward=353.53 +/- 37.97
Episode length: 46.25 +/- 13.18
New best mean reward!
Eval num_timesteps=320000, episode_reward=352.12 +/- 39.72
Episode length: 46.50 +/- 13.65
New best mean reward!
[I 2024-12-11 05:12:39,437] Trial 786 pruned. 
[I 2024-12-11 05:13:12,888] Trial 785 finished with value: 339.0226739 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.0011747551315226376, 'clip_range': 0.3, 'n_epochs': 4, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 738 with value: 341.30618304999996.
Eval num_timesteps=320000, episode_reward=353.96 +/- 38.11
Episode length: 45.55 +/- 13.00
New best mean reward!
[I 2024-12-11 05:26:35,946] Trial 787 finished with value: 341.13206299999996 and parameters: {'n_envs': 32, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 0.00010229912603675722, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 738 with value: 341.30618304999996.
Eval num_timesteps=40000, episode_reward=-2563.42 +/- 1759.82
Episode length: 874.20 +/- 299.99
New best mean reward!
[I 2024-12-11 05:27:22,663] Trial 788 pruned. 
Eval num_timesteps=320000, episode_reward=352.62 +/- 39.20
Episode length: 46.20 +/- 13.53
New best mean reward!
[I 2024-12-11 05:29:13,436] Trial 789 pruned. 
Eval num_timesteps=320000, episode_reward=342.86 +/- 45.79
Episode length: 50.95 +/- 15.88
New best mean reward!
[I 2024-12-11 05:30:29,122] Trial 790 pruned. 
Eval num_timesteps=320000, episode_reward=353.39 +/- 38.51
Episode length: 45.70 +/- 13.16
New best mean reward!
[I 2024-12-11 05:39:09,659] Trial 791 pruned. 
Eval num_timesteps=320000, episode_reward=353.61 +/- 38.88
Episode length: 45.65 +/- 13.37
New best mean reward!
[I 2024-12-11 05:42:08,955] Trial 792 finished with value: 341.21691695 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.000503205203733499, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 738 with value: 341.30618304999996.
Eval num_timesteps=320000, episode_reward=353.66 +/- 38.22
Episode length: 45.50 +/- 12.98
New best mean reward!
[I 2024-12-11 05:45:10,600] Trial 793 finished with value: 341.02048089999994 and parameters: {'n_envs': 32, 'batch_size': 512, 'gamma': 0.99, 'learning_rate': 0.0004271803071127057, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'use_sde': False, 'net_arch': 'medium'}. Best is trial 738 with value: 341.30618304999996.
Eval num_timesteps=320000, episode_reward=352.37 +/- 40.08
Episode length: 46.05 +/- 13.82
New best mean reward!
[I 2024-12-11 05:54:23,998] Trial 794 pruned. 
Eval num_timesteps=320000, episode_reward=352.35 +/- 39.20
Episode length: 46.10 +/- 13.37
New best mean reward!
[I 2024-12-11 05:57:03,528] Trial 795 pruned. 
