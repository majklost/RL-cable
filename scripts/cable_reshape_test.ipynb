{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv,VecMonitor\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback,BaseCallback\n",
    "from gymnasium.wrappers import FlattenObservation,TimeLimit,NormalizeObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deform_rl.envs.Cable_reshape_env.environment import CableReshape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(seed=None):\n",
    "    env = CableReshape(render_mode='human',seed=seed,seg_num=10,cable_length=300,scale_factor=800)\n",
    "    env = TimeLimit(env, max_episode_steps=1000)\n",
    "    return env\n",
    "\n",
    "save_dir = \"./saved_models/cable_reshape\"\n",
    "log_dir = \"./logs/cable_reshape\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment will not be deterministic\n",
      "Planning will not be deterministic\n",
      "seed for  BezierSampler  is  7744\n",
      "seed for  NDIMSampler  is  6513\n"
     ]
    }
   ],
   "source": [
    "env = make_env()\n",
    "check_env(env, warn=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show random actions\n",
    "env.reset()\n",
    "for i in range(1000):\n",
    "    env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    if pygame.event.get(pygame.QUIT):\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michal/Documents/Skola/bakalarka/RL/RL-cable/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "eval_env = VecMonitor(make_vec_env(make_env, n_envs=1))\n",
    "env = VecMonitor(make_vec_env(make_env, n_envs=4))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env=eval_env,\n",
    "    n_eval_episodes=15,\n",
    "    eval_freq=10000,\n",
    "    best_model_save_path=save_dir,\n",
    "    verbose=1,\n",
    "    render=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=37136, episode_reward=-7808.76 +/- 4080.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77136, episode_reward=-13068.24 +/- 8610.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=117136, episode_reward=-11978.72 +/- 9715.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=157136, episode_reward=-26054.72 +/- 22275.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=197136, episode_reward=-12944.29 +/- 7707.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=237136, episode_reward=-11357.61 +/- 7064.17\n",
      "Episode length: 1000.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = PPO('MlpPolicy', env, device='cpu', verbose=0,tensorboard_log=\"./logs/cable_reshape/\").learn(500000, tb_log_name=\"ppo_cable_reshape\",callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=520000, episode_reward=-8443.81 +/- 4895.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=-6626.92 +/- 1104.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=-4967.46 +/- 3395.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=-6417.61 +/- 2660.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=-5449.54 +/- 2278.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=-5261.87 +/- 2013.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=-4042.01 +/- 1441.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=800000, episode_reward=-3135.46 +/- 2015.26\n",
      "Episode length: 941.07 +/- 220.51\n",
      "New best mean reward!\n",
      "Eval num_timesteps=840000, episode_reward=-2569.95 +/- 1292.00\n",
      "Episode length: 937.00 +/- 235.72\n",
      "New best mean reward!\n",
      "Eval num_timesteps=880000, episode_reward=-3070.71 +/- 1448.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=-2566.73 +/- 1033.10\n",
      "Episode length: 947.27 +/- 197.31\n",
      "New best mean reward!\n",
      "Eval num_timesteps=960000, episode_reward=-1705.91 +/- 281.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000000, episode_reward=-1705.13 +/- 964.57\n",
      "Episode length: 844.87 +/- 314.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1040000, episode_reward=-1756.09 +/- 888.85\n",
      "Episode length: 892.20 +/- 275.23\n",
      "Eval num_timesteps=1080000, episode_reward=-1526.70 +/- 659.00\n",
      "Episode length: 903.07 +/- 248.77\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1120000, episode_reward=-1589.48 +/- 1146.49\n",
      "Episode length: 782.80 +/- 362.88\n",
      "Eval num_timesteps=1160000, episode_reward=-1761.13 +/- 726.15\n",
      "Episode length: 936.60 +/- 237.22\n",
      "Eval num_timesteps=1200000, episode_reward=-1366.57 +/- 816.41\n",
      "Episode length: 839.20 +/- 322.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1240000, episode_reward=-1323.24 +/- 1204.70\n",
      "Episode length: 759.73 +/- 399.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1280000, episode_reward=-1164.77 +/- 1044.70\n",
      "Episode length: 709.47 +/- 412.07\n",
      "New best mean reward!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7525cb1bc850>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(800000,tb_log_name=\"ppo_cable_reshape\",callback=eval_callback,reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1320000, episode_reward=-1572.55 +/- 638.62\n",
      "Episode length: 942.00 +/- 217.02\n",
      "Eval num_timesteps=1360000, episode_reward=-1996.52 +/- 1131.96\n",
      "Episode length: 936.93 +/- 235.97\n",
      "Eval num_timesteps=1400000, episode_reward=-1176.67 +/- 1130.61\n",
      "Episode length: 722.47 +/- 392.76\n",
      "Eval num_timesteps=1440000, episode_reward=-1510.86 +/- 757.96\n",
      "Episode length: 943.47 +/- 211.53\n",
      "Eval num_timesteps=1480000, episode_reward=-1818.68 +/- 658.35\n",
      "Episode length: 944.20 +/- 208.78\n",
      "Eval num_timesteps=1520000, episode_reward=-1419.79 +/- 1067.03\n",
      "Episode length: 769.27 +/- 382.98\n",
      "Eval num_timesteps=1560000, episode_reward=-1348.45 +/- 799.69\n",
      "Episode length: 876.47 +/- 315.07\n",
      "Eval num_timesteps=1600000, episode_reward=-1083.74 +/- 845.09\n",
      "Episode length: 762.87 +/- 394.78\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1640000, episode_reward=-1325.08 +/- 894.63\n",
      "Episode length: 799.73 +/- 337.35\n",
      "Eval num_timesteps=1680000, episode_reward=-928.82 +/- 861.74\n",
      "Episode length: 669.53 +/- 408.52\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1720000, episode_reward=-1374.52 +/- 814.38\n",
      "Episode length: 856.53 +/- 287.16\n",
      "Eval num_timesteps=1760000, episode_reward=-1162.08 +/- 712.05\n",
      "Episode length: 834.07 +/- 335.04\n",
      "Eval num_timesteps=1800000, episode_reward=-1259.74 +/- 905.05\n",
      "Episode length: 828.53 +/- 344.59\n",
      "Eval num_timesteps=1840000, episode_reward=-910.25 +/- 806.85\n",
      "Episode length: 729.00 +/- 386.83\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1880000, episode_reward=-1007.03 +/- 1184.57\n",
      "Episode length: 598.73 +/- 431.23\n",
      "Eval num_timesteps=1920000, episode_reward=-1001.06 +/- 650.16\n",
      "Episode length: 820.73 +/- 347.40\n",
      "Eval num_timesteps=1960000, episode_reward=-1027.02 +/- 798.24\n",
      "Episode length: 784.47 +/- 358.21\n",
      "Eval num_timesteps=2000000, episode_reward=-796.42 +/- 799.30\n",
      "Episode length: 677.00 +/- 401.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2040000, episode_reward=-989.17 +/- 607.73\n",
      "Episode length: 813.73 +/- 310.93\n",
      "Eval num_timesteps=2080000, episode_reward=-1119.08 +/- 997.81\n",
      "Episode length: 736.33 +/- 381.48\n",
      "Eval num_timesteps=2120000, episode_reward=-951.88 +/- 985.11\n",
      "Episode length: 681.07 +/- 394.57\n",
      "Eval num_timesteps=2160000, episode_reward=-1165.61 +/- 1105.23\n",
      "Episode length: 740.00 +/- 368.02\n",
      "Eval num_timesteps=2200000, episode_reward=-845.28 +/- 938.27\n",
      "Episode length: 624.47 +/- 403.98\n",
      "Eval num_timesteps=2240000, episode_reward=-1125.65 +/- 1034.28\n",
      "Episode length: 726.00 +/- 389.46\n",
      "Eval num_timesteps=2280000, episode_reward=-572.56 +/- 835.18\n",
      "Episode length: 554.20 +/- 421.76\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2320000, episode_reward=-751.94 +/- 789.93\n",
      "Episode length: 658.73 +/- 422.53\n",
      "Eval num_timesteps=2360000, episode_reward=-481.50 +/- 799.45\n",
      "Episode length: 494.93 +/- 414.32\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2400000, episode_reward=-724.15 +/- 971.80\n",
      "Episode length: 556.40 +/- 420.93\n",
      "Eval num_timesteps=2440000, episode_reward=-1154.63 +/- 749.04\n",
      "Episode length: 827.87 +/- 344.66\n",
      "Eval num_timesteps=2480000, episode_reward=-721.17 +/- 782.69\n",
      "Episode length: 649.13 +/- 430.72\n",
      "Eval num_timesteps=2520000, episode_reward=-1019.66 +/- 951.81\n",
      "Episode length: 660.00 +/- 421.91\n",
      "Eval num_timesteps=2560000, episode_reward=-197.71 +/- 596.96\n",
      "Episode length: 336.27 +/- 342.34\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2600000, episode_reward=-569.08 +/- 759.06\n",
      "Episode length: 562.47 +/- 411.42\n",
      "Eval num_timesteps=2640000, episode_reward=-1237.09 +/- 1248.66\n",
      "Episode length: 721.40 +/- 396.15\n",
      "Eval num_timesteps=2680000, episode_reward=-461.80 +/- 844.07\n",
      "Episode length: 464.73 +/- 386.47\n",
      "Eval num_timesteps=2720000, episode_reward=-146.67 +/- 575.07\n",
      "Episode length: 369.00 +/- 384.87\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2760000, episode_reward=-662.74 +/- 734.34\n",
      "Episode length: 627.07 +/- 405.85\n",
      "Eval num_timesteps=2800000, episode_reward=-325.22 +/- 914.77\n",
      "Episode length: 358.80 +/- 389.74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7525cb1bc850>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(1500000,tb_log_name=\"ppo_cable_reshape\",callback=eval_callback,reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random pick actions and visualize\n",
    "tenv = make_env(30)\n",
    "obs, _ = tenv.reset()\n",
    "# tenv = eval_env\n",
    "\n",
    "# t_model = model\n",
    "t_model = PPO.load(os.path.join(save_dir, \"best_model.zip\"),force_reset=True,device='cpu')\n",
    "EP_CNT = 10\n",
    "ep_cnt = 0\n",
    "cnt = 0\n",
    "rev_sum = 0\n",
    "for i in range(10000):\n",
    "    if cnt >= 1000:\n",
    "        print(\"Killed by timeout\")\n",
    "        obs,_ = tenv.reset()\n",
    "        cnt = 0\n",
    "    action,_ = t_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done,truncated, info = tenv.step(action)\n",
    "    rev_sum += reward\n",
    "    tenv.render()\n",
    "    if done:\n",
    "        obs,_ = tenv.reset()\n",
    "        ep_cnt += 1\n",
    "        print(\"Episode done: \", cnt, \"Reward: \", rev_sum)\n",
    "        cnt=0\n",
    "        rev_sum = 0\n",
    "        if ep_cnt >= EP_CNT:\n",
    "            break\n",
    "    if pygame.event.get(pygame.QUIT):\n",
    "        break\n",
    "    cnt +=1\n",
    "tenv.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
