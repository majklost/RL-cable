{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pygame\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines3 import PPO,SAC\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv,VecMonitor\n",
    "from gymnasium.wrappers import FlattenObservation,TimeLimit,NormalizeObservation\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "\n",
    "from deform_rl.sim.Rectangle_env.environment import Rectangle1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import ObservationWrapper\n",
    "from gymnasium.spaces import Box,Dict\n",
    "\n",
    "class CustomNormalizeObsrvation(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.width = env.width\n",
    "        self.height = env.height\n",
    "        # self.observation_space = Dict({\n",
    "        #     'position': Box(low=-1, high=1, shape=(2,)),\n",
    "        #     'velocity': Box(low=-1, high=1, shape=(2,)),\n",
    "        #     'goal': Box(low=-1, high=1, shape=(2,)),\n",
    "        # })\n",
    "        self.observation_space = Box(low=-1, high=1, shape=(6,))\n",
    "    def observation(self, observation):\n",
    "        mean = np.array([self.width, self.height]) / 2\n",
    "        position = (observation['position'] - mean)/ [self.width, self.height]\n",
    "        velocity = np.tanh(observation['velocity'])\n",
    "        target = (observation['target'] - mean)/ [self.width, self.height]\n",
    "        # return {'position': position, 'velocity': velocity, 'goal': target}\n",
    "        return np.concatenate([position, velocity, target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_cfg = {\n",
    "    'width': 800,\n",
    "    'height': 800,\n",
    "    'FPS': 60,\n",
    "    'gravity': 0,\n",
    "    'damping': .15,\n",
    "    'collision_slope': 0.01,\n",
    "}\n",
    "save_dir = os.path.join(\"saved_models\")\n",
    "log_dir = os.path.join(\"logs\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "def _init(threshold=10):\n",
    "    # Base env\n",
    "    env = Rectangle1D(sim_config=sim_cfg, threshold=threshold, oneD=False, render_mode='human')\n",
    "    env = CustomNormalizeObsrvation(env)\n",
    "    # Apply wrappers\n",
    "    # env = FlattenObservation(env)\n",
    "    env = TimeLimit(env, max_episode_steps=1000)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# env = _init()\n",
    "env = VecMonitor(DummyVecEnv([_init]*4))\n",
    "\n",
    "eval_env = VecMonitor(DummyVecEnv([_init]))\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env=eval_env,\n",
    "    n_eval_episodes=5,\n",
    "    eval_freq=10000,\n",
    "    best_model_save_path=save_dir,\n",
    "    verbose=1,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "\n",
    "# check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done:  64 Reward:  1337.928094030994\n",
      "Episode done:  68 Reward:  1364.1087888998504\n",
      "Episode done:  44 Reward:  1169.3059259478953\n",
      "Episode done:  31 Reward:  1084.0659515070558\n",
      "Episode done:  74 Reward:  1417.5990872330117\n",
      "Episode done:  35 Reward:  1108.1976164432085\n",
      "Episode done:  73 Reward:  1408.5880448121527\n",
      "Episode done:  47 Reward:  1191.4454631244207\n",
      "Episode done:  20 Reward:  1030.0934000121047\n",
      "Episode done:  41 Reward:  1147.995100209791\n",
      "Episode done:  30 Reward:  1078.3693987663914\n",
      "Episode done:  69 Reward:  1372.9237137287614\n",
      "Episode done:  82 Reward:  1490.8814493912118\n",
      "Episode done:  81 Reward:  1481.614498431591\n",
      "Episode done:  64 Reward:  1329.2939852331208\n",
      "Episode done:  43 Reward:  1162.106326728532\n",
      "Episode done:  37 Reward:  1121.0134576802907\n",
      "Episode done:  34 Reward:  1101.9712556534782\n",
      "Episode done:  44 Reward:  1169.3059259478955\n",
      "Episode done:  88 Reward:  1547.0350717065062\n",
      "Episode done:  87 Reward:  1537.614476171962\n",
      "Episode done:  85 Reward:  1518.8449242486354\n",
      "Episode done:  68 Reward:  1364.10878889985\n"
     ]
    }
   ],
   "source": [
    "class LinearAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def predict(self, obs, deterministic=True):\n",
    "        pos = obs[:2]\n",
    "        target = obs[4:]\n",
    "        diff = target - pos\n",
    "        return diff/np.linalg.norm(diff), None\n",
    "\n",
    "tenv = _init(threshold=10)\n",
    "obs, _ = tenv.reset()\n",
    "# tenv = eval_env\n",
    "\n",
    "cnt = 0\n",
    "rev_sum = 0\n",
    "la = LinearAgent()\n",
    "for i in range(10000):\n",
    "    if cnt >= 1000:\n",
    "        print(\"Killed by timeout\")\n",
    "        obs,_ = tenv.reset()\n",
    "        cnt = 0\n",
    "    action,_ = la.predict(obs, deterministic=True)\n",
    "    obs, reward, done,truncated, info = tenv.step(action)\n",
    "    rev_sum += reward\n",
    "    tenv.render()\n",
    "    if done:\n",
    "        obs,_ = tenv.reset()\n",
    "        print(\"Episode done: \", cnt, \"Reward: \", rev_sum)\n",
    "        cnt=0\n",
    "        rev_sum = 0\n",
    "    if pygame.event.get(pygame.QUIT):\n",
    "        break\n",
    "    cnt +=1\n",
    "tenv.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# for i in range(10):\n",
    "#     act =env.action_space.sample()\n",
    "#     obs, reward, done,truncated, info = env.step(act)\n",
    "#     print(obs)\n",
    "#     print(reward)\n",
    "#     print(done)\n",
    "#     print(truncated)\n",
    "#     print(info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=34752, episode_reward=433.97 +/- 413.32\n",
      "Episode length: 916.40 +/- 167.20\n",
      "Eval num_timesteps=74752, episode_reward=576.12 +/- 344.83\n",
      "Episode length: 792.00 +/- 258.57\n",
      "Eval num_timesteps=114752, episode_reward=963.56 +/- 393.09\n",
      "Episode length: 490.80 +/- 227.95\n",
      "Eval num_timesteps=154752, episode_reward=869.68 +/- 438.38\n",
      "Episode length: 534.40 +/- 261.47\n",
      "Eval num_timesteps=194752, episode_reward=1201.57 +/- 198.52\n",
      "Episode length: 255.40 +/- 63.68\n",
      "Eval num_timesteps=234752, episode_reward=1168.97 +/- 231.88\n",
      "Episode length: 234.20 +/- 121.99\n",
      "Eval num_timesteps=274752, episode_reward=1097.11 +/- 209.57\n",
      "Episode length: 221.20 +/- 52.57\n",
      "Eval num_timesteps=314752, episode_reward=1012.44 +/- 236.01\n",
      "Episode length: 353.40 +/- 201.20\n",
      "Eval num_timesteps=354752, episode_reward=1145.32 +/- 204.11\n",
      "Episode length: 276.20 +/- 138.33\n",
      "Eval num_timesteps=394752, episode_reward=1293.55 +/- 168.96\n",
      "Episode length: 215.20 +/- 73.35\n",
      "Eval num_timesteps=434752, episode_reward=1561.42 +/- 62.08\n",
      "Episode length: 193.20 +/- 84.78\n",
      "New best mean reward!\n",
      "Eval num_timesteps=474752, episode_reward=1341.43 +/- 231.82\n",
      "Episode length: 176.80 +/- 66.80\n",
      "Eval num_timesteps=514752, episode_reward=1148.34 +/- 173.06\n",
      "Episode length: 198.80 +/- 107.20\n",
      "Eval num_timesteps=554752, episode_reward=1364.11 +/- 172.67\n",
      "Episode length: 124.20 +/- 60.94\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\",env,device='cpu',verbose=0,tensorboard_log=\"./logs/\",learning_rate=2.0633e-05).learn(550000,tb_log_name=\"test_run\",callback=eval_callback)\n",
    "# model2 = SAC(\"MlpPolicy\",env,device='cpu',verbose=0,tensorboard_log=\"./logs/\").learn(250000,tb_log_name=\"test_run_SAC\",callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1114752, episode_reward=1209.40 +/- 214.73\n",
      "Episode length: 191.00 +/- 78.93\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m550000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_run\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Skola/bakalarka/RL/RL-cable/.venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Skola/bakalarka/RL/RL-cable/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Skola/bakalarka/RL/RL-cable/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:195\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 195\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Skola/bakalarka/RL/RL-cable/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Skola/bakalarka/RL/RL-cable/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_monitor.py:76\u001b[0m, in \u001b[0;36mVecMonitor.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m---> 76\u001b[0m     obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_returns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Skola/bakalarka/RL/RL-cable/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:71\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m     70\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_from_buf(), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones), deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos))\n",
      "File \u001b[0;32m~/Documents/Skola/bakalarka/RL/RL-cable/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:108\u001b[0m, in \u001b[0;36mDummyVecEnv._save_obs\u001b[0;34m(self, env_idx, obs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_obs[key][env_idx] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_obs[key][env_idx] \u001b[38;5;241m=\u001b[39m obs[key]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(550000,tb_log_name=\"test_run\",callback=eval_callback,reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done:  347 Reward:  990.4906206767868\n",
      "Episode done:  210 Reward:  908.9175057479426\n",
      "Episode done:  163 Reward:  1629.611486663179\n",
      "Episode done:  355 Reward:  1542.0011643323676\n",
      "Episode done:  201 Reward:  976.2292812610376\n",
      "Episode done:  125 Reward:  1250.9331226697027\n",
      "Episode done:  201 Reward:  1025.3510298013796\n",
      "Episode done:  110 Reward:  1168.7497052666145\n",
      "Episode done:  53 Reward:  1093.5533647905193\n",
      "Episode done:  366 Reward:  1171.0749608500323\n",
      "Episode done:  207 Reward:  1263.9801724198317\n",
      "Episode done:  141 Reward:  1231.0711786812685\n",
      "Episode done:  212 Reward:  1073.1031949866433\n",
      "Episode done:  194 Reward:  878.6350305493085\n",
      "Episode done:  122 Reward:  1218.4329858080985\n",
      "Episode done:  378 Reward:  1332.7576633968902\n",
      "Episode done:  200 Reward:  961.7833556524434\n",
      "Episode done:  125 Reward:  1639.5413952479475\n",
      "Episode done:  204 Reward:  1141.1998018103818\n",
      "Episode done:  352 Reward:  1198.9951019479624\n",
      "Episode done:  39 Reward:  999.01375819836\n",
      "Episode done:  225 Reward:  1534.431809974453\n",
      "Episode done:  75 Reward:  1302.4788440979928\n",
      "Episode done:  121 Reward:  1220.8523971295265\n",
      "Episode done:  116 Reward:  1028.8682052767538\n",
      "Episode done:  142 Reward:  1499.2271397928894\n",
      "Episode done:  134 Reward:  1068.0387072440924\n",
      "Episode done:  107 Reward:  1582.1077273668725\n",
      "Episode done:  208 Reward:  1054.863333400802\n",
      "Episode done:  301 Reward:  1054.5331013193734\n",
      "Episode done:  199 Reward:  1031.7190551389158\n",
      "Episode done:  129 Reward:  1547.8240929637814\n",
      "Episode done:  118 Reward:  1182.0714446202344\n",
      "Episode done:  367 Reward:  1317.0432591387275\n",
      "Episode done:  171 Reward:  1442.6263318119911\n",
      "Episode done:  209 Reward:  1188.1398159888993\n",
      "Episode done:  330 Reward:  1588.5334575729062\n",
      "Episode done:  144 Reward:  1116.5787148836105\n",
      "Episode done:  326 Reward:  806.5972600286975\n",
      "Episode done:  228 Reward:  1581.3981846019283\n"
     ]
    }
   ],
   "source": [
    "# random pick actions and visualize\n",
    "tenv = _init()\n",
    "obs, _ = tenv.reset()\n",
    "# tenv = eval_env\n",
    "\n",
    "t_model = PPO.load(os.path.join(save_dir, \"best_model.zip\"),force_reset=True)\n",
    "cnt = 0\n",
    "rev_sum = 0\n",
    "for i in range(10000):\n",
    "    if cnt >= 1000:\n",
    "        print(\"Killed by timeout\")\n",
    "        obs,_ = tenv.reset()\n",
    "        cnt = 0\n",
    "    action,_ = t_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done,truncated, info = tenv.step(action)\n",
    "    rev_sum += reward\n",
    "    tenv.render()\n",
    "    if done:\n",
    "        obs,_ = tenv.reset()\n",
    "        print(\"Episode done: \", cnt, \"Reward: \", rev_sum)\n",
    "        cnt=0\n",
    "        rev_sum = 0\n",
    "    if pygame.event.get(pygame.QUIT):\n",
    "        break\n",
    "    cnt +=1\n",
    "tenv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
