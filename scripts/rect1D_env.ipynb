{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pygame\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines3 import PPO,SAC\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv,VecMonitor\n",
    "from gymnasium.wrappers import FlattenObservation,TimeLimit,NormalizeObservation\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback,BaseCallback\n",
    "\n",
    "from deform_rl.sim.Rectangle_env.environment import Rectangle1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import ObservationWrapper\n",
    "from gymnasium.spaces import Box,Dict\n",
    "\n",
    "class CustomNormalizeObsrvation(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.width = env.width\n",
    "        self.height = env.height\n",
    "        # self.observation_space = Dict({\n",
    "        #     'position': Box(low=-1, high=1, shape=(2,)),\n",
    "        #     'velocity': Box(low=-1, high=1, shape=(2,)),\n",
    "        #     'goal': Box(low=-1, high=1, shape=(2,)),\n",
    "        # })\n",
    "        self.observation_space = Box(low=-1, high=1, shape=(6,))\n",
    "    def observation(self, observation):\n",
    "        mean = np.array([self.width, self.height]) / 2\n",
    "        position = (observation['position'] - mean)/ [self.width, self.height]\n",
    "        velocity = np.tanh(observation['velocity'])\n",
    "        target = (observation['target'] - mean)/ [self.width, self.height]\n",
    "        # return {'position': position, 'velocity': velocity, 'goal': target}\n",
    "        return np.concatenate([position, velocity, target])\n",
    "    def observation(self, observation):\n",
    "        # mean = np.array([self.width,self.height]) / 2\n",
    "        position = observation['position']\n",
    "        target = observation['target']\n",
    "        velocity = observation['velocity']\n",
    "        rel_target = target - position\n",
    "        rel_target /= np.array([self.width,self.height])\n",
    "        velocity /= np.array([self.width,self.height])\n",
    "        return np.concatenate([rel_target,velocity])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "    def _on_step(self) -> bool:\n",
    "        self\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_cfg = {\n",
    "    'width': 800,\n",
    "    'height': 800,\n",
    "    'FPS': 60,\n",
    "    'gravity': 0,\n",
    "    'damping': .15,\n",
    "    'collision_slope': 0.01,\n",
    "}\n",
    "save_dir = os.path.join(\"saved_models\")\n",
    "log_dir = os.path.join(\"logs\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "def _init(threshold=30,seed=None):\n",
    "    # Base env\n",
    "    env = Rectangle1D(sim_config=sim_cfg, threshold=threshold, oneD=False, render_mode='human', seed=seed)\n",
    "    env = CustomNormalizeObsrvation(env)\n",
    "    # Apply wrappers\n",
    "    # env = FlattenObservation(env)\n",
    "    env = TimeLimit(env, max_episode_steps=1000)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "# env = _init()\n",
    "env = VecMonitor(DummyVecEnv([_init]*4))\n",
    "\n",
    "eval_env = VecMonitor(DummyVecEnv([_init]))\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env=eval_env,\n",
    "    n_eval_episodes=15,\n",
    "    eval_freq=10000,\n",
    "    best_model_save_path=save_dir,\n",
    "    verbose=1,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "\n",
    "# check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done:  67 Reward:  285.9732495199314\n",
      "Episode done:  24 Reward:  414.36746980236035\n",
      "Episode done:  89 Reward:  226.54679748558226\n",
      "Episode done:  103 Reward:  187.88013940257133\n",
      "Episode done:  67 Reward:  288.94574275322304\n",
      "Episode done:  67 Reward:  288.43492763146935\n",
      "Episode done:  31 Reward:  392.0880789974382\n",
      "Episode done:  37 Reward:  375.84620660106606\n",
      "Episode done:  46 Reward:  347.76249522108196\n",
      "Episode done:  31 Reward:  392.5163887286036\n"
     ]
    }
   ],
   "source": [
    "class LinearAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    # def predict(self, obs, deterministic=True):\n",
    "    #     pos = obs[:2]\n",
    "    #     target = obs[4:]\n",
    "    #     diff = target - pos\n",
    "    #     return diff/np.linalg.norm(diff), None\n",
    "    def predict(self, obs, deterministic=True):\n",
    "        rel_target = obs[:2]\n",
    "        # target = obs[4:]\n",
    "        # diff = target - pos\n",
    "        return rel_target/np.linalg.norm(rel_target), None\n",
    "\n",
    "tenv = _init(seed=60)\n",
    "obs, _ = tenv.reset()\n",
    "# tenv = eval_env\n",
    "\n",
    "cnt = 0\n",
    "rev_sum = 0\n",
    "la = LinearAgent()\n",
    "EP_CNT = 10\n",
    "ep_cnt = 0\n",
    "for i in range(10000):\n",
    "    if cnt >= 1000:\n",
    "        print(\"Killed by timeout\")\n",
    "        obs,_ = tenv.reset()\n",
    "        cnt = 0\n",
    "    action,_ = la.predict(obs, deterministic=True)\n",
    "    obs, reward, done,truncated, info = tenv.step(action)\n",
    "    rev_sum += reward\n",
    "    tenv.render()\n",
    "    if done:\n",
    "        obs,_ = tenv.reset()\n",
    "        ep_cnt += 1\n",
    "        print(\"Episode done: \", cnt, \"Reward: \", rev_sum)\n",
    "        cnt=0\n",
    "        rev_sum = 0\n",
    "        if ep_cnt >= EP_CNT:\n",
    "            break\n",
    "    if pygame.event.get(pygame.QUIT):\n",
    "        break\n",
    "    cnt +=1\n",
    "tenv.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# for i in range(10):\n",
    "#     act =env.action_space.sample()\n",
    "#     obs, reward, done,truncated, info = env.step(act)\n",
    "#     print(obs)\n",
    "#     print(reward)\n",
    "#     print(done)\n",
    "#     print(truncated)\n",
    "#     print(info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=40000, episode_reward=-5171.34 +/- 2672.33\n",
      "Episode length: 945.67 +/- 188.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-3069.64 +/- 1411.82\n",
      "Episode length: 974.93 +/- 93.79\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-6455.56 +/- 3841.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-3261.99 +/- 1254.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-1889.18 +/- 662.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=-9475.90 +/- 11345.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-6259.55 +/- 8684.22\n",
      "Episode length: 823.27 +/- 354.54\n",
      "Eval num_timesteps=320000, episode_reward=-142.26 +/- 440.13\n",
      "Episode length: 419.67 +/- 413.03\n",
      "New best mean reward!\n",
      "Eval num_timesteps=360000, episode_reward=240.05 +/- 53.68\n",
      "Episode length: 116.93 +/- 44.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=284.71 +/- 59.70\n",
      "Episode length: 79.53 +/- 40.71\n",
      "New best mean reward!\n",
      "Eval num_timesteps=440000, episode_reward=265.10 +/- 57.55\n",
      "Episode length: 81.67 +/- 29.85\n",
      "Eval num_timesteps=480000, episode_reward=288.43 +/- 50.13\n",
      "Episode length: 68.80 +/- 18.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=520000, episode_reward=305.60 +/- 60.33\n",
      "Episode length: 70.93 +/- 33.79\n",
      "New best mean reward!\n",
      "Eval num_timesteps=560000, episode_reward=288.62 +/- 54.56\n",
      "Episode length: 68.73 +/- 19.53\n",
      "Eval num_timesteps=600000, episode_reward=299.28 +/- 89.29\n",
      "Episode length: 67.67 +/- 35.98\n",
      "Eval num_timesteps=640000, episode_reward=291.62 +/- 55.58\n",
      "Episode length: 66.40 +/- 19.20\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\",env,device='cpu',verbose=0,tensorboard_log=\"./logs/\").learn(650000,tb_log_name=\"test_run\",callback=eval_callback)\n",
    "# model2 = SAC(\"MlpPolicy\",env,device='cpu',verbose=0,tensorboard_log=\"./logs/\").learn(250000,tb_log_name=\"test_run_SAC\",callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=680000, episode_reward=302.18 +/- 42.08\n",
      "Episode length: 62.60 +/- 14.66\n",
      "Eval num_timesteps=720000, episode_reward=298.92 +/- 58.47\n",
      "Episode length: 63.60 +/- 20.14\n",
      "Eval num_timesteps=760000, episode_reward=306.94 +/- 72.93\n",
      "Episode length: 61.07 +/- 25.64\n",
      "New best mean reward!\n",
      "Eval num_timesteps=800000, episode_reward=327.50 +/- 42.72\n",
      "Episode length: 53.53 +/- 14.81\n",
      "New best mean reward!\n",
      "Eval num_timesteps=840000, episode_reward=291.29 +/- 67.85\n",
      "Episode length: 66.27 +/- 23.86\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x77d0b02441f0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(200000,tb_log_name=\"test_run\",callback=eval_callback,reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done:  67 Reward:  284.7484789764993\n",
      "Episode done:  24 Reward:  414.1190133139253\n",
      "Episode done:  90 Reward:  222.9232719430945\n",
      "Episode done:  107 Reward:  180.2535479333676\n",
      "Episode done:  68 Reward:  287.1883751214751\n",
      "Episode done:  67 Reward:  287.08307143609983\n",
      "Episode done:  32 Reward:  390.740646853306\n",
      "Episode done:  37 Reward:  375.75657497062764\n",
      "Episode done:  47 Reward:  346.77900302309934\n",
      "Episode done:  32 Reward:  391.1615670080315\n"
     ]
    }
   ],
   "source": [
    "# random pick actions and visualize\n",
    "tenv = _init(seed=60)\n",
    "obs, _ = tenv.reset()\n",
    "# tenv = eval_env\n",
    "\n",
    "# t_model = model\n",
    "t_model = PPO.load(os.path.join(save_dir, \"best_model.zip\"),force_reset=True,device='cpu')\n",
    "EP_CNT = 10\n",
    "ep_cnt = 0\n",
    "cnt = 0\n",
    "rev_sum = 0\n",
    "for i in range(10000):\n",
    "    if cnt >= 1000:\n",
    "        print(\"Killed by timeout\")\n",
    "        obs,_ = tenv.reset()\n",
    "        cnt = 0\n",
    "    action,_ = t_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done,truncated, info = tenv.step(action)\n",
    "    rev_sum += reward\n",
    "    tenv.render()\n",
    "    if done:\n",
    "        obs,_ = tenv.reset()\n",
    "        ep_cnt += 1\n",
    "        print(\"Episode done: \", cnt, \"Reward: \", rev_sum)\n",
    "        cnt=0\n",
    "        rev_sum = 0\n",
    "        if ep_cnt >= EP_CNT:\n",
    "            break\n",
    "    if pygame.event.get(pygame.QUIT):\n",
    "        break\n",
    "    cnt +=1\n",
    "tenv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
