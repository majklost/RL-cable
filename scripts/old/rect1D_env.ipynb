{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pygame\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines3 import PPO,SAC\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv,VecMonitor\n",
    "from gymnasium.wrappers import FlattenObservation,TimeLimit,NormalizeObservation\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback,BaseCallback\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "\n",
    "from deform_rl.envs.Rectangle_env.environment import Rectangle1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import ObservationWrapper\n",
    "from gymnasium.spaces import Box,Dict\n",
    "\n",
    "class CustomNormalizeObsrvation(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.width = env.width\n",
    "        self.height = env.height\n",
    "        max_length = np.linalg.norm([self.width,self.height])\n",
    "        low = np.array([-self.width,-self.height,-np.inf,-np.inf])\n",
    "        high = np.array([self.width,self.height,np.inf,np.inf])\n",
    "        self.observation_space = Box(low=low,high=high,dtype=np.float32)\n",
    "        \n",
    "        # self.observation_space = Box(low=np.array([0.,0.,-1000.,-1000.,0.,0.]), high=np.array([800.,800.,1000.,1000.,800.,800.]), shape=(6,), dtype=np.float64)\n",
    "        # self.observation_space = Box(low=0, high=1, shape=(4,), dtype=np.float64)\n",
    "    # def observation(self, observation):\n",
    "    #     mean = np.array([self.width, self.height]) / 2\n",
    "    #     position = (observation['position'])\n",
    "    #     velocity = observation['velocity']\n",
    "    #     target = (observation['target'])    \n",
    "    # return np.concatenate([position, velocity, target])\n",
    "    def observation(self, observation):\n",
    "        # mean = np.array([self.width,self.height]) / 2\n",
    "        position = observation['position']\n",
    "        target = observation['target']\n",
    "        velocity = observation['velocity']\n",
    "        rel_target = target - position\n",
    "        # rel_target /= np.array([self.width,self.height])\n",
    "        # velocity /= np.array([self.width,self.height])\n",
    "\n",
    "        return np.concatenate([rel_target,velocity],dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "    def _on_step(self) -> bool:\n",
    "        self\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_cfg = {\n",
    "    'width': 800,\n",
    "    'height': 800,\n",
    "    'FPS': 60,\n",
    "    'gravity': 0,\n",
    "    'damping': .15,\n",
    "    'collision_slope': 0.01,\n",
    "}\n",
    "save_dir = os.path.join(\"saved_models\")\n",
    "log_dir = os.path.join(\"logs\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "def _init(threshold=30,seed=None):\n",
    "    # Base env\n",
    "    env = Rectangle1D(sim_config=sim_cfg, threshold=threshold, oneD=False, render_mode='human', seed=seed)\n",
    "    env = CustomNormalizeObsrvation(env)\n",
    "    # Apply wrappers\n",
    "    # env = FlattenObservation(env)\n",
    "    env = TimeLimit(env, max_episode_steps=1000)\n",
    "    check_env(env, warn=True)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "# env = _init()\n",
    "env = VecMonitor(VecNormalize(DummyVecEnv([_init]*4)))\n",
    "\n",
    "eval_env = VecMonitor(VecNormalize(DummyVecEnv([_init])))\n",
    "# eval_env = _init()\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env=eval_env,\n",
    "    n_eval_episodes=15,\n",
    "    eval_freq=10000,\n",
    "    best_model_save_path=save_dir,\n",
    "    verbose=1,\n",
    "    render=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done:  59 Reward:  -260.1273653982537\n",
      "Episode done:  52 Reward:  -209.78596299994038\n",
      "Episode done:  93 Reward:  -418.87192741785867\n"
     ]
    }
   ],
   "source": [
    "class LinearAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    # def predict(self, obs, deterministic=True):\n",
    "    #     pos = obs[:2]\n",
    "    #     target = obs[4:]\n",
    "    #     diff = target - pos\n",
    "    #     return diff/np.linalg.norm(diff), None\n",
    "    def predict(self, obs, deterministic=True):\n",
    "        rel_target = obs[:2]\n",
    "        # target = obs[4:]\n",
    "        # diff = target - pos\n",
    "        return rel_target/np.linalg.norm(rel_target), None\n",
    "\n",
    "tenv = _init(seed=60)\n",
    "obs, _ = tenv.reset()\n",
    "# tenv = eval_env\n",
    "\n",
    "cnt = 0\n",
    "rev_sum = 0\n",
    "la = LinearAgent()\n",
    "EP_CNT = 10\n",
    "ep_cnt = 0\n",
    "for i in range(10000):\n",
    "    if cnt >= 1000:\n",
    "        print(\"Killed by timeout\")\n",
    "        obs,_ = tenv.reset()\n",
    "        cnt = 0\n",
    "    action,_ = la.predict(obs, deterministic=True)\n",
    "    obs, reward, done,truncated, info = tenv.step(action)\n",
    "    rev_sum += reward\n",
    "    tenv.render()\n",
    "    if done:\n",
    "        obs,_ = tenv.reset()\n",
    "        ep_cnt += 1\n",
    "        print(\"Episode done: \", cnt, \"Reward: \", rev_sum)\n",
    "        cnt=0\n",
    "        rev_sum = 0\n",
    "        if ep_cnt >= EP_CNT:\n",
    "            break\n",
    "    if pygame.event.get(pygame.QUIT):\n",
    "        break\n",
    "    cnt +=1\n",
    "tenv.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# for i in range(10):\n",
    "#     act =env.action_space.sample()\n",
    "#     obs, reward, done,truncated, info = env.step(act)\n",
    "#     print(obs)\n",
    "#     print(reward)\n",
    "#     print(done)\n",
    "#     print(truncated)\n",
    "#     print(info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=40000, episode_reward=-29.23 +/- 10.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-9.03 +/- 5.13\n",
      "Episode length: 940.93 +/- 221.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-0.22 +/- 0.27\n",
      "Episode length: 228.27 +/- 102.70\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=0.03 +/- 0.30\n",
      "Episode length: 94.67 +/- 20.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=0.29 +/- 0.28\n",
      "Episode length: 77.47 +/- 31.86\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=0.41 +/- 0.24\n",
      "Episode length: 77.07 +/- 23.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=280000, episode_reward=0.45 +/- 0.31\n",
      "Episode length: 84.87 +/- 27.44\n",
      "New best mean reward!\n",
      "Eval num_timesteps=320000, episode_reward=0.62 +/- 0.16\n",
      "Episode length: 84.47 +/- 28.53\n",
      "New best mean reward!\n",
      "Eval num_timesteps=360000, episode_reward=0.67 +/- 0.24\n",
      "Episode length: 81.00 +/- 24.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=0.77 +/- 0.16\n",
      "Episode length: 92.47 +/- 24.44\n",
      "New best mean reward!\n",
      "Eval num_timesteps=440000, episode_reward=0.50 +/- 0.51\n",
      "Episode length: 91.73 +/- 23.29\n",
      "Eval num_timesteps=480000, episode_reward=0.88 +/- 0.18\n",
      "Episode length: 86.33 +/- 27.89\n",
      "New best mean reward!\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\",env,device='cpu',verbose=0,tensorboard_log=\"./logs/\").learn(500000,tb_log_name=\"test_run\",callback=eval_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michal/Documents/Skola/bakalarka/RL/RL-cable/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=-10268.87 +/- 24757.20\n",
      "Episode length: 514.93 +/- 398.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-479.49 +/- 1017.03\n",
      "Episode length: 305.53 +/- 231.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=52.70 +/- 122.21\n",
      "Episode length: 189.07 +/- 164.52\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-29.46 +/- 130.06\n",
      "Episode length: 106.47 +/- 32.53\n",
      "Eval num_timesteps=50000, episode_reward=85.34 +/- 110.98\n",
      "Episode length: 107.20 +/- 48.62\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=130.48 +/- 76.02\n",
      "Episode length: 92.73 +/- 26.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=163.63 +/- 75.37\n",
      "Episode length: 86.00 +/- 23.78\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=208.35 +/- 57.85\n",
      "Episode length: 89.67 +/- 31.77\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=224.78 +/- 44.42\n",
      "Episode length: 95.67 +/- 43.38\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=220.05 +/- 58.11\n",
      "Episode length: 89.13 +/- 21.05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# env = VecMonitor(DummyVecEnv([_init]))\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model2 \u001b[38;5;241m=\u001b[39m \u001b[43mSAC\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMlpPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_run_SAC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Skola/bakalarka/RL/RL-cable/.venv/lib/python3.10/site-packages/stable_baselines3/sac/sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[1;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Skola/bakalarka/RL/RL-cable/.venv/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Skola/bakalarka/RL/RL-cable/.venv/lib/python3.10/site-packages/stable_baselines3/sac/sac.py:267\u001b[0m, in \u001b[0;36mSAC.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Optimize the critic\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 267\u001b[0m \u001b[43mcritic_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Compute actor loss\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Alternative: actor_loss = th.mean(log_prob - qf1_pi)\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Min over all critic networks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Skola/bakalarka/RL/RL-cable/.venv/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Skola/bakalarka/RL/RL-cable/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Skola/bakalarka/RL/RL-cable/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# env = VecMonitor(DummyVecEnv([_init]))\n",
    "\n",
    "model2 = SAC(\"MlpPolicy\",env,device='cpu',verbose=0,tensorboard_log=\"./logs/\").learn(500000,tb_log_name=\"test_run_SAC\",callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=547904, episode_reward=0.87 +/- 0.13\n",
      "Episode length: 94.73 +/- 19.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=587904, episode_reward=0.79 +/- 0.35\n",
      "Episode length: 88.73 +/- 26.23\n",
      "Eval num_timesteps=627904, episode_reward=0.87 +/- 0.24\n",
      "Episode length: 84.53 +/- 32.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=667904, episode_reward=0.76 +/- 0.48\n",
      "Episode length: 99.93 +/- 27.44\n",
      "Eval num_timesteps=707904, episode_reward=0.97 +/- 0.25\n",
      "Episode length: 87.33 +/- 34.67\n",
      "New best mean reward!\n",
      "Eval num_timesteps=747904, episode_reward=0.86 +/- 0.50\n",
      "Episode length: 90.53 +/- 29.06\n",
      "Eval num_timesteps=787904, episode_reward=0.91 +/- 0.34\n",
      "Episode length: 94.07 +/- 34.38\n",
      "Eval num_timesteps=827904, episode_reward=0.96 +/- 0.30\n",
      "Episode length: 86.87 +/- 24.95\n",
      "Eval num_timesteps=867904, episode_reward=1.06 +/- 0.32\n",
      "Episode length: 80.53 +/- 24.99\n",
      "New best mean reward!\n",
      "Eval num_timesteps=907904, episode_reward=1.12 +/- 0.24\n",
      "Episode length: 84.93 +/- 23.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=947904, episode_reward=0.98 +/- 0.28\n",
      "Episode length: 92.53 +/- 25.40\n",
      "Eval num_timesteps=987904, episode_reward=1.24 +/- 0.24\n",
      "Episode length: 83.47 +/- 25.72\n",
      "New best mean reward!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7333f9ba3c70>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.learn(500000,tb_log_name=\"test_run\",callback=eval_callback,reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done:  65 Reward:  -155.66247846575294\n",
      "Episode done:  55 Reward:  -41.586381049078966\n"
     ]
    }
   ],
   "source": [
    "# random pick actions and visualize\n",
    "tenv = _init(seed=600)\n",
    "obs, _ = tenv.reset()\n",
    "# tenv = eval_env\n",
    "\n",
    "# t_model = model\n",
    "t_model = PPO.load(os.path.join(save_dir, \"best_model.zip\"),force_reset=True,device='cpu')\n",
    "EP_CNT = 10\n",
    "ep_cnt = 0\n",
    "cnt = 0\n",
    "rev_sum = 0\n",
    "for i in range(10000):\n",
    "    if cnt >= 1000:\n",
    "        print(\"Killed by timeout\")\n",
    "        obs,_ = tenv.reset()\n",
    "        cnt = 0\n",
    "    action,_ = t_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done,truncated, info = tenv.step(action)\n",
    "    rev_sum += reward\n",
    "    tenv.render()\n",
    "    if done:\n",
    "        obs,_ = tenv.reset()\n",
    "        ep_cnt += 1\n",
    "        print(\"Episode done: \", cnt, \"Reward: \", rev_sum)\n",
    "        cnt=0\n",
    "        rev_sum = 0\n",
    "        if ep_cnt >= EP_CNT:\n",
    "            break\n",
    "    if pygame.event.get(pygame.QUIT):\n",
    "        break\n",
    "    cnt +=1\n",
    "tenv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
